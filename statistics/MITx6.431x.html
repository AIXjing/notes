<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Probability - The Science of Uncertainty and Data - Note</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="../favicon.svg">
        
        
        <link rel="shortcut icon" href="../favicon.png">
        
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        
        <link rel="stylesheet" href="../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../chapter_1.html"><strong aria-hidden="true">1.</strong> Chapter 1</a></li><li class="chapter-item expanded "><a href="../git.html"><strong aria-hidden="true">2.</strong> Git</a></li><li class="chapter-item expanded "><a href="../tips.html"><strong aria-hidden="true">3.</strong> Visit with the Client &amp; Setup Overview</a></li><li class="chapter-item expanded "><a href="../frontend/frontend.html"><strong aria-hidden="true">4.</strong> Frontend</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../frontend/vue3.html"><strong aria-hidden="true">4.1.</strong> Vue3 note</a></li><li class="chapter-item expanded "><a href="../frontend/css.html"><strong aria-hidden="true">4.2.</strong> CSS</a></li></ol></li><li class="chapter-item expanded "><a href="../sql/sql.html"><strong aria-hidden="true">5.</strong> SQL</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../sql/duke.html"><strong aria-hidden="true">5.1.</strong> Duke-coursera</a></li></ol></li><li class="chapter-item expanded "><a href="../python/python.html"><strong aria-hidden="true">6.</strong> Python</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../python/Coursera-Michigan.html"><strong aria-hidden="true">6.1.</strong> Python-coursera-Michigan</a></li><li class="chapter-item expanded "><a href="../python/leanningfrommis.html"><strong aria-hidden="true">6.2.</strong> Learning in a hard way</a></li></ol></li><li class="chapter-item expanded "><a href="../statistics/statistics.html"><strong aria-hidden="true">7.</strong> Statistics rewind</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../statistics/MITx6.431x.html" class="active"><strong aria-hidden="true">7.1.</strong> Probability - The Science of Uncertainty and Data</a></li><li class="chapter-item expanded "><a href="../statistics/statinfer.html"><strong aria-hidden="true">7.2.</strong> Improve your statistical inferences</a></li><li class="chapter-item expanded "><a href="../statistics/inferentialstat.html"><strong aria-hidden="true">7.3.</strong> Inferential Statistics</a></li><li class="chapter-item expanded "><a href="../statistics/python.html"><strong aria-hidden="true">7.4.</strong> Statistics with Python</a></li><li class="chapter-item expanded "><a href="../statistics/problems.html"><strong aria-hidden="true">7.5.</strong> Common problems</a></li><li class="chapter-item expanded "><a href="../statistics/Infe_stats_python.html"><strong aria-hidden="true">7.6.</strong> Inferential Statistics with Python </a></li></ol></li><li class="chapter-item expanded "><a href="../dataviztools/dataviztools.html"><strong aria-hidden="true">8.</strong> Data Viz</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../dataviztools/projects.html"><strong aria-hidden="true">8.1.</strong> Dataviz Projects</a></li></ol></li><li class="chapter-item expanded "><a href="../english/english.html"><strong aria-hidden="true">9.</strong> English</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../english/speaking.html"><strong aria-hidden="true">9.1.</strong> Speaking English</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Note</h1>

                    <div class="right-buttons">
                        
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#probability---the-science-of-uncertainty-and-data-2021" id="probability---the-science-of-uncertainty-and-data-2021">Probability - The Science of Uncertainty and Data (2021)</a></h1>
<p>Use the course to re-build my statistics knowledge.</p>
<p><a href="https://www.edx.org/course/probability-the-science-of-uncertainty-and-data">The course link</a></p>
<p><a href="https://ece307.cankaya.edu.tr/uploads/files/introduction%20to%20probability%20(bertsekas,%202nd,%202008).pdf">The Book</a></p>
<h2><a class="header" href="#1-sample-space-and-probability" id="1-sample-space-and-probability">1 Sample Space and Probability</a></h2>
<h3><a class="header" href="#11-sample-space---a-set-of-outcomes" id="11-sample-space---a-set-of-outcomes">1.1 Sample space - A set of outcomes</a></h3>
<ul>
<li>
<p>discrete/finite example</p>
</li>
<li>
<p>continuous example</p>
</li>
</ul>
<h3><a class="header" href="#12-probability-axioms" id="12-probability-axioms">1.2 Probability Axioms</a></h3>
<ul>
<li>
<p>Nonnegativity
\(P(A) \geq 0 \)</p>
</li>
<li>
<p>Normalization
\( P( \Omega ) = 1 \), \(\Omega \) is the entire sample space.</p>
</li>
<li>
<p>(finite) Additivity: A and B are disjoint, then the probability of their unions satisfies \(P(A \cup B) = P(A) + P(B)\) (to be strengthened later)</p>
</li>
</ul>
<h4><a class="header" href="#121-simple-consequences-of-the-axioms" id="121-simple-consequences-of-the-axioms">1.2.1 Simple consequences of the axioms</a></h4>
<ol>
<li>
<p>For a sampe space consist of a finite number of disjointed events,
\[ 
P({s_1, s_2, ...., s_n}) = P(s_1) + P(s_2) + ...... P(s_n)
\]</p>
</li>
<li>
<p>\(A \subset B\), then \(P(A) \leq P(B)\)</p>
</li>
<li>
<p>\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</p>
</li>
<li>
<p>\(P(A \cup B) \leq P(A) + P(B))\)</p>
</li>
</ol>
<h3><a class="header" href="#13-probability-calculations" id="13-probability-calculations">1.3 Probability calculations</a></h3>
<h4><a class="header" href="#131-uniform-probability-law" id="131-uniform-probability-law">1.3.1 Uniform Probability Law</a></h4>
<ul>
<li>
<p>Discrete example</p>
<p>If the sample space consists of n possible outcomes which are equally likely (i.e., all single-element events have the same probability),
\[ 
P(A) = \frac{\text{number of elements of A}}{n}
\]</p>
</li>
<li>
<p>continuous example</p>
<p>probability = area</p>
</li>
</ul>
<h4><a class="header" href="#132-discrete-but-infinite-sample-space" id="132-discrete-but-infinite-sample-space">1.3.2 Discrete but infinite sample space</a></h4>
<ul>
<li>
<p>Sample space: {1, 2, 3 ....}</p>
<p>Given \(P(n) = \frac{1}{2^n}\), n = 1, 2, 3....</p>
<p>As \( P(\Omega) = 1 \): \(\frac{1}{2} + \frac{1}{4} + ....=  \sum\limits_{n=1}^\infty \frac{1}{2^n} = \frac{1}{2}\sum\limits_{n=0}^\infty \frac{1}{2^n} = \frac{1}{2}\frac{1}{1-1/2} = 1\) </p>
</li>
</ul>
<h4><a class="header" href="#133-countable-aditivity-axiom" id="133-countable-aditivity-axiom">1.3.3 Countable aditivity axiom</a></h4>
<p><em>Additivity holds only for &quot;<strong>countable</strong>&quot; sequences of events</em></p>
<p>If \(A_1, A_2, A_3 ...\) is an \(\underline{\text{infinite sequence of disjoined events}}\),</p>
<p>\[
P(A_1 \cup A_2 ......) = P(A_1) + P(A_2) + ......
\]</p>
<br>
<h3><a class="header" href="#14-mathematical-background" id="14-mathematical-background">1.4 Mathematical background</a></h3>
<h4><a class="header" href="#141-sets---a-collection-of-distinc-elements" id="141-sets---a-collection-of-distinc-elements">1.4.1 Sets - A collection of distinc elements</a></h4>
<ul>
<li>
<p>finite: e.g. {a, b, c, d}</p>
</li>
<li>
<p>infinite: the reals (R)</p>
</li>
<li>
<p>\( \Omega \) - the universal set</p>
</li>
<li>
<p>Ø - empty set</p>
</li>
</ul>
<p><em>What are reals?</em></p>
<p><em>The reals include rational numbers (terminating decimals and non-terminating recurring decimals and irrational numbers (non-terminating non-reccuring decimals</em></p>
<h4><a class="header" href="#142-unions-and-intersection" id="142-unions-and-intersection">1.4.2 Unions and intersection</a></h4>
<h4><a class="header" href="#143-de-morgans-law" id="143-de-morgans-law">1.4.3 De Morgans' Law</a></h4>
<ul>
<li>
<p>\( (S \cap T)^c = S^c \cup T^c \) and \( (S \cup T)^c = S^c \cap T^c \)</p>
</li>
<li>
<p>\( (S^c \cap T^c)^c = S \cup T \)</p>
</li>
</ul>
<h4><a class="header" href="#144-other-important-mathematical-backgrounds" id="144-other-important-mathematical-backgrounds">1.4.4 Other important mathematical backgrounds</a></h4>
<ul>
<li>
<p>Sequences and their limits </p>
<p><em>squence: an enumerated collection of objects</em></p>
</li>
<li>
<p>When does a sequence converge</p>
<ul>
<li>
<p>if \(a_i \leq a_{i+1}\)</p>
<ul>
<li>
<p>the sequence &quot;converge to \(\infty\)&quot;</p>
</li>
<li>
<p>the sequence converge to some real number a </p>
</li>
</ul>
</li>
<li>
<p>if \(|a_i - a| \leq b\), for \(b_i \to 0\), then \(a_i \to a\)</p>
</li>
</ul>
</li>
<li>
<p>Infinite series</p>
<p><em>series(infinte sums) vs. summation(finite sums)</em></p>
<p>\(\sum\limits_{n=1}^\infty a_i = \lim\limits_{n\to\infty}\sum\limits_{i=1}^n a_i\) </p>
<ul>
<li>
<p>\(a_i \leq 0\): limit exists</p>
</li>
<li>
<p>if term \(a_i\) do not all have the same sign:</p>
<p>a. limit does not exist</p>
<p>b. limit may exist but be different if we sum in a different order</p>
<p>c. <strong>Fact</strong>: limit exists and independent of order of summation if  \(\sum\limits_{n=1}^\infty |a_i| \leq \infty\) </p>
</li>
</ul>
</li>
<li>
<p>Geometric series (等比数列、等比级数)</p>
<p>\(\sum\limits_{i=0}^\infty a^i = 1 + a + a^2 + ...... = \frac{1}{1-a} \text{           |a| &lt; 1} \)</p>
</li>
</ul>
<h3><a class="header" href="#14-sets" id="14-sets">1.4 Sets</a></h3>
<h4><a class="header" href="#141-countable-and-uncountable-infinite-sets" id="141-countable-and-uncountable-infinite-sets">1.4.1 Countable and uncountable infinite sets</a></h4>
<ul>
<li>
<p>Countable</p>
<ul>
<li>
<p>integers, pairs of positive integers, etc.</p>
</li>
<li>
<p>rational numbers q (有理数), with 0 &lt; q &lt; 1</p>
</li>
</ul>
</li>
<li>
<p>Uncountable - <em>continuous numbers</em></p>
<ul>
<li>
<p>the interval [0, 1]</p>
</li>
<li>
<p>the reals, the plane, etc.</p>
</li>
</ul>
<p><em>How to prove the reals are uncountable - &quot;Control's diagonalization argument&quot;</em></p>
</li>
</ul>
<br>
<h2><a class="header" href="#unit-2-conditioning-and-independence" id="unit-2-conditioning-and-independence">Unit 2 Conditioning and independence</a></h2>
<p>Refer to Section 1.3 - 1.5 in the textbook</p>
<h3><a class="header" href="#21-conditional-and-bayes-rules" id="21-conditional-and-bayes-rules">2.1 Conditional and Bayes' Rules</a></h3>
<h4><a class="header" href="#211-the-definition-of-conditional-probability" id="211-the-definition-of-conditional-probability">2.1.1 The definition of conditional probability</a></h4>
<p>P(A|B) = &quot;probability of A, given that B occurred&quot;</p>
<p>\[
P(A|B) = \frac{P(A \cap B )}{P(B)}
\]</p>
<p>defined only when P(B) &gt; 0</p>
<h4><a class="header" href="#212-conditional-probabilities-share-properties-of-ordinary-probabilities" id="212-conditional-probabilities-share-properties-of-ordinary-probabilities">2.1.2 Conditional probabilities share properties of ordinary probabilities</a></h4>
<ul>
<li>
<p>\(P(A|B) \geq 0\)</p>
</li>
<li>
<p>\(P(\Omega|B) = 1\)</p>
</li>
<li>
<p>\(P(B|B) &lt; 0\)</p>
</li>
<li>
<p>If \(A \cap C = Ø\), then \(P(A \cup C|B) = P(A|B) + P(C|B)\) also only applies to countable and finite sequence (countable additivity axioms).</p>
</li>
</ul>
<h4><a class="header" href="#213-models-base-on-conditional-probabilities" id="213-models-base-on-conditional-probabilities">2.1.3 Models base on conditional probabilities</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/117574884-993c6600-b0df-11eb-9125-c5501b77d001.png" alt="image" /></p>
<p><strong>1. The multiplication rule</strong></p>
<pre><code>\\(P(A \cap B) = P(B)P(A|B) = P(A)P(B|A)\\)

\\(P(A^c \cap B \cap C^c) = P(A^c \cap B) P(C^c|A^c \cap B) = P(A^c) P(B|A^c) P(C^c|A^c \cap B)\\)

\\(P(A_1 \cap A_2...\cap A_n) = P(A_1) \prod\limits_{i=2}^n P(A_i|A_1 \cap A_2...\cap A_i)\\)
</code></pre>
<p><img src="https://user-images.githubusercontent.com/41487483/118396328-55051480-b64f-11eb-9c9e-9703528df69e.png" alt="image" /></p>
<p><strong>2. Total probability theorem</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118396377-9e556400-b64f-11eb-8e1a-f5530bb63fd3.png" alt="image" /></p>
<p><strong>3. Bayes' rules</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118396464-eeccc180-b64f-11eb-9080-e7e86ad223bf.png" alt="image" /></p>
<h3><a class="header" href="#22-independence" id="22-independence">2.2 Independence</a></h3>
<h4><a class="header" href="#221-conditional-independence" id="221-conditional-independence">2.2.1 Conditional independence</a></h4>
<p>Independent of two events</p>
<ul>
<li>
<p>Intuitive &quot;definition&quot;: P(B|A) = P(B) </p>
<ul>
<li>Occurence of A provides no new information about B</li>
</ul>
</li>
</ul>
<p>Definition of independence:</p>
<p>\(P(A \cap B) = P(A) \times P(B)\)</p>
<p><em>whether two events disjoined or joined is not associated with independence</em></p>
<p><strong>Independent of events complements</strong></p>
<p>If A and B are independent, then A and \(B^c\) are independent. </p>
<p><strong>Independent of events complements</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118397139-df9b4300-b652-11eb-9e1c-e8b293e070fc.png" alt="image" /></p>
<p><strong>Conditioning may affect independence</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118397436-35241f80-b654-11eb-89cf-d6eacefd924e.png" alt="image" /></p>
<h4><a class="header" href="#222-independence-of-a-collection-of-events" id="222-independence-of-a-collection-of-events">2.2.2 Independence of a collection of events</a></h4>
<ul>
<li>
<p>Intuitive &quot;definition&quot;: Information on some of the events does not change probabilities related to the remaining events</p>
</li>
<li>
<p>Definition: Events \(A_1, A_2,....., A_n\) are called independent if: </p>
<p>\(P(A_i \cap A_j \cap .... \cap A_m) = P(A_i)P(A_j)...P(A_m)\)</p>
</li>
</ul>
<p><strong>Pairwise independence</strong></p>
<p>n = 3:</p>
<p>\(P(A_1 \cap A_2) = P(A_1)P(A_2)\)</p>
<p>\(P(A_1 \cap A_3) = P(A_1)P(A_3)\)</p>
<p>\(P(A_2 \cap A_3) = P(A_2)P(A_3)\)</p>
<p>vs. 3-way indenpendence</p>
<p>\(P(A_1 \cap A_2 \cap A_3) = P(A_1)P(A_2)P(A_3)\)</p>
<p><strong>Independence vs. pairwise independence</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118398068-4d496e00-b657-11eb-91d3-ade5aa82f245.png" alt="image" /></p>
<h4><a class="header" href="#223-reliability" id="223-reliability">2.2.3 Reliability</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/118398123-7bc74900-b657-11eb-8f00-9313ffc249f6.png" alt="image" /></p>
<h2><a class="header" href="#unit-3-couting" id="unit-3-couting">Unit 3 Couting</a></h2>
<h3><a class="header" href="#31-basic-counting-principle" id="31-basic-counting-principle">3.1 Basic counting principle</a></h3>
<p>r stages and \(n_i\) choices at stage i give the total number of possible choices \( n_1 * n_2 * ....n_r \)</p>
<h3><a class="header" href="#32-permutation" id="32-permutation">3.2 Permutation</a></h3>
<ul>
<li><strong>Permutation</strong> - number of ways of ordering n elements (repetition is prohibited)</li>
</ul>
<p>\[n * (n-1) * (n-2) * ... * 2 * 1 = n!\]</p>
<ul>
<li>Number of subsets of {1, 2, ...n} = \(2^n\)</li>
</ul>
<h3><a class="header" href="#33-combinations" id="33-combinations">3.3 Combinations</a></h3>
<ul>
<li>
<p><strong>combinations</strong> \(\binom{n}{k}\)- number of <em>k</em>-element subsets of a given <em>n</em>-element set</p>
<p><em>How is combination equation derived?</em></p>
<p>Two ways of constructing an <strong>ordered</strong> sequence of <em>k</em> <strong>distinct</strong> items:</p>
<ul>
<li>
<p>choose the <em>k</em> items one at a time: </p>
<p>\[
n (n-1) ... (n-k+1) = \frac{n!}{k!(n-k)!}
\]</p>
</li>
<li>
<p>choose <em>k</em> items, then order them:</p>
<p>\[
\left(
\begin{array}{c}
n \\
k
\end{array}
\right)k!
\]</p>
</li>
</ul>
<p>There we have 
\[
\left(
\begin{array}{c}
n \\
k
\end{array}
\right) = \frac{n!}{k!(n-k)!}
\]</p>
</li>
</ul>
<h3><a class="header" href="#33-binominal-coeffficient" id="33-binominal-coeffficient">3.3 Binominal coeffficient</a></h3>
<ul>
<li>
<p><strong>Binominal coeffficient</strong> \(\binom{n}{k}\) - Binomial probabilities</p>
<p>Toss coins n times and each toss is given independent, P(Head) = p</p>
<p>\[
P(\text{k heads}) = \binom{n}{k}p^k (1-p)^{n-k}
\]</p>
<p>If asking P(k heads without ordered), then </p>
<p>\[
P(\text{k heads}) = p^k (1-p)^{n-k}
\]</p>
<p>Therefore, \(\binom{n}{k}\) is the number of <em>k</em>-head sequence</p>
</li>
</ul>
<h3><a class="header" href="#34-partitions" id="34-partitions">3.4 Partitions</a></h3>
<p><img src="https://user-images.githubusercontent.com/41487483/119098744-73d02600-ba16-11eb-8406-20acdf555e25.png" alt="image" /></p>
<ul>
<li>
<p><strong>multinomial coeffecient</strong> (number of partitions) =</p>
<p>\[
\frac{n!}{n_1! n_2! ... n_r!}
\]</p>
</li>
</ul>
<p>If r = 2, then \(n_1 = k\) and \(n_2 = n - k\). There is \(\frac{n!}{n! (n-k)!}\) which is \(\binom{n}{k}\)</p>
<ul>
<li>A simple example</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/119102618-83516e00-ba1a-11eb-86e6-b87e6ecf3467.png" alt="image" /></p>
<p><img src="https://user-images.githubusercontent.com/41487483/119102964-e8a55f00-ba1a-11eb-9693-d29eb512ac3f.png" alt="image" /></p>
<h2><a class="header" href="#4-discrete-random-variables" id="4-discrete-random-variables">4 Discrete random variables</a></h2>
<h3><a class="header" href="#41-probability-mass-function-pmf" id="41-probability-mass-function-pmf">4.1 Probability mass function (PMF)</a></h3>
<p><strong>Random variable</strong>(r.v.): a function from the sample space to the real numbers, notated as X.</p>
<p><strong>PMF</strong>: probability distribution of X </p>
<p>\[
p_X(x) = P(X = x) = P({w \in \Omega, s.t. X(\omega) = x})
\]</p>
<h3><a class="header" href="#42-discrete-random-variable-examples" id="42-discrete-random-variable-examples">4.2 Discrete Random variable examples</a></h3>
<h4><a class="header" href="#421-bernoulli-random-variables" id="421-bernoulli-random-variables">4.2.1 Bernoulli random variables</a></h4>
<p>with parameter \(p \in [0,1]\)</p>
<p>\[
p_X(x) = \begin{cases} 1, p(x) = p \\ 0, p(x) = 1 - p \end{cases} 
\]</p>
<ul>
<li>
<p>Models a trial that results in either success/failure, Heads/Tails, etc.</p>
</li>
<li>
<p><strong>Indicator random variables</strong> of an event A, \(I_A\) iff A occurs</p>
</li>
</ul>
<h4><a class="header" href="#422-uniform-random-variables" id="422-uniform-random-variables">4.2.2 Uniform random variables</a></h4>
<p>with paramters a,b</p>
<ul>
<li>
<p>Experiment: pick one of a, a+1 .... b at a random; all equally likely</p>
</li>
<li>
<p>Sample space; {a, a + 1, .... b}</p>
</li>
<li>
<p>Random variables X: \(X(\omega) = \omega\)</p>
</li>
</ul>
<h4><a class="header" href="#423-binomial-random-variables" id="423-binomial-random-variables">4.2.3 Binomial random variables</a></h4>
<p>with parameters: pasitive integer \(n; p \in [0,1]\)</p>
<ul>
<li>
<p>Experiment: n independent toses of a coin with P(Heads) = p</p>
</li>
<li>
<p>Sample space: set of sequences of H and T of length n</p>
</li>
<li>
<p>Random variables X: number of Heads observed</p>
</li>
<li>
<p>Model of: number of successes in a given number of independent trials </p>
</li>
</ul>
<p>\[
p_X(k) = \left(\begin{array}{c} n \\ k \end{array}  \right)p^k(1-p)^{n-k}, k = 0, 1 ..., n
\]</p>
<h4><a class="header" href="#424-geometric-random-variables" id="424-geometric-random-variables">4.2.4 Geometric random variables</a></h4>
<p>with parameter p: 0 &lt; p ≤ 1</p>
<ul>
<li>
<p>Experiment: infinitely many independent tosses of a coin: P(Heads) = p</p>
</li>
<li>
<p>Random variable X: number of tosses until the first Heads</p>
</li>
<li>
<p>Model of waiting times; number of tirals until a success</p>
</li>
</ul>
<p>\[
p_X(k) = P(X = k) = P(T...TH) =(1-p)^{k-1}p, k = 1,2,3...<br />
\]</p>
<h3><a class="header" href="#43-expectationmean-of-a-random-variable" id="43-expectationmean-of-a-random-variable">4.3 Expectation/mean of a random variable</a></h3>
<ul>
<li>
<p><strong>Definition</strong>: </p>
<p>\[
E[X] = \sum\limits_{x} xp_X(x)
\]</p>
</li>
<li>
<p>Interpretation: average in large number of independet repetitions of the experiment</p>
</li>
<li>
<p>Elementary properties</p>
<ul>
<li>
<p>If X ≥ 0, then E(X) ≥ 0</p>
</li>
<li>
<p>If a ≤ X ≤ b, then a ≤ E[X] ≤ b</p>
</li>
<li>
<p>If c is a constant, E[c] = c</p>
</li>
<li>
<p>The expected value rule: </p>
<p>\[
E[Y] = \sum\limits_y yp_Y(y) = E[g(X)] = \sum\limits_x g(x)p_X(x)<br />
\]</p>
</li>
<li>
<p>Linearity of expectation: \(E[aX+b] = aE[X] + b\)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#44-variance---a-measure-of-the-spread-of-a-pmf" id="44-variance---a-measure-of-the-spread-of-a-pmf">4.4 Variance - a measure of the spread of a PMF</a></h3>
<h4><a class="header" href="#441-definition-of-variance" id="441-definition-of-variance">4.4.1 Definition of variance:</a></h4>
<p>\[
var(X) = E[(X - \mu)^2] = \sum\limits_x (x - \mu)^2 p_X(x)
\]</p>
<p>standard deviation: \(\sigma_X = \sqrt{var(X)}\)</p>
<h4><a class="header" href="#442-properties-of-the-variance" id="442-properties-of-the-variance">4.4.2 Properties of the variance</a></h4>
<ul>
<li>
<p>Notation: \(\mu = E[X] \)</p>
</li>
<li>
<p>\(var(aX + b) = a^2var(X)\)</p>
</li>
<li>
<p>A useful formula:</p>
<p>\[
var(X) = E(X^2) - (E[X])^2<br />
\]</p>
</li>
</ul>
<p><strong>Summary of Expectation and Variance of Discrete Random Variables</strong></p>
<table><thead><tr><th>Random Variables</th><th align="center">Formula</th><th align="right">E(X)</th><th align="right">var(X)</th></tr></thead><tbody>
<tr><td>Bernoulli (p)</td><td align="center">\(p_X(x) = \begin{cases} 1, p(x) = p \\ 0, p(x) = 1 - p \end{cases} \)</td><td align="right">\(p\)</td><td align="right">\(p(1-p)\)</td></tr>
<tr><td>Uniform (a,b)</td><td align="center">\(p_X(x) = \frac{1}{b-a}, a ≤ x ≤ b\)</td><td align="right">\(\frac{a+b}{2}\)</td><td align="right">\(\frac{1}{12}(b-a)(b-a-2)\)</td></tr>
<tr><td>Binomial \(p \in [0,1]\)</td><td align="center">\(p_X(k) = \left(\begin{array}{c} n \\ k \end{array}  \right)p^k(1-p)^{n-k}, k = 0, 1 ..., n\)</td><td align="right">\( np \)</td><td align="right">\(np(1-p)\)</td></tr>
<tr><td>Geometric  \(0 &lt; p ≤ 1\)</td><td align="center">\(p_X(k) = (1-p)^{k-1}p, k = 1,2,3.... \)</td><td align="right">\(\frac{1}{p}\)</td><td align="right">\(\)</td></tr>
</tbody></table>
<h3><a class="header" href="#45-conditional-pmf-and-expectation-given-an-event" id="45-conditional-pmf-and-expectation-given-an-event">4.5 Conditional PMF and expectation, given an event</a></h3>
<h4><a class="header" href="#451-conditional-pmfs" id="451-conditional-pmfs">4.5.1 Conditional PMFs</a></h4>
<p>\(p_{X|A}(x|A) = P(X = x|A)\), given A = {Y = y}</p>
<p>\[
p_{X|Y}(x|y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}<br />
\]</p>
<h4><a class="header" href="#452-conditional-pmfs-involing-more-than-two-random-variables" id="452-conditional-pmfs-involing-more-than-two-random-variables">4.5.2 Conditional PMFs involing more than two random variables</a></h4>
<ul>
<li>
<p>\(p_{X|Y,Z}(x|y,z) = P(X = x|Y = y, Z = z) = \frac{P(X=x,Y=y,Z=z)}{P(Y=y, Z=z)} = \frac{P_{X,Y,Z}(x,y,z)}{P_{Y,Z}(y,z)} \)</p>
</li>
<li>
<p>Multiplication rules: \(p_{X,Y,Z}(x,y,z) = p_X(x)p_{Y|X}(y|x)p_{Z|X,Y}(z|x,y) \)</p>
</li>
<li>
<p>Total probability and expectation theorems</p>
<p>\(p_X(x) = P(A_1)p_{X|A_1}(x) + ... + P(A_n)p_{X|A_n}(x) \implies p_X(x) = \sum\limits_y p_Y(y)p_{X|Y}(x|y)\)</p>
<p>\(E[X] = P(A_1)E[X|A_1] + ... + P(A_n)E[X|A_n] \implies E[X] = \sum\limits_y p_Y(y) E[X|Y = y]\)</p>
</li>
</ul>
<h3><a class="header" href="#46-multiple-random-variables-and-joint-pmfs" id="46-multiple-random-variables-and-joint-pmfs">4.6 Multiple random variables and joint PMFs</a></h3>
<h4><a class="header" href="#461-joint-pmf" id="461-joint-pmf">4.6.1 Joint PMF</a></h4>
<p>\[
p_{X,Y}(x,y) = P(X = x, Y =y)
\]</p>
<ul>
<li>
<p>\(\sum\limits_x \sum\limits_y p_{X,Y}(x,y) = 1\)</p>
</li>
<li>
<p>Marginal PMFs: \(p_X(x) = \sum\limits_y p_{X,Y}(x,y)\)</p>
<p>\(p_Y(y) = \sum\limits_x p_{X,Y}(x,y)\)</p>
</li>
</ul>
<h4><a class="header" href="#462-functions-of-multiple-random-variables" id="462-functions-of-multiple-random-variables">4.6.2 Functions of multiple random variables</a></h4>
<p>\(Z = g(X,Y)\)</p>
<ul>
<li>
<p>PMF: \(p_Z(z) = P(Z=z) =P(g(X,Y) = z) \)</p>
</li>
<li>
<p>Expected value rules: \(E[g(X,Y)] = \sum\limits_x \sum\limits_y g(x,y) p_{X,Y}(x,y)\)</p>
</li>
<li>
<p>Linearity of expectations</p>
<ul>
<li>
<p>\(E[aX + b] = aE[X] + b\)</p>
</li>
<li>
<p>\(E[X + Y] = E[X] + E[Y]\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#463-independence-of-multiple-random-variables" id="463-independence-of-multiple-random-variables">4.6.3 Independence of multiple random variables</a></h4>
<ul>
<li>
<p>\(P(X = x and Y = y) = P(X = x) \times P(Y = y), for all x, y \)</p>
</li>
<li>
<p>\(P_{X|Y}(x|y) = P_X(x)\) and \(P_{Y|X}(y|x) = P_Y(y)\)</p>
</li>
<li>
<p><strong>Independence and expectations</strong></p>
<ul>
<li>
<p>In general, \(E[g(X,Y)] \neq g(E[X], E[Y])\)</p>
</li>
<li>
<p>If X, Y are independent: \(E[XY] = E[X]E[Y]\)</p>
<p>g(X) and h(Y) are also independent: \(E[g(X)h(Y)] = E[g(X)]E[h(Y)]\)</p>
</li>
</ul>
</li>
<li>
<p><strong>Independence and variances</strong></p>
<ul>
<li>
<p>Always true: \(var(aX) = a^2var(X)\) and \(var(X+a) = var(X)\)</p>
</li>
<li>
<p>In general: \(var(X+Y) \neq var(X) + var(Y)\)</p>
</li>
<li>
<p>If X, Y are independent, \(var(X,Y) = var(X) + var(Y)\)</p>
</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#5-continuous-random-variables" id="5-continuous-random-variables">5 Continuous random variables</a></h2>
<h3><a class="header" href="#51-probability-density-function-pdfs" id="51-probability-density-function-pdfs">5.1 Probability density function (PDFs)</a></h3>
<h4><a class="header" href="#511-definition" id="511-definition">5.1.1 Definition</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/120918805-91f48200-c6b6-11eb-9319-23713295480c.png" alt="image" /></p>
<p><em>PDFs are not probabilities. Their units are probability per unit length.</em></p>
<p><strong>Contiunous random variables</strong>: a random variable is continuous <strong>if it can be described by a PDF</strong>.</p>
<ul>
<li>
<p>\(P(X = a) = 0\)</p>
</li>
<li>
<p>\(f_X(x) \geq 0\)</p>
</li>
<li>
<p>\(\int_{-\infty}^{+\infty}f(x)dx = 1\)</p>
</li>
</ul>
<p><strong>Expectation/Mean</strong></p>
<p>Expection/mean of a continuous random variable: <em>average in large number of independent repetitions of the experiment</em></p>
<p>\[
E[X] = \int_{-\infty}^{+\infty}xf_X(x)dx
\]</p>
<p><strong>Properties of expectations</strong></p>
<ul>
<li>
<p>if X ≥ 0, then \(E[X] ≥ 0\)</p>
</li>
<li>
<p>if a ≤ X ≤ b, then \(a ≤ E[X] ≤ b\)</p>
</li>
<li>
<p>Expected value rule: \(E[g(X)] = \sum\limits_{x} g(x) f_X(x) dx \)</p>
</li>
<li>
<p>Linearity: \(E[aX + b] = aE(X) + b\)</p>
</li>
</ul>
<p><strong>Variance</strong></p>
<p>According to the definition of variance: \(var(X) = E[(X - \mu)^2] \)</p>
<p>\[
var(X) = \int_{-\infty}^{+\infty} (x - \mu)^2 f_X(x) dx
\] </p>
<ul>
<li>
<p>Standard deviation = \(\sigma_X = \sqrt{var(X)} \)</p>
</li>
<li>
<p>\(var(aX + b) = a^2 var(X)\)</p>
</li>
<li>
<p>\(var(X) = E[X^2] - (E[X])^2\)</p>
</li>
</ul>
<p><strong>Summary of Expectation and Variance of continuous random variables</strong></p>
<table><thead><tr><th>Random Variables</th><th align="center">Formula</th><th align="right">E(X)</th><th align="right">var(X)</th></tr></thead><tbody>
<tr><td>Uniform</td><td align="center">\(f(x) = \frac{1}{b-a}, a ≤ x ≤ b\)</td><td align="right">\(\frac{a+b}{2}\)</td><td align="right">\(\frac{(b-a)^2}{12}\)</td></tr>
<tr><td>Exponential \( \lambda &gt; 0 \)</td><td align="center">\(f(x) = \begin{cases} \lambda e^{-\lambda x}, x ≥ 0 \\ 0, x &lt; 0 \end{cases}\)</td><td align="right">\(\frac{1}{\lambda}\)</td><td align="right">\(\frac{1}{\lambda^2}\)</td></tr>
</tbody></table>
<h4><a class="header" href="#512-cumulative-distribution-functions-cdf" id="512-cumulative-distribution-functions-cdf">5.1.2 Cumulative distribution functions (CDF)</a></h4>
<p>CDF defination: \(F_X(x) = P(X ≤ x )\)</p>
<ul>
<li>
<p>Non-decreasing</p>
</li>
<li>
<p>\(F_X(x)\) tends to 1, as \(x \to \infty\)</p>
</li>
<li>
<p>\(F_X(x)\) tends to 0, as \(x \to - \infty\)</p>
</li>
</ul>
<h4><a class="header" href="#513-normalgaussian-random-variables" id="513-normalgaussian-random-variables">5.1.3 Normal(Gaussian) random variables</a></h4>
<ol>
<li>
<p>Standard normal(Gaussian) random variables</p>
<p>Stardard  normal \(N(0,1): f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \)</p>
<ul>
<li>
<p>\(E[X] = 0\)</p>
</li>
<li>
<p>\(var(X) = 1\)</p>
</li>
</ul>
</li>
<li>
<p>General normal(Gaussian) random variables</p>
<p>General  normal \(N(\mu,\sigma^2): f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}, \sigma &gt; 0 \)</p>
<ul>
<li>
<p>\(E[X] = \mu \)</p>
</li>
<li>
<p>\( var(X) = \sigma^2 \)</p>
</li>
</ul>
<pre><code> \\( \sigma^2 \to small\\), the shape of normal distribution becomes more narrow.
</code></pre>
</li>
<li>
<p>Linear functions of a normal random variable</p>
<ul>
<li>
<p>Let \(Y = aX + b, X \sim N(\mu, \sigma^2)\)</p>
<p>\(E[Y] = a\mu + b\)</p>
<p>\(Var(Y) = a^2 \sigma^2 \)</p>
</li>
<li>
<p>Fact: \(Y \sim N(a\mu + b, a^2 \sigma^2)\)</p>
</li>
<li>
<p>Special case: a = 0. There is Y = b, \(N(b, 0)\)</p>
</li>
</ul>
</li>
</ol>
<h4><a class="header" href="#514-calculation-of-normal-probabilities" id="514-calculation-of-normal-probabilities">5.1.4 Calculation of normal probabilities</a></h4>
<ol>
<li>
<p><strong>Standard normal tables</strong> </p>
<p>\(\Phi(y) = F_Y(y) = P(Y \leq y)\) which can be find in the table, where y ≥ 0.</p>
</li>
<li>
<p>Standardizing a random variable</p>
<p>\(X \sim N(\mu, \sigma^2), \sigma^2 &gt; 0 \)</p>
<p>\(Y = \frac{X - \mu}{\sigma}\)</p>
</li>
</ol>
<h3><a class="header" href="#52-conditioning-on-an-event-multiple-continuous-rvs" id="52-conditioning-on-an-event-multiple-continuous-rvs">5.2 Conditioning on an event: multiple continuous r.v.'s</a></h3>
<p>\[
P( X \in B|A) =  \int_B f_{X|A}(x)dx
\]</p>
<h4><a class="header" href="#521-conditional-pdf-of-x-given-that-x-in-a-" id="521-conditional-pdf-of-x-given-that-x-in-a-">5.2.1 Conditional PDf of X, given that \(X \in A \)</a></h4>
<p>\[
f_{X|X \in A}(x) = \begin{cases} 0, if x \notin A \\ \frac{f_X(x)}{P(A)}, if x \in A \end{cases}
\]</p>
<h4><a class="header" href="#522-conditional-expectation-of-x-given-an-event" id="522-conditional-expectation-of-x-given-an-event">5.2.2 Conditional expectation of X, given an event</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121066009-b5f4b800-c7c9-11eb-9e82-cca4ded3a17f.png" alt="image" /></p>
<h4><a class="header" href="#523-memorylessness-of-the-exponential-pdf" id="523-memorylessness-of-the-exponential-pdf">5.2.3 Memorylessness of the exponential PDF</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121071392-2a325a00-c7d0-11eb-8e77-c14ded14fe0c.png" alt="image" /></p>
<h4><a class="header" href="#524-total-probability-and-expectation-theorems" id="524-total-probability-and-expectation-theorems">5.2.4 Total probability and expectation theorems</a></h4>
<ul>
<li>Probability theorem: </li>
</ul>
<p>\[
P(B) = P(A_1)P(B|A_1) + \dotsb + P(A_n)P(B|A_n)
\]</p>
<ul>
<li>For the discrete random variable: </li>
</ul>
<p>\[
p_X(x) = P(A_1)p_{X|A_1}(x) + \dotsb + P(A_n)p_{X|A_n}(x)
\]</p>
<ul>
<li>For CDF: </li>
</ul>
<p>\[
F_X(x) = P(X \leq x) = P(A_1)P(X \leq x | A_1) + \dotsb + P(A_n)P(X \leq x | A_n) 
\\= P(A_1)F_{X|A_1}(x) + \dotsb + P(A_n)F_{X|A_n}(x)
\]</p>
<ul>
<li>For PDF, the derivative of CDF:</li>
</ul>
<p>\[
f_X(x) = P(X \leq x) = P(A_1)f_{X|A_1}(x) + \dotsb + P(A_n)f_{X|A_n}(x)
\]</p>
<ul>
<li>Integral above equation, we will obtain the expectation equation:</li>
</ul>
<p>\[
\int xf_X(x)dx = P(A_1) \int xf_{X|A_1}(x)dx + \dotsb + P(A_n) \int xf_{X|A_n}(x)dx
\]</p>
<p>\[
E[X] = P(A_1)E[X|A_1] + \dotsb + P(A_n)E[X|A_n]<br />
\]</p>
<h3><a class="header" href="#53-mixed-random-varibles" id="53-mixed-random-varibles">5.3 Mixed random varibles</a></h3>
<h4><a class="header" href="#531-mixed-distirbutions" id="531-mixed-distirbutions">5.3.1 Mixed distirbutions</a></h4>
<p>\[
X = \begin{cases} Y, \text{with probability } p \text{ (Y discrete)}\\ Z, \text{with probability } 1-p \text{ (Z continuous)} \end{cases}<br />
\]</p>
<ul>
<li>
<p>do not have PDF or PMF but can be defined with CDF and expectation</p>
<p>\[
F_X(x) = p P(Y \leq x) + (1-p) P(Z \leq x)
\\
=pF_Y(x) + (1-p)F_Z(x)
\\
= E[X] = p E[Y] + (1-p) E[Z]
\]</p>
<p><img src="https://user-images.githubusercontent.com/41487483/121077061-6fa65580-c7d7-11eb-84f5-a7c1d5c36df3.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#532-joint-pdfs" id="532-joint-pdfs">5.3.2 Joint PDFs</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121728365-75f54400-caed-11eb-9935-66e24af94023.png" alt="image" /></p>
<ul>
<li>
<p>Joint PDFs are denoted as \(f_{X,Y}(x,y)\): probaility per unit area</p>
<p>When X = Y, equal to a line, meaning X and Y are not joint PDFs.</p>
</li>
</ul>
<h4><a class="header" href="#533-from-the-joint-to-the-marginal" id="533-from-the-joint-to-the-marginal">5.3.3 From the joint to the marginal</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121731040-b73b2300-caf0-11eb-8a17-90bb4190224e.png" alt="image" /></p>
<h4><a class="header" href="#534-joint-cdf" id="534-joint-cdf">5.3.4 Joint CDF</a></h4>
<p>\[
F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \int\limits_{-\infty}^{y} \int\limits_{-\infty}^{x} f_{x,y}(s,t)dsdt 
\]</p>
<h3><a class="header" href="#54-conditioning-on-a-random-variable-and-bayers-rule" id="54-conditioning-on-a-random-variable-and-bayers-rule">5.4 Conditioning on a random variable and Bayers rule</a></h3>
<h4><a class="header" href="#541-conditional-pdfs-given-another-rv" id="541-conditional-pdfs-given-another-rv">5.4.1 Conditional PDFs, given another r.v.</a></h4>
<ul>
<li>
<p>\(f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}\), if \(f_y(y) &gt; 0\)</p>
<ul>
<li>
<p>\(f_{X|Y}(x|y) \geq 0\)</p>
</li>
<li>
<p>Think of value of Y as fixed at some y shape of \(f_{X|Y}(\cdot|y)\): slice of the joint</p>
</li>
<li>
<p>multiplication rule: </p>
<p>\[
f_{X|Y}(x,y) = f_Y(y) \cdot f_{X|Y}(x|y)<br />
\]</p>
</li>
</ul>
</li>
<li>
<p>\(P(X \in A | Y = y) = \int_A f_{X|Y}(x/y)dx\)</p>
</li>
</ul>
<h4><a class="header" href="#542-total-probability-and-expectation-theorems" id="542-total-probability-and-expectation-theorems">5.4.2 Total probability and expectation theorems</a></h4>
<ul>
<li>
<p>Anolog to the PMFs of discrete randome varable \(p_X(x) = \sum\limits_y p_Y(y)p_{X|Y}(x|y)\)</p>
<p>For continuous r.v., there is </p>
<p>\[
f_X(x) = \sum_{-\infty}^{\infty} f_Y(y)f_{X|Y}(x|y)dy<br />
\]</p>
</li>
<li>
<p>Anolog to the Expectation of discrete randome varable \(E[X|Y=y] = \sum\limits_x x p_{X|Y}(x|y)\)</p>
<p>For continuous r.v., there is </p>
<p>\[
E[X|Y=y] = \int_{-\infty}^{\infty} xf_{X|Y}(x|y)dx<br />
\]</p>
</li>
<li>
<p>Anolog to the discrete randome varable \(E[X] = \sum\limits_y p_Y(y) E[X|Y=y]\)</p>
<p>For continuous r.v., there is </p>
<p>\[<br />
E[X] = \int_{-\infty}^{\infty} f_Y(y)E[X|Y=y]dy<br />
\\ = \int_{-\infty}^{\infty} xf_X(x)dx<br />
\]</p>
</li>
<li>
<p>Expected value rule</p>
<p>\[
E[g(X)|Y=y] = \int_{-\infty}^{\infty} g(x)f_{X|Y}(x|y)dx<br />
\]</p>
</li>
</ul>
<h4><a class="header" href="#543-independence" id="543-independence">5.4.3 Independence</a></h4>
<p>\[
f_{X,Y}(x,y) = f_X(x)f_Y(y), for all x and y<br />
\]</p>
<ul>
<li>
<p>\(f_{X,Y}(x,y) = f_X(x)\), for all y with \(f_Y(y) &gt; 0\) and all x</p>
</li>
<li>
<p>If X, Y are <strong>independent</strong>:</p>
<p>\[
E[XY] = E[X]E[Y] \\
var(X + Y) = var(X) + var(Y)
\]</p>
<p>g(X) and h(Y) are also independent: \(E[g(X)h(Y)] = E[g(X)] \cdot E[h(Y)]\)</p>
</li>
</ul>
<h4><a class="header" href="#544-the-bayes-rule-----a-theme-with-variations" id="544-the-bayes-rule-----a-theme-with-variations">5.4.4 The Bayes rule --- a theme with variations</a></h4>
<ul>
<li>
<p>For discrete r.v., </p>
<ul>
<li>
<p>\(p_{X|Y}(x|y) = \frac{p_X(x) p_{Y|X}(y|x)}{p_Y(y)}\)</p>
</li>
<li>
<p>\(p_Y(y) = \sum\limits_{x'} p_X(x')p_{Y|X}(y|x')\)</p>
</li>
</ul>
</li>
<li>
<p>For continuous r.v., </p>
<ul>
<li>
<p>\(f_{X|Y}(x|y) = \frac{f_X(x) f_{Y|X}(y|x)}{_Y(y)}\)</p>
</li>
<li>
<p>\(p_Y(y) = \int\limits f_X(x')f_{Y|X}(y|x')\)</p>
</li>
</ul>
</li>
<li>
<p>One discrete and one continuous r.v.</p>
<p><img src="https://user-images.githubusercontent.com/41487483/121778872-456ae400-cb99-11eb-8fc6-d1cb9ea4f3f2.png" alt="image" /></p>
</li>
</ul>
<h2><a class="header" href="#unit-6-further-topics-on-random-variables" id="unit-6-further-topics-on-random-variables">Unit 6 Further topics on random variables</a></h2>
<h3><a class="header" href="#61-derived-distributions" id="61-derived-distributions">6.1 Derived distributions</a></h3>
<h4><a class="header" href="#611-a-linear-function-y---ax--b" id="611-a-linear-function-y---ax--b">6.1.1 A linear function \(Y =  aX + b\)</a></h4>
<ul>
<li>
<p>Discrete r.v. </p>
<p>\(
p_Y(y) = p_X(\frac{y-b}{a})
\)</p>
</li>
<li>
<p>Continuous r.v.</p>
<p>\(
f_Y(y) = \frac{1}{|a|}f_X(\frac{y-b}{a})
\)</p>
<ul>
<li>
<p>A linear function of normal r.v. is normal</p>
<p>If \(X \sim N(\mu, \sigma^2)\), then \(aX + b \sim N(a\mu + b, a^2\sigma^2)\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#612-a-general-function-gx-of-a-continuous-rv" id="612-a-general-function-gx-of-a-continuous-rv">6.1.2 A general function \(g(X)\) of a continuous r.v.</a></h4>
<p><strong>Two-step procedure:</strong></p>
<ul>
<li>
<p>Find the CDF of Y: \(F_Y(y) = P(Y \leq y) = P(g(x) \leq y)\) and the valid range of y</p>
</li>
<li>
<p>Differentiate: \(f_Y(y) = \frac{dF_Y(y)}{dy}\)</p>
</li>
</ul>
<ol>
<li>
<p>A general formula for the PDF of \(Y = g(X)\) when <em>g</em> is monotomic</p>
<p>\[
f_Y(y) = f_X(h(y))\left|\frac{dh(y)}{dy}\right|<br />
\]</p>
<p>\(x = h(y)\) is the inverse function of \(y = g(x)\)</p>
</li>
<li>
<p>A nonmonotonic example \(Y = X^2\)</p>
<ul>
<li>
<p>the discrete case: \(p_Y(y) = p_X(\sqrt{y}) + p_X(-\sqrt{y})\)</p>
</li>
<li>
<p>the continuous case: \(f_Y(y) = f_X(\sqrt{y})\frac{1}{2\sqrt{y}} + p_X(-\sqrt{y})\frac{1}{2\sqrt{y}}\)</p>
</li>
</ul>
</li>
<li>
<p>A function of multiple r.v.'s: \(Z = g(X,Y)\)</p>
<p><img src="https://user-images.githubusercontent.com/41487483/122451427-69b72e00-cfa8-11eb-9aa5-b1c0855886d0.png" alt="image" /></p>
</li>
</ol>
<h3><a class="header" href="#62-sums-of-independent-vadom-variables" id="62-sums-of-independent-vadom-variables">6.2 Sums of independent vadom variables</a></h3>
<h4><a class="header" href="#621-the-distribution-of-x--y-the-discrete-case" id="621-the-distribution-of-x--y-the-discrete-case">6.2.1 The distribution of \(X + Y\): the discrete case</a></h4>
<p>Z = X + Y; X,Y independent, discrete known PMFs</p>
<p>\[
p_Z(z) = \sum\limits_x p_X(x)p_Y(z-x)<br />
\]</p>
<p><strong>Dsicrete convoltion mechanics</strong></p>
<ol>
<li>
<p>Flip the PMF of Y and put it underneath the PMF of X</p>
</li>
<li>
<p>Shift the flipped PMF by z </p>
</li>
<li>
<p>Cross-multiply and add</p>
</li>
</ol>
<h4><a class="header" href="#622-the-distribution-of-x--y-the-continous-case" id="622-the-distribution-of-x--y-the-continous-case">6.2.2 The distribution of \(X + Y\): the continous case</a></h4>
<p>Z = X + Y; X,Y independent, continuous known PDFs</p>
<p>\[
f_Z(z) = \int\limits_x f_X(x)f_Y(z-x)dx<br />
\]</p>
<ul>
<li>
<p>conditional on \(X = x\): </p>
<p>\[
f_{Z|x}(z|x) = f_Y(z-x)<br />
\]</p>
<p>which can then be used to calculate Joint PDF of Z and X and marginal PDF of Z.</p>
</li>
<li>
<p>Same mechanics as in discrete case</p>
</li>
</ul>
<h4><a class="header" href="#623-the-sum-of-independent-normal-rvs" id="623-the-sum-of-independent-normal-rvs">6.2.3 The sum of independent normal r.v.'s</a></h4>
<ul>
<li>
<p>\(X \sim N(\mu_x, \sigma_x^2), Y \sim N(\mu_y, \sigma_y^2\) <strong>Independent</strong></p>
<p>\(Z = X + Y: \sim N(N(\mu_x + \mu_y,  \sigma_x^2 + \sigma_y^2))\)</p>
<p><strong>The sum of finitely many independent normals is normal</strong></p>
</li>
</ul>
<h3><a class="header" href="#63-covariance-协方差" id="63-covariance-协方差">6.3 Covariance (协方差)</a></h3>
<h4><a class="header" href="#631-definition" id="631-definition">6.3.1 Definition</a></h4>
<p>\[
cov(X,Y) = E[(X - E[X]) \cdot (Y - E(Y))]<br />
\]</p>
<ul>
<li>If \(X,Y\) <strong>independent: \(cov(X,Y) = 0 \)</strong> </li>
</ul>
<p><em>convers is not true!</em></p>
<h4><a class="header" href="#632-covariance-properties" id="632-covariance-properties">6.3.2 Covariance properties</a></h4>
<ol>
<li>
<p>\(cov(X,X) = var(X) = E[X^2] - (E[X])^2\)</p>
</li>
<li>
<p>\(cov(aX+b,Y) = a \cdot cov(X,Y)\)</p>
</li>
<li>
<p>\(cov(X,Y+Z) = cov(X,Y) + cov(X,Z)\)</p>
</li>
</ol>
<p><strong>Practical covariance formula:</strong> </p>
<p>\[
cov(X,Y) = E[XY] - E[X]E[Y]
\]</p>
<h4><a class="header" href="#633-the-variance-of-a-sum-of-random-variables" id="633-the-variance-of-a-sum-of-random-variables">6.3.3 The variance of a sum of random variables</a></h4>
<ul>
<li>
<p>two r.v.s</p>
<p>\[
var(X_1 + X_2) = var(X_1) + var(X_2) + 2cov(X_1,X_2)<br />
\]</p>
<p><em>X,Y indepedent, then \(var(X_1 + X_2) = var(X_1) + var(X_2)\)</em></p>
</li>
<li>
<p>multiple r.v.s</p>
<p>\[
var(X_1 + \dots + X_n) = \sum\limits_{i=1}^nvar(X_i) + \sum\limits_{(i,j):i \neq j}^n cov(X_i,X_j)<br />
\]</p>
<p><em>\(\sum\limits_{(i,j):i \neq j}^n \) contains \((n^2 - n)\) terms</em></p>
</li>
</ul>
<h3><a class="header" href="#64-the-correlation-coefficient" id="64-the-correlation-coefficient">6.4 The correlation coefficient</a></h3>
<p>\[
\rho(X,Y) = E\left[\frac{(X - E[X])}{\sigma_X} \cdot \frac{(Y - E[Y])}{\sigma_Y}\right] = \frac{cov(X,Y)}{\sigma_X \sigma_Y}
\]</p>
<h4><a class="header" href="#641-interpretation-of-correlation-coeffecient" id="641-interpretation-of-correlation-coeffecient">6.4.1 Interpretation of correlation coeffecient</a></h4>
<ul>
<li>
<p>Dimensionless version of covariance </p>
</li>
<li>
<p>Measure of the defree of &quot;association&quot; between X and Y</p>
</li>
<li>
<p>Association does not imply causation or influence</p>
</li>
<li>
<p>Correlation often refleces underlying, common, hidden factor</p>
</li>
</ul>
<h4><a class="header" href="#642-key-properties-of-the-correlation-coeffecient" id="642-key-properties-of-the-correlation-coeffecient">6.4.2 Key properties of the correlation coeffecient</a></h4>
<ul>
<li>
<p>\(-1 \leq \rho \leq 1\)</p>
</li>
<li>
<p><strong>Independent</strong> \(\implies \rho = 0\) &quot;uncorrelated&quot; (converse is not true)</p>
</li>
<li>
<p>\(|\rho| = 1 \Leftrightarrow\) linearly related</p>
</li>
<li>
<p>\(cov(aX+b, Y) = a \cdot cov(X,Y) \implies \rho(aX+b,Y) = sigma(a)\rho(X,Y)\)</p>
</li>
</ul>
<h3><a class="header" href="#65-conditional-expectation-and-variance-as-a-random-variable" id="65-conditional-expectation-and-variance-as-a-random-variable">6.5 Conditional expectation and variance as a random variable</a></h3>
<h4><a class="header" href="#651-conditional-expecation" id="651-conditional-expecation">6.5.1 Conditional expecation</a></h4>
<ul>
<li><strong>Definition</strong>: \(g(Y)\) is the <em>random variable</em> that takes the value \(E[X|Y=y]\), if \(Y\) happens to take the value \(y\).</li>
</ul>
<p>\[
E[X|Y] = g(Y)<br />
\]</p>
<ul>
<li><strong>Law of iterated expectations</strong></li>
</ul>
<p>\[
E[E[X|Y]] = E[g(Y)] = E[X]<br />
\]</p>
<h4><a class="header" href="#652-conditional-variance" id="652-conditional-variance">6.5.2 Conditional variance</a></h4>
<ul>
<li>
<p>Variance fundamentals</p>
<p>\[
var(X) = E[(X - E[X])^2] \\
var(X|Y=y) = E[(X - E[X|Y=y])^2|Y=y]
\]</p>
</li>
</ul>
<p><strong>var(X|Y) is the r.v. that takes the value var(X|Y=y), when Y=y</strong></p>
<ul>
<li>
<p><strong>Law of total variance</strong></p>
<p>\[
var(X) = E[var(X|Y)] + var(E[X|Y])<br />
\]</p>
<p><em>var(X) = (average variability within sections) + (variability between sections)</em></p>
</li>
</ul>
<h3><a class="header" href="#66-sum-a-random-number-of-indepedent-rvs" id="66-sum-a-random-number-of-indepedent-rvs">6.6 Sum a random number of indepedent r.v.'s</a></h3>
<p><em>Example of shopping</em></p>
<ul>
<li>
<p>N: number of stores visited (N is a nonnegative integer r.v.)</p>
</li>
<li>
<p>\(X_i\): money spent in store i </p>
<ul>
<li>
<p>\(X_i\) independent, identically distributed</p>
</li>
<li>
<p>independent of N</p>
</li>
</ul>
</li>
<li>
<p>Let \(Y = X_1 + \dots + X_N\)</p>
</li>
</ul>
<h4><a class="header" href="#661-expecatation-of-the-sum" id="661-expecatation-of-the-sum">6.6.1 Expecatation of the sum</a></h4>
<p>Based on the Law of iterated expectations:</p>
<p>\[
E[Y] = E[E[Y|N]] = E[N \cdot E[X]] = \cdot E[X]E[N]<br />
\]</p>
<h4><a class="header" href="#662-variance-of-the-sum" id="662-variance-of-the-sum">6.6.2 Variance of the sum</a></h4>
<p>Based on the Law of total variance: \(var(Y) = E[var(Y|N)] + var(E[Y|N])\):</p>
<p>\[
var(Y) = E[N]var(X) + (E[X])^2var(N) 
\]</p>
<h2><a class="header" href="#unit-7-bayesian-inferences" id="unit-7-bayesian-inferences">Unit 7 Bayesian inferences</a></h2>
<h3><a class="header" href="#71-introduction-to-bayesian-inference" id="71-introduction-to-bayesian-inference">7.1 Introduction to Bayesian inference</a></h3>
<h4><a class="header" href="#711-basic-concepts" id="711-basic-concepts">7.1.1 Basic concepts</a></h4>
<ol>
<li>
<p>Model building versus inferring unobserved variables</p>
<p>\[X = aS + W\]</p>
<p>S: signal; W: noise; a: medium (image a black box where S goes through and output X with W as noise)</p>
<ul>
<li>
<p>Model building: known signal S, observe X -&gt; infer a</p>
</li>
<li>
<p>Variable estimation: known a, observe X -&gt; infer S</p>
</li>
</ul>
</li>
<li>
<p>Hypothesis testing vs. estimation</p>
<ul>
<li>
<p>Hypothesis testing</p>
<ul>
<li>
<p>unknown takes one of few possible values</p>
</li>
<li>
<p>aim at small probability of incorrect decision</p>
</li>
</ul>
</li>
<li>
<p>Estimation</p>
<ul>
<li>
<p>numerical unknown(s)</p>
</li>
<li>
<p>aim at an estimate that is &quot;close&quot; to the true but unknown value</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4><a class="header" href="#712-the-bayescian-inference-framework" id="712-the-bayescian-inference-framework">7.1.2 The Bayescian inference framework</a></h4>
<ul>
<li>
<p>Unknown \(\Theta\) - treated as a random variable <strong>prior distribution: \(p_{\Theta}\) or \(f_{\Theta}\)</strong></p>
</li>
<li>
<p>Observation \(X\) - observation model \(p_{X|\Theta}\) or \(f_{X|\Theta}\)</p>
</li>
<li>
<p>Use appropriate version of the Bayes rule to find \(p_{X|\Theta}(\cdot | X = x)\) or \(f_{X|\Theta} (\cdot| X = x)\)</p>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/124636223-9617e900-de88-11eb-90fa-6c5b17f9f000.png" alt="image" /></p>
<ul>
<li>
<p>The output of Bayesian inference - <strong>posterior distribution</strong></p>
<ul>
<li>
<p><strong>Maximum a posterior probability (MAP)</strong>:</p>
<p>\(p_{\Theta|X}(\theta^*|x) = \max\limits_{\theta} p_{\Theta|X}(\theta|x)\)</p>
<p>\(f_{\Theta|X}(\theta^*|x) = \max\limits_{\theta} f_{\Theta|X}(\theta|x)\)</p>
</li>
<li>
<p><strong>Conditional expectation: \(E[\Theta|X = x]\)</strong> <strong>Least Mean Square (LMS)</strong></p>
</li>
<li>
<p>estimate: \(\hat{\theta} = g(x)\) (number)</p>
</li>
<li>
<p>estimator: \(\hat{\Theta} = g(X)\) (random variable)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#713-four-cases" id="713-four-cases">7.1.3 Four cases</a></h4>
<ol>
<li>
<p><strong>Discrete \(\Theta\), discrete X</strong></p>
<ul>
<li>values of \(\Theta\): alternative hypotheses</li>
</ul>
<p>\[
p_{\Theta|X}(\theta|x) = \frac{p_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}<br />
\]</p>
<p>\[
p_X(x) = \sum\limits_{\theta'}p_{\Theta}(\theta')p_{X|\Theta}(x|\theta')
\]</p>
<ul>
<li>conditional prob of error: <strong>Smallest under the MAP rule</strong></li>
</ul>
<pre><code> \\[
     P(\hat{\theta} \neq \Theta|X = x)
 \\] 
</code></pre>
<ul>
<li>overal probability of error: </li>
</ul>
<pre><code> \\[
     P(\hat{\Theta} \neq \Theta) = \sum\limits_{x} P(\hat{\Theta} \neq \Theta|X = x)p_X(x) = \sum\limits_{\theta}P(\hat{\Theta} \neq \Theta|\Theta = \theta)p_{\Theta}(\theta)
 \\] 
</code></pre>
</li>
<li>
<p><strong>Discrete \(\Theta\), Continuous X</strong></p>
<p>\[
p_{\Theta|X}(\theta|x) = \frac{p_{\Theta}(\theta)f_{X|\Theta}(x|\theta)}{f_X(x)}<br />
\]</p>
<p>\[
f_X(x) = \sum\limits_{\theta'}p_{\Theta}(\theta')f_{x|\Theta}(x|\theta')
\]</p>
<ul>
<li>
<p>the same equation for conditional prob. of error</p>
</li>
<li>
<p>overall probability of error</p>
<p>\[
P(\hat{\Theta} \neq \Theta) = \int\limits_{x} P(\hat{\Theta} \neq \Theta|X = x)f_X(x)dx = \sum\limits_{\theta}P(\hat{\Theta} \neq \Theta|\Theta = \theta)p_{\Theta}(\theta)
\] </p>
</li>
</ul>
</li>
<li>
<p><strong>Continuous \(\Theta\), Discrete X</strong></p>
<p>\[
f_{\Theta|X}(\theta|x) = \frac{p_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}<br />
\]</p>
<p>\[
p_X(x) = \int\limits_{\theta'}f_{\Theta}(\theta')p_{x|\Theta}(x|\theta')d\theta'
\]</p>
<ul>
<li>Inferring the unknown bias of a coin and <strong>the Beta distribution</strong></li>
</ul>
</li>
<li>
<p><strong>Continuous \(\Theta\), Continuous X</strong></p>
<p>\[
f_{\Theta|X}(\theta|x) = \frac{f_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}<br />
\]</p>
<p>\[
f_X(x) = \int\limits_{\theta'}f_{\Theta}(\theta')p_{x|\Theta}(x|\theta')d\theta'
\]</p>
<ul>
<li>
<p>Linear normal models: estimation of a noisy singal</p>
</li>
<li>
<p>Estimating the parameter of a uniform </p>
<p>\(X\): uniform \([0, \Theta]\)</p>
<p>\(\Theta\): uniform \([0, 1]\)</p>
</li>
<li>
<p>Performance evaluation of an estimator \(\hat{\Theta}\)</p>
<p>\(E[(\hat{\Theta} - \Theta)^2|X = x]\)</p>
<p>\(E[(\hat{\Theta} - \Theta)^2]\)</p>
</li>
</ul>
</li>
</ol>
<p>Useful equation: </p>
<p>\[
\int_0^1 \theta^\alpha(1-\theta)^\beta d\theta = \frac{\alpha!\beta!}{(\alpha + \beta + 1)!}<br />
\]</p>
<h3><a class="header" href="#72-linear-models-with-normal-noise" id="72-linear-models-with-normal-noise">7.2 Linear models with normal noise</a></h3>
<h4><a class="header" href="#721-recognizing-normal-pdfs" id="721-recognizing-normal-pdfs">7.2.1 Recognizing normal PDFs</a></h4>
<ul>
<li>
<p>Normal distribution: \(X \sim N(\mu, \sigma^2)\) </p>
<p>\(f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}\)</p>
</li>
<li>
<p>\(f_X(x) = c e^{-(\alpha x^2 + \beta x + \gamma)}\), \(\alpha &gt; 0\) Normal with mean \(-\beta/2\alpha\) and variance \(-1/2\alpha\)</p>
</li>
</ul>
<h4><a class="header" href="#722-estimating-a-normal-random-variable-in-the-presence-of-additive-normal-noise" id="722-estimating-a-normal-random-variable-in-the-presence-of-additive-normal-noise">7.2.2 Estimating a normal random variable in the presence of additive normal noise</a></h4>
<p>\(X = \Theta + W\), \(\Theta, W,N :(0,1), independent\)</p>
<ul>
<li>
<p>\( \hat{\theta} _{MAP} = \hat{\theta} _{LMS} = E[\Theta|X = x] = x/2\)</p>
</li>
<li>
<p>even with general means and variances:</p>
<ul>
<li>
<p>posterior is normal</p>
</li>
<li>
<p>LMS and MAP estimators conincide</p>
</li>
<li>
<p>these estimators are &quot;linear&quot; of the form \(\hat{\Theta} = aX + b\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#723-the-case-of-multiple-observations" id="723-the-case-of-multiple-observations">7.2.3 The case of multiple observations</a></h4>
<p>\(X_i = \Theta + W_1\), \(\Theta \sim N(x_0, \sigma_0^2)\), \(W_i \sim N(x_i, \sigma_i^2), \Theta, W_i\) indepedent</p>
<ul>
<li>
<p>\(\hat{\theta} _{MAP} = \hat{\theta} _{LMS} = E[\Theta|X = x] = \frac{\sum\limits _{i=0}^n\frac{x_i}{\sigma_i^2}}{\sum\limits _{i=0}^n\frac{1}{\sigma_i^2}}\)</p>
</li>
<li>
<p>Key conclusions</p>
<ul>
<li>
<p>posterior is normal</p>
</li>
<li>
<p>LMS and  MAP estimates coincide</p>
</li>
<li>
<p>these estimates are &quot;linear&quot; of the form \(\hat{\theta} = a_0 + a_1x_1 + \dots + a_nx_n\)</p>
</li>
</ul>
</li>
<li>
<p>Interpretations</p>
<ul>
<li>
<p>estimate \(\hat{\theta}\): weighted average of \(x_0\) (prior mean) and \(x_i\) (observations)</p>
</li>
<li>
<p>weights determined by variances</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#724-the-mean-square-error" id="724-the-mean-square-error">7.2.4 The mean square error</a></h4>
<ul>
<li>
<p>Performance measures</p>
<ul>
<li>
<p>\(E[(\Theta - \hat{\Theta})^2|X = x] = E[(\Theta - \hat{\theta})^2|X = x] = var(\Theta|X = x) = \frac{1}{\sum\limits _{i=0}^n \frac{1}{\sigma_i^2}}\)</p>
</li>
<li>
<p>\(E[(\Theta - \hat{\Theta})^2] = \int E[(\Theta - \hat{\Theta})^2|X = x] f_X(x) dx = \frac{1}{\sum\limits _{i=0}^n \frac{1}{\sigma_i^2}}\)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#73-least-mean-squares-lms-estimation" id="73-least-mean-squares-lms-estimation">7.3 Least mean squares (LMS) estimation</a></h3>
<h4><a class="header" href="#731-in-the-absence-of-observations" id="731-in-the-absence-of-observations">7.3.1 In the absence of observations</a></h4>
<ul>
<li>
<p>Least Mean Square formulation: minimize <strong>Mean Squared Error (MSE)</strong> \(E[(\Theta - \hat{\theta})^2]: \hat{\theta} = E[\Theta]\)</p>
</li>
<li>
<p>\(E[(\Theta - E[\Theta])^2]:var(\Theta)\)</p>
</li>
</ul>
<h4><a class="header" href="#732-lms-estimation-of-theta-based-on-x" id="732-lms-estimation-of-theta-based-on-x">7.3.2 LMS estimation of \(\Theta\) based on X</a></h4>
<ul>
<li>Minimize <strong>conditional mean square error</strong>: \(E[(\Theta - \hat{\theta})^2|X = x]: \hat{\theta} = E[\Theta|X = x]\)</li>
</ul>
<h4><a class="header" href="#733-lms-performance-evaluation" id="733-lms-performance-evaluation">7.3.3 LMS performance evaluation</a></h4>
<ul>
<li>
<p>LMS estimate: \(\hat{\theta} = E[\Theta|X=x]\)</p>
</li>
<li>
<p>Estimator: \(\hat{\Theta} = E[\Theta|X]\)</p>
</li>
<li>
<p>Expected performance, once we have a measurement - <strong>Conditional mean square error</strong></p>
<p>\(MSE = E[(\Theta - E[\Theta|X=x])^2|X=x] = var(\Theta|X=x)\)</p>
</li>
<li>
<p>Expected perfornamce of the design:</p>
<p>\(MSE = E[(\Theta - E[\Theta|X])^2] = E[var(\Theta|X)] = \int var(\Theta|X=x) \cdot f_X(x) dx\) <em>Average of conditional variance</em></p>
</li>
<li>
<p>A good example</p>
<p><img src="https://user-images.githubusercontent.com/41487483/124714295-e1baa900-df01-11eb-8618-b411d877495e.png" alt="image" /></p>
<p><img src="https://user-images.githubusercontent.com/41487483/124714412-04e55880-df02-11eb-915d-996236e5958e.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#734-properties-of-the-estimation-error-in-lms-estimation" id="734-properties-of-the-estimation-error-in-lms-estimation">7.3.4 Properties of the estimation error in LMS estimation</a></h4>
<p>Given Estimator: \(\hat{\Theta} = E[\Theta|X]\) and Error: \(\tilde{\Theta} = \hat{\Theta} - \Theta\)</p>
<ul>
<li>
<p>\(E[\tilde{\Theta|X=x}] = 0\)</p>
</li>
<li>
<p>\(cov(\tilde{\Theta},\hat{\Theta}) = 0\)</p>
</li>
<li>
<p>\(var(\Theta) = var(\hat{\Theta}) + var({\tilde{\Theta}})\)</p>
</li>
</ul>
<h3><a class="header" href="#74-linear-least-mean-squares-llms-estimation" id="74-linear-least-mean-squares-llms-estimation">7.4 Linear least mean squares (LLMS) estimation</a></h3>
<p><em>Motivation: Conditional expectation \(E[\Theta|X]\) maybe hard to compute/implement</em></p>
<h4><a class="header" href="#741-llms-formulation" id="741-llms-formulation">7.4.1 LLMS formulation</a></h4>
<p>Consider estimators of \(\Theta\) of the form \(\hat{\Theta} = aX + b\), minimize \(E[(\hat{\Theta} - \Theta)^2] \implies E[(\hat{\Theta} - aX - b)^2] \) </p>
<h4><a class="header" href="#742-llms-solution" id="742-llms-solution">7.4.2 LLMS solution</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/124715231-0cf1c800-df03-11eb-9cf0-ec04d3cf70d4.png" alt="image" /></p>
<p>Minimize \(E[(\hat{\Theta} - \Theta)^2]\), that is \(E[(\Theta - aX - b)^2]\)</p>
<p>\[
\hat{\Theta}_L  = E[\Theta] + \frac{Cov(\Theta,X)}{var(X)}(X - E[X]) = E[\Theta] + \rho \frac{\sigma _\Theta}{\sigma_X}(X - E[X])<br />
\]</p>
<p>\(\rho\) corelation coefficiency</p>
<p><strong>Remarks on the solution and on the error variance</strong></p>
<ul>
<li>
<p>Only means, variances, covariances matter (we do not need to know everything)</p>
<p>\(E[(\hat{\Theta}_L - \Theta)^2] = (1 - \rho^2)var(\Theta)\)</p>
</li>
</ul>
<h4><a class="header" href="#743-llms-with-multiple-observations" id="743-llms-with-multiple-observations">7.4.3 LLMS with multiple observations</a></h4>
<ul>
<li>
<p>Consider the form \(\hat{\Theta} = a_1X_1 + \dots + a_nX_n + b\)</p>
</li>
<li>
<p>Minimize \(E[(a_1X_1 + \dots + a_nX_n + b - \Theta)^2]\)</p>
</li>
<li>
<p>Solve linear system in \(b\) and \(a_i\)</p>
</li>
<li>
<p>if \(E[\Theta|X]\) is linear in X, then \(\hat{\Theta} _{LMS} = \hat{\Theta} _{LLMS}\)</p>
</li>
<li>
<p>suppose general distributions with same mean, variances</p>
<ul>
<li>
<p>\(\hat{\theta} _{MAP} = \hat{\theta} _{LMS} = E[\Theta|X = x] = \frac{\sum\limits _{i=0}^n\frac{x_i}{\sigma_i^2}}{\sum\limits _{i=0}^n\frac{1}{\sigma_i^2}}\)</p>
</li>
<li>
<p>\(\hat{\Theta} _{LMS} = E[\Theta|X] = \frac{\frac{x_0}{\sigma _0^2} + \sum\limits _{i=i}^n\frac{X_i}{\sigma_i^2}}{\sum\limits _{i=0}^n\frac{1}{\sigma_i^2}} = \hat{\Theta} _{LLMS}\)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#75-bayesian-inference-summary" id="75-bayesian-inference-summary">7.5 Bayesian inference summary</a></h3>
<p><img src="https://user-images.githubusercontent.com/41487483/124724545-5eeb1b80-df0c-11eb-964b-82371cd4799c.png" alt="image" /></p>
</br>
<h2><a class="header" href="#unit-8-limit-theorems-and-clasic-statistics" id="unit-8-limit-theorems-and-clasic-statistics">Unit 8 Limit theorems and clasic statistics</a></h2>
<h3><a class="header" href="#81-inequalities-comvergence-and-the-weak-law-of-large-numbers" id="81-inequalities-comvergence-and-the-weak-law-of-large-numbers">8.1 Inequalities, comvergence, and the Weak Law of Large Numbers</a></h3>
<h4><a class="header" href="#811-markov-and-chebyshev-inequality" id="811-markov-and-chebyshev-inequality">8.1.1 Markov and Chebyshev inequality</a></h4>
<ol>
<li>
<p><strong>Markov inequality</strong></p>
<p>&quot;If \(X \geq 0\) and \(E[X]\) is small, then X is unlikely to be very large&quot;</p>
<p>\[
P(X \geq a) \leq \frac{E[X]}{a} \text{, for all }  a &gt; 0 \text{ and }  X \geq 0
\]</p>
</li>
<li>
<p><strong>Chebyshev inequality</strong></p>
<p>&quot;If the variance is small, then X is unlikely to be too far from the mean&quot;</p>
<p>\[
P(|X - \mu| \geq c) \leq \frac{\sigma^2}{c^2} 
\text{, for all }  c &gt; 0 \text{ and }  X \text{ is a random variable with mean } \mu \text{ and variance } \sigma^2
\]</p>
</li>
</ol>
<h4><a class="header" href="#812-the-weak-law-of-large-numbers-wlln" id="812-the-weak-law-of-large-numbers-wlln">8.1.2 The Weak Law of Large Numbers (WLLN)</a></h4>
<p>\(X_1, X_2, \dots\) i.i.d.: infinite mean \(\mu\) and variance \(\sigma^2\)</p>
<p>\[
\text{Sample mean: } M_n = \frac{X_1 + \dots + X_n}{n}
\]</p>
<ul>
<li>
<p>\(E[M_n] = \mu\)</p>
</li>
<li>
<p>\(Var(M_n) = \frac{\sigma^2}{n}\)</p>
</li>
<li>
<p><strong>WLLN:</strong> for \(\varepsilon &gt; 0\),</p>
<p>\[
P(|M_n - \mu|) \geq \varepsilon = P \left( \left| \frac{X_1 + \dots + X_n}{n} - \mu\right| \geq \varepsilon \right) \to 0 \text{, as n} \to \infty
\]</p>
</li>
<li>
<p>Interpreting the WLLN </p>
<ul>
<li>
<p><strong>Sample mean</strong> \(M_n\) is unlikely to be far off from <strong>true mean</strong> \(\mu\)</p>
</li>
<li>
<p><strong>Sample mean</strong> \(M_n\) is the <strong>emperical frequency</strong> of even \(A\), with \(p = P(A)\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#813-convergence-in-probability" id="813-convergence-in-probability">8.1.3 Convergence in Probability</a></h4>
<p>Sequence of random variables \(Y_n\), not necessarily independent</p>
<p><strong>Definition</strong>: A sequence <a style="color:red;">\(Y_n\)</a> <strong>converges in probability</strong> to a certain number <a style="color:red;">a</a> if:</p>
<p>\[
\lim_\limits{n \to \infty} P(|Y_n - a| \geq \varepsilon) = 0<br />
\]</p>
<p><em>Almost all of the PMF/PDF of \(Y_n\) eventually gets concentrated (arbitrarily) close to a</em></p>
<ul>
<li>
<p>Some properties - suppose that \(X_n \to a, Y_n \to b\)</p>
<ol>
<li>
<p>if <em>g</em> is continuous, then \(g(X_n) \to g(a)\)</p>
</li>
<li>
<p>\(X_n + Y_n \to a + b\)</p>
</li>
<li>
<p>\(E[X_n]\) <b>need not converge to a</b></p>
</li>
</ol>
</li>
</ul>
</br>
<h3><a class="header" href="#82-the-central-limit-theorem-clt" id="82-the-central-limit-theorem-clt">8.2 The Central Limit Theorem (CLT)</a></h3>
<p><img src="https://user-images.githubusercontent.com/41487483/125804339-0461c83e-e397-488a-81ff-bfc51017512b.png" alt="image" /></p>
<h4><a class="header" href="#822-what-exactly-does-the-clt-say" id="822-what-exactly-does-the-clt-say">8.2.2 What exactly does the CLT say?</a></h4>
<ol>
<li>
<p><strong>Theory</strong></p>
<p>\(Z_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}\) and \(Z \sim N(0,1)\)</p>
<ul>
<li>
<p>CDF of Z<sub>n</sub> converges to normal <strong>CDF</strong></p>
</li>
<li>
<p>results for convergence of PDFs or PMFs (with more assumptions)</p>
</li>
<li>
<p>results without assuming that X<sub>i</sub> are identically distributed</p>
</li>
<li>
<p>results under &quot;weak dependence&quot;</p>
</li>
</ul>
<p><strong>In short, CLT applies to a sequence of random variables that do not need to be i.i.d.</strong> </p>
</li>
<li>
<p><strong>Practice</strong></p>
<ul>
<li>
<p>The practiec of normal approximations:</p>
<ul>
<li>
<p>treat Z<sub>n</sub> as if it were normal</p>
</li>
<li>
<p>treat S<sub>n</sub> as if normal: \(N(n\mu, n\sigma^2)\) as \(S_n = \sqrt{n}\sigma Z_n + n\mu\)</p>
</li>
</ul>
</li>
<li>
<p>Can we use the CLT when n is &quot;moderate&quot;? </p>
<ul>
<li>
<p>usually, yes</p>
</li>
<li>
<p>symmetry and unimodality help</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</br>
<h3><a class="header" href="#83-an-introduction-to-classical-statistics" id="83-an-introduction-to-classical-statistics">8.3 An introduction to classical statistics</a></h3>
<h4><a class="header" href="#831-overview" id="831-overview">8.3.1 Overview</a></h4>
<ul>
<li>
<p>Inference using the Bayes rule:</p>
<p>unknown \(\Theta\) and observation \(X\) are both random variables: Find \(p_{\Theta|X}\)</p>
</li>
<li>
<p>Classical statistics: unknown <strong>constant</strong> \(\theta\)</p>
<ul>
<li>
<p>Problem types in classical statistics</p>
<ul>
<li>
<p>Hypothesis testing: \(H_0: \theta = 1/2 \text{ vs. } H_1: \theta = 3/4\)</p>
</li>
<li>
<p>Composite hypotheses: \(H_0: \theta = 1/2 \text{ vs. } H_1: \theta \neq 1/2\)</p>
</li>
<li>
<p>Estimation: design an estimator \(\hat{\Theta}\), to keep estimation error \((\hat{\Theta} - \theta)\) small.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#832-the-sample-mean-and-some-terminology" id="832-the-sample-mean-and-some-terminology">8.3.2 The sample mean and some terminology</a></h4>
<ul>
<li>
<p>Estimating a mean</p>
<ul>
<li>
<p>\(X_1, \dots, X_n\): i.i.d, mean \(\theta\), variance \(\sigma^2\)</p>
</li>
<li>
<p><strong>Sample mean</strong> \(= \hat{\Theta}_n = M_n = \frac{X_1 + \dots + X_n}{n}\) </p>
</li>
</ul>
</li>
<li>
<p>Properties and terminology</p>
<ul>
<li>
<p>\(E[\hat{\Theta}_n] = \theta\)  <a style="color:red;">(unbiased)</a> for all \(\theta\)</p>
</li>
<li>
<p>WLLN: \(E[\hat{\Theta}_n] \to \theta\)  <a style="color:red;">(consistency)</a> for all \(\theta\)</p>
</li>
<li>
<p>Mean square error <a style="color:red;">(MSE)</a>: \(E[(\hat{\Theta}_n - \theta)^2] = var(\hat{\Theta}_n) = \frac{\sigma^2}{n}\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#833-on-the-mean-square-error-of-an-estimator" id="833-on-the-mean-square-error-of-an-estimator">8.3.3 On the mean square error of an estimator</a></h4>
<p>\[
E[(\hat{\Theta} - \theta)^2] = var(\hat{\Theta} - \theta) + (E[\hat{\Theta} - \theta])^2 = var(\hat{\Theta}) + (bias)^2<br />
\]</p>
<ul>
<li>
<p>Sample mean estimator (\(\hat{\Theta}_n = M_n\)): \(MSE = \frac{\sigma^2}{n} + 0\)</p>
</li>
<li>
<p>Zero estimator (\(\hat{\Theta} = 0\)): \(MSE = 0 + \theta^2\)</p>
</li>
<li>
<p>\(\sqrt{var(\hat{\Theta})}\) is the <a style="color:red"> standard error </a>. </p>
<p><em>Standard Error refers to sampling distribution, whereas standard deviation refers to sample distribution</em></p>
</li>
</ul>
<h4><a class="header" href="#834-confidence-intervals-cis" id="834-confidence-intervals-cis">8.3.4 Confidence intervals (CIs)</a></h4>
<p>An \(1 - \alpha\) confidence interval is an interval \([\hat{\Theta}^-, \hat{\Theta}^+]\), for all \(\theta\)</p>
<p>\[
P(\hat{\Theta}^- \leq \theta \leq \hat{\Theta}^+)<br />
\]</p>
<ul>
<li>
<p><strong>CI for the estimation of the mean</strong></p>
<ul>
<li>
<p>\(X_1, \dots, X_n\): i.i.d, mean \(\theta\), variance \(\sigma^2\)</p>
</li>
<li>
<p><strong>Sample mean</strong> \(= \hat{\Theta}_n = M_n = \frac{X_1 + \dots + X_n}{n}\) </p>
</li>
<li>
<p>95% CI: \(\Phi(1.96) = 0.975 = 1 - 0.025\)</p>
<p>\[
P \left( \frac{|\hat{\Theta}_n - \theta|}{\sigma/\sqrt{n}}\right) \leq 1.96 \approx 0.95 \text{ (CLT) } \implies P \left(\hat{\Theta}_n - \frac{1.96\sigma}{\sqrt{n}} \leq \theta \leq \hat{\Theta}_n + \frac{1.96\sigma}{\sqrt{n}}\right)
\]</p>
</li>
</ul>
</li>
<li>
<p><strong>CI for the mean when \(\sigma\) is unknown</strong></p>
<ol>
<li>
<p>use upper bound on \(\sigma\)</p>
<ul>
<li>for \(X_i\) Bernoulli: \(\sigma \leq 1/2\)</li>
</ul>
</li>
<li>
<p>use ad hoc estimate of \(\sigma\)</p>
<ul>
<li>for \(X_i\) Bernoulli: \(\sigma = \sqrt{\hat{\Theta}_n(1 - \hat{\Theta}_n)}\)</li>
</ul>
</li>
<li>
<p>use sample mean estimate of the variance</p>
<p>\(\sigma^2 = E[(X_i - \theta)^2] \implies \frac{1}{n} \sum\limits_{i = 1}^n (X_i - \hat{\Theta}_n)^2 \to \sigma^2\)</p>
</li>
</ol>
</li>
</ul>
<hr>
<ul>
<li>
<p>Two approximations involved here:</p>
<ul>
<li>
<p>CLT: approximately normal</p>
</li>
<li>
<p>using estimate of \(\sigma\)</p>
</li>
</ul>
</li>
<li>
<p>correction for second approximation <a style="color:red">(t-tables)</a> used when <em>n</em> is small.</p>
</li>
</ul>
<hr>
<h4><a class="header" href="#835-other-natural-estimators" id="835-other-natural-estimators">8.3.5 Other natural estimators</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/125932005-aaf38a3c-d0e4-41e8-9be0-7fd8c9896573.png" alt="image" /></p>
<h4><a class="header" href="#836-maximum-likelihood-ml-estimation" id="836-maximum-likelihood-ml-estimation">8.3.6 Maximum Likelihood (ML) estimation</a></h4>
<ul>
<li>
<p>Pick <a style="color:red">\(\theta\)</a> that &quot;makes data most likely&quot;</p>
<p>\[
\hat{\theta}_ {ML} = arg \max\limits_{\theta} p_X(x;\theta)<br />
\]</p>
<p><i>compare to maximum a posterior probability Bayesian posterior \(p_{\Theta|X}(\theta^*|x) = \max\limits_{\theta}p_{\Theta|X}(\theta|x)\)</i></p>
</li>
</ul>
<h2><a class="header" href="#unit-9-the-bernoulli-and-poisson-process" id="unit-9-the-bernoulli-and-poisson-process">Unit 9 The Bernoulli and Poisson process</a></h2>
<h3><a class="header" href="#91-the-bernoulli-process" id="91-the-bernoulli-process">9.1 The Bernoulli process</a></h3>
<h4><a class="header" href="#911-definition" id="911-definition">9.1.1 Definition</a></h4>
<ul>
<li>
<p>A sequence of independent Bernoulli tirals, \(X_i\)</p>
</li>
<li>
<p>At each trial, i:</p>
<p>\(P(X_i = 1) = P(\text{success at the ith trial}) = p\)</p>
<p>\(P(X_i = 0) = P(\text{failure at the ith trial}) = 1 - p\)</p>
</li>
<li>
<p>Properties</p>
<ul>
<li>
<p>\(E[X_i] = p\)</p>
</li>
<li>
<p>\(var(X_i) = p*(1-p)\)</p>
</li>
</ul>
</li>
<li>
<p>Key assumption</p>
<ul>
<li>
<p>Independence</p>
</li>
<li>
<p>Time-homogeneity</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#912-stochastic-processes" id="912-stochastic-processes">9.1.2 Stochastic processes</a></h4>
<ul>
<li>
<p>A sequence of random variables \(X_1, X_2, \dots\)</p>
</li>
<li>
<p>Sample space: \(\Omega = \text{a set of infinite sequence of 0's and 1's}\) </p>
</li>
</ul>
<h4><a class="header" href="#913-number-of-successesarrivals-s-in-n-time-slots-binomial-distribution" id="913-number-of-successesarrivals-s-in-n-time-slots-binomial-distribution">9.1.3. Number of successes/arrivals S in n time slots (Binomial distribution)</a></h4>
<ul>
<li>
<p>\(S = X_1 + X_2 + \dots + X_n\)</p>
</li>
<li>
<p>\(P(S=k) = \binom{n}{k}p^k(1-p)^{n-k}\), k = 0, 1, 2 ....</p>
</li>
<li>
<p>\(E[S] = np\)</p>
</li>
<li>
<p>\(var(S) = np(1-p)\)</p>
</li>
</ul>
<h4><a class="header" href="#914-time-until-the-first-successarrival-geometric-distribution" id="914-time-until-the-first-successarrival-geometric-distribution">9.1.4 Time until the first success/arrival (Geometric distribution)</a></h4>
<ul>
<li>
<p>\(T_i = min \{i: X_i=1 \}\)</p>
</li>
<li>
<p>\(P(T_1 = k) = (1-p)^(k-1)p\), k = 1,2,...</p>
</li>
<li>
<p>\(E[T_1] = \frac{1}{p}\)</p>
</li>
<li>
<p>\(var(T_1) = \frac{1-p}{p^2}\)</p>
</li>
</ul>
<h4><a class="header" href="#915-independence-memorylessness-and-fresh-start-properties" id="915-independence-memorylessness-and-fresh-start-properties">9.1.5 Independence, memorylessness, and fresh-start properties</a></h4>
<ul>
<li>
<p>Fresh-start after time n (slots), after time T1</p>
</li>
<li>
<p>Fresh-start after a random time N</p>
<ul>
<li>
<p>N = time of 3rd sucess</p>
</li>
<li>
<p>N = first time that 3 successes in a row have been observed</p>
</li>
</ul>
</li>
<li>
<p>The process \(X_{N+1}, X_{N+2}\), ... is </p>
<ul>
<li>
<p>A Bernoulli process</p>
</li>
<li>
<p>independent of N, \(X_1, X_2, \dots, X_N\)</p>
</li>
</ul>
<p><em>as long as N is determined &quot;casually&quot;</em></p>
</li>
</ul>
<h4><a class="header" href="#916-time-of-the-kth-successarrival" id="916-time-of-the-kth-successarrival">9.1.6 Time of the kth success/arrival</a></h4>
<ul>
<li>
<p>\(Y_k\) = time of kth arrival</p>
</li>
<li>
<p>\(T_k\) = kth inter-arrival time = \(Y_k - Y_{k-1} \text{, } k \geq 2 \)</p>
</li>
<li>
<p>\(Y_k = T_1 + \dots + T_k\)</p>
<ul>
<li>
<p>The process starts fresh after time T1</p>
</li>
<li>
<p>T2 is independent of T1: Geometric(p)</p>
</li>
<li>
<p>\(E[Y_k] = \frac{k}{p}\)</p>
</li>
<li>
<p>\(var(Y_k) = \frac{k(1-p)}{p^2}\)</p>
</li>
<li>
<p>PMF: \(p_{Y_k}(t) = \binom{t-1}{k-1}p^k(1-p)^{t-k} \text{, } t = k, k +1, ..\).</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#917-merging-of-independent-bernoulli-processes" id="917-merging-of-independent-bernoulli-processes">9.1.7 Merging of independent Bernoulli processes</a></h4>
<ul>
<li>
<p>\(X_i\): Bernoulli(p)</p>
</li>
<li>
<p>\(Y_i\): Bernoulli(q)</p>
</li>
<li>
<p>Merged process: \(Z_i: g(X_i, Y_i)\) Bernoulli(p + q - pq)</p>
</li>
</ul>
<h4><a class="header" href="#917-splitting-of-a-bernoulli-process" id="917-splitting-of-a-bernoulli-process">9.1.7 Splitting of a Bernoulli process</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/126913885-cf7863e9-8053-4a4c-90d4-c6afc0585538.png" alt="image" /></p>
<h4><a class="header" href="#918-poisson-approximation-to-binomial" id="918-poisson-approximation-to-binomial">9.1.8 Poisson approximation to binomial</a></h4>
<ul>
<li>
<p>Interesting regime: large n, small p, <a style='color:red'> moderate λ = np</a></p>
</li>
<li>
<p>Number of arrivals S in n slots: \(p_S(k) \to \frac{\lambda^k}{k!}e^{-\lambda}\) (For fixed k = 0, 1...)</p>
</li>
</ul>
</br>
<h3><a class="header" href="#92-the-poison-process" id="92-the-poison-process">9.2 The Poison process</a></h3>
<h4><a class="header" href="#921-definition" id="921-definition">9.2.1 Definition</a></h4>
<p>Poisson process is similar to Bernoulli process, but in a continuous time interval.</p>
<ul>
<li>Numbers of arrivals in disjoint time intervals are independent</li>
</ul>
<pre><code>\\(P(k, \tau)\\) = Prob. of *k* arrivals in interval of duration \\(\tau\\)
</code></pre>
<ul>
<li>
<p>Small interval probabilities - For VERY small \(\delta\):</p>
<p>\[
P(k, \delta) = \begin{cases} 1-\lambda\delta + O(\delta^2) &amp; \quad \text{if } k = 0 \\
\lambda\delta + O(\delta^2)  &amp; \quad \text{if } k=1 \\
0 + O(\delta^2) &amp; \quad \text{if } k&gt;1 \end{cases}
\]</p>
<p>\[
P(k, \delta) \approx \begin{cases} 1-\lambda\delta &amp; \quad \text{if } k = 0 \\
\lambda\delta &amp; \quad \text{if } k=1 \\
0 &amp; \quad \text{if } k&gt;1 \end{cases}
\]</p>
<p><strong><a style='color:red'>λ: &quot;Arrival rates&quot; </a></strong></p>
</li>
</ul>
<h4><a class="header" href="#922-the-poisson-pmf-for-the-number-of-arrivals" id="922-the-poisson-pmf-for-the-number-of-arrivals">9.2.2 The Poisson PMF for the number of arrivals</a></h4>
<ul>
<li>
<p>\(N_{\tau}:\text{ arrivals in }[0, \tau]\) </p>
</li>
<li>
<p>\(N_\tau \approx Binomial(n,p)\), \(n = \frac{\tau}{\delta}\), \(p = \lambda\delta + O(\delta^2)\)</p>
</li>
<li>
<p>\[
P(k, \tau) = P(N_\tau =k) = \frac{(\lambda\tau)^ke^{-\lambda\tau}}{k!}, \text{k = 0, 1, 2,...}
\]</p>
</li>
<li>
<p>\(E[N_\tau] \approx np \approx \lambda\tau\)</p>
</li>
<li>
<p>\(var(N_\tau) \approx np(1-p) \approx \lambda\tau\)</p>
</li>
</ul>
<h4><a class="header" href="#923-the-time-t_1-until-the-first-arrival" id="923-the-time-t_1-until-the-first-arrival">9.2.3 The time \(T_1\) until the first arrival</a></h4>
<p>Find the CDF: \(P(T_1 \leq t) = 1 - P(T_1 &gt; t) = 1 - P(0,t) = 1 - e^{-\lambda t}\)</p>
<p>\[
f_{T_1}(t) = \lambda e^{-\lambda t} \text{, for } t \geq 0<br />
\]</p>
<p><strong><a style='color:red'>Exponential(λ) </a></strong></p>
<h4><a class="header" href="#924-the-time-y_k-of-the-kth-arrival" id="924-the-time-y_k-of-the-kth-arrival">9.2.4 The time \(Y_k\) of the kth arrival</a></h4>
<p>Two ways to derive: </p>
<ul>
<li>
<p>Through CDF: \(P(Y_k \leq y) = \sum\limits_{n=k}^{\infty}P(n, y)\)</p>
</li>
<li>
<p>More intuitive argument</p>
<p>\[
f_{Y_k}(y)\delta \approx P(y \leq Y_k \leq y + \delta) \approx P(k-1, y)\lambda\delta
\]</p>
</li>
</ul>
<p><strong><a style='color:red'>Erlang distribution</a></strong></p>
<p>\[
f_{Y_k}(y) = \frac{\lambda^k y^{k-1} e^{-\lambda y} }{(k-1)!} \text{, } y \geq 0<br />
\]</p>
<h4><a class="header" href="#925-memorylessness-and-the-fresh-start-property" id="925-memorylessness-and-the-fresh-start-property">9.2.5 Memorylessness and the fresh-start property</a></h4>
<ul>
<li>
<p>If we start watching at time <em>t</em>, we see Poisson process, independent of the history until time <em>t</em>. Then, time until next arrival follows exp(λ)</p>
</li>
<li>
<p>Time between first and second arrival, \(T_2 = Y_2 - Y_1\) follows exp(λ)</p>
</li>
<li>
<p>Similar for all \(T_k = Y_k - Y_{k-1} \text{, } k \geq 2\)</p>
</li>
<li>
<p>\(Y_k = T_1 + \dots + T_k\) is sum of i.i.d. exponentials</p>
<ul>
<li>
<p>\(E[Y_k] = \frac{k}{\lambda}\)</p>
</li>
<li>
<p>\(var(Y_k) = \frac{k}{\lambda^2}\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#926-bernoullipoisson-relation" id="926-bernoullipoisson-relation">9.2.6 Bernoulli/Poisson relation</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/126948921-e39258a2-2396-4efb-8e13-4dbf605356f4.png" alt="image" /></p>
<table><thead><tr><th></th><th align="center">Poisson</th><th align="center">Bernoulli</th></tr></thead><tbody>
<tr><td>Times of Arrival</td><td align="center">Continuous</td><td align="center">Discrete</td></tr>
<tr><td>Arrival Rate</td><td align="center">λ per unit time</td><td align="center">p per trial</td></tr>
<tr><td>PMF of # of arrivals</td><td align="center">\[P(k,\tau) = \frac{(\lambda\tau)^ke^{-\lambda\tau}}{k!} \\E[N_\tau] \approx \lambda\tau \\ var(N_\tau) \approx \lambda\tau\]</td><td align="center">\[P_S(k) = \binom{n}{k}p^k(1-p)^{(n-k)} \\ \to \frac{\lambda^k}{k!}e^{-\lambda} \\ E[S] = np \\ var(S) = np(1-p) \]</td></tr>
<tr><td>Interarrival Time Distr.</td><td align="center">\[f_{T1}(t) = \lambda e^{-\lambda t}\] Exponential </br> \[E[T_1] = 1/\lambda \\ var(T_1) = 1/\lambda^2\]</td><td align="center">\[P_{T1} = (1-p)^{n-1}p\] Geometric </br> \[E[T_1] = 1/p \\ var(T_1) = \frac{1-p}{p^2}\]</td></tr>
<tr><td>Time to k-th arrival</td><td align="center">\[f_{Y_k}(y) = \frac{\lambda^k y^{k-1} e^{-\lambda y}}{(k-1)!}\] Erlang </br> \[E[Y_k] = k/\lambda \\ var(Y_k) = k/\lambda^2\]</td><td align="center">\[p_{Y_k}(t) = \binom{t-1}{k-1}p^k(1-p)^{t-k}\] Pascal</td></tr>
</tbody></table>
</br>
<h3><a class="header" href="#93-more-on-the-poisson-process" id="93-more-on-the-poisson-process">9.3 More on the Poisson process</a></h3>
<h4><a class="header" href="#931-the-sum-of-independent-poisson-random-variables" id="931-the-sum-of-independent-poisson-random-variables">9.3.1 The sum of independent Poisson random variables</a></h4>
<p>\[
P(k, \tau) = \frac{(\lambda\tau)^k e^{-\lambda\tau}}{k!}
\]</p>
<p>We call it a Poisson random variable with parameters \(\lambda\tau\)</p>
<p><a style='color:red'>The sum of independent Poisson random variables, with means/parameters \(\mu\) and \(\nu\) is Poisson with mean/parameter \(\mu + \nu\)</a></p>
<h4><a class="header" href="#932-merging-independent-poisson-processes" id="932-merging-independent-poisson-processes">9.3.2 Merging independent Poisson processes</a></h4>
<table><thead><tr><th align="center"></th><th align="center">0 </br> \(1 - \lambda_1\delta\)</th><th align="center">1 </br> \(\lambda_1\delta\)</th><th align="center">≥ 2 </br> \(O(\delta^2)\)</th></tr></thead><tbody>
<tr><td align="center"><strong>0</strong> \(1 - \lambda_2\delta\)</td><td align="center">\((1-\lambda_1\delta)(1-\lambda_2\delta)\)</td><td align="center">\(\lambda_1\delta(1-\lambda_2\delta)\)</td><td align="center">-</td></tr>
<tr><td align="center"><strong>1</strong> \(\lambda_2\delta\)</td><td align="center">\(\lambda_2\delta(1-\lambda_1\delta)\)</td><td align="center">\(\lambda_1\lambda_2\delta^2\)</td><td align="center">-</td></tr>
<tr><td align="center"><strong>≥ 2</strong> \(O(\delta^2)\)</td><td align="center">-</td><td align="center">-</td><td align="center">-</td></tr>
</tbody></table>
<ul>
<li>
<p>0 Arrivals \(\approx 1 - (\lambda_1 + \lambda_2)\delta\)</p>
</li>
<li>
<p>1 Arrivals \(\approx (\lambda_1 + \lambda_2)\delta\)</p>
</li>
<li>
<p>≥ 2 Arrivals \(O(\delta^2)\)</p>
</li>
</ul>
<p><a style='color:red'>Merging independent Poisson(λ1) and Poisson(λ1) result in Poisson(λ1 + λ2))</a></p>
<h4><a class="header" href="#933-the-time-the-firstlast-light-bulb-burns-out---minxyz-and-maxxyz-problem" id="933-the-time-the-firstlast-light-bulb-burns-out---minxyz-and-maxxyz-problem">9.3.3 The time the first(last) light bulb burns out - min{X,Y,Z} and max{X,Y,Z} problem</a></h4>
<p>Three lightbulbs have independent lifetimes X, Y, Z exponential(λ)</p>
<ol>
<li>
<p>The expected time until first lightbulb burnout:</p>
<ul>
<li>
<p>X, Y, Z: first arrivals in independent Poisson processes</p>
</li>
<li>
<p>Merged process: Poisson(3λ)</p>
</li>
<li>
<p>min{X, Y, Z}: 1st arrival in merged process \(\to E[min] = 1/3\lambda\)</p>
</li>
</ul>
</li>
<li>
<p>The expected time until the last lightbulb burnout:</p>
<ul>
<li>Merged process in different intervals</li>
</ul>
<p>\[
E[max] = \frac{1}{3\lambda} + \frac{1}{2\lambda} + \frac{1}{\lambda}
\]</p>
</li>
</ol>
<h4><a class="header" href="#934-splitting-of-a-poisson-process" id="934-splitting-of-a-poisson-process">9.3.4 Splitting of a Poisson process</a></h4>
<p>Split arrivals into two streams using independent coin flips of a coin with bias <em>q</em></p>
<p><em>Assume that coin flips are independent from the original Poisson process</em></p>
<ul>
<li>
<p><a style='color:red'>Resulting streams are Poisson with rate \(\lambda q, \lambda (1-q)\)</a></p>
</li>
<li>
<p>The splitted Poisson processes are <a style='color:red'>independent!</a></p>
</li>
</ul>
<h4><a class="header" href="#935-random-incidence-in-the-poisson-process" id="935-random-incidence-in-the-poisson-process">9.3.5 'Random incidence' in the Poisson process</a></h4>
<ol>
<li>
<p>Analysis</p>
<p><img src="https://user-images.githubusercontent.com/41487483/126982230-c8be5322-3130-43d5-8909-c9df592315a1.png" alt="image" /></p>
</li>
<li>
<p>Random incidence &quot;Paradox&quot; is not special to the Poisson process</p>
<ul>
<li>
<p>Example: interarrival times, i.i.d., equally likely to be 5 or 10 mins. Then expected value of <a style='color:red'>k-th interarrival time </a> = 7.5</p>
</li>
<li>
<p>Show up at a <a style='color:red'>&quot;random time&quot;</a> </p>
<ul>
<li>
<p>P(arrival duaring a 5-minute interarrival interval) = 1/3</p>
</li>
<li>
<p>Expected length of interarrival interval during which you arrive ≈ 8.3</p>
</li>
</ul>
</li>
<li>
<p><a style='color:red'>Sampling method matters</a> - <em>Different sampling methods can give different results</em></p>
<ul>
<li>
<p>Average family size? (3 families with one person, 1 family with 6 persons)</p>
<ul>
<li>
<p>look at a random family: 3/4x1 + 1/4x6</p>
</li>
<li>
<p>looat at a random persons's family: 3/9x1 + 6/9x6</p>
</li>
</ul>
</li>
<li>
<p>Average bus occupancy?</p>
</li>
<li>
<p>Average class size?</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3><a class="header" href="#94-additional-theoretical-background" id="94-additional-theoretical-background">9.4 Additional theoretical background</a></h3>
<h4><a class="header" href="#941-poisson-versus-normal-approximation-to-the-binomial" id="941-poisson-versus-normal-approximation-to-the-binomial">9.4.1 Poisson versus normal approximation to the binomial</a></h4>
<p>We have seen that a binomial random variable with parameters <em>n</em> and <em>p</em> can be approximated by a normal random variable (central limit theorem) but also by a Poisson random variable. Are these two facts contradictory? Fortunately not; the two approximations apply to different regimes:</p>
<ol>
<li>
<p>if we fix <em>p</em> and let \(n \to \infty)\), we are in the setting where the <strong>central limit theorem</strong> applies.</p>
</li>
<li>
<p>If we let \(n \to \infty)\), \(p \to 0)\), while keeping the product <em>np</em> fixed, the Poisson approximation applies.</p>
</li>
<li>
<p>If <em>p</em> is very small but <em>np</em> is very large, then two approximations agree.</p>
</li>
</ol>
<h4><a class="header" href="#942-sums-of-a-binomial-and-a-poisson-distributed-number-of-bernoulli-rvs" id="942-sums-of-a-binomial-and-a-poisson-distributed-number-of-bernoulli-rvs">9.4.2 Sums of a binomial and a Poisson-distributed number of Bernoulli r.v.'s</a></h4>
<p>Let \(X_1,X_2,...\) be independent Bernoulli random variables with parameter <em>p</em>, and <em>N</em> be a random variable that takes integer values and is independent of \(X_i, i = 1,2, \dots\) Let \(Y=X_1+X_2+ \dots +X_N\) for positive values of <em>N</em>, and let \(Y =0\)  when \(N=0\).</p>
<ul>
<li>
<p>If <em>N</em> is binomial with parameters <em>m</em> and <em>q</em>, then <em>Y</em> is binomial with parameters <em>m</em> and <em>pq</em>.</p>
<p><img src="https://user-images.githubusercontent.com/41487483/127369301-0884ba53-e79c-4cd9-8545-368f7ef5c03f.png" alt="image" /></p>
</li>
<li>
<p>If <em>N</em> is poisson with parameters \(\lambda\), then <em>Y</em> is Poisson with parameter \(\lambda\).</p>
<p><img src="https://user-images.githubusercontent.com/41487483/127369385-7d06f2f1-2866-4949-b342-933fe8bfe6d2.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#943-sums-of-a-geometrically-distributed-number-of-geometric-and-exponential-rvs" id="943-sums-of-a-geometrically-distributed-number-of-geometric-and-exponential-rvs">9.4.3 Sums of a geometrically-distributed number of geometric and exponential r.v.'s</a></h4>
<p>Let <em>N</em> be a geometric random variable with parameter <em>q</em>, and let \(X_1, X_2, \dots\) be random variables that are independent and independent of <em>N</em>. Let \(Y=X_1+\dots+X_N\).</p>
<ul>
<li>
<p>If \(X_i\) is geometric with parameter <em>p</em>, then <em>Y</em> is geometric with parameter <em>pq</em></p>
<p><img src="https://user-images.githubusercontent.com/41487483/127370021-c2299c8b-8107-4826-b1df-048ec9bdde81.png" alt="image" /></p>
</li>
<li>
<p>If \(X_i\) is exponential with parameter \(\lambda\), then <em>Y</em> is exponential with parameter \(\lambda q\)</p>
<p><img src="https://user-images.githubusercontent.com/41487483/127370100-fc323f23-7d82-4b50-aa98-2fc0bb17c31c.png" alt="image" /></p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../statistics/statistics.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../statistics/statinfer.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../statistics/statistics.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="../statistics/statinfer.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
