<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Note</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="chapter_1.html"><strong aria-hidden="true">1.</strong> Chapter 1</a></li><li class="chapter-item expanded "><a href="git.html"><strong aria-hidden="true">2.</strong> Git</a></li><li class="chapter-item expanded "><a href="tips.html"><strong aria-hidden="true">3.</strong> Visit with the Client &amp; Setup Overview</a></li><li class="chapter-item expanded "><a href="frontend/frontend.html"><strong aria-hidden="true">4.</strong> Frontend</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="frontend/vue3.html"><strong aria-hidden="true">4.1.</strong> Vue3 note</a></li><li class="chapter-item expanded "><a href="frontend/css.html"><strong aria-hidden="true">4.2.</strong> CSS</a></li></ol></li><li class="chapter-item expanded "><a href="sql/sql.html"><strong aria-hidden="true">5.</strong> SQL</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="sql/duke.html"><strong aria-hidden="true">5.1.</strong> Duke-coursera</a></li></ol></li><li class="chapter-item expanded "><a href="python/python.html"><strong aria-hidden="true">6.</strong> Python</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="python/Coursera-Michigan.html"><strong aria-hidden="true">6.1.</strong> Python-coursera-Michigan</a></li><li class="chapter-item expanded "><a href="python/leanningfrommis.html"><strong aria-hidden="true">6.2.</strong> Learning in a hard way</a></li></ol></li><li class="chapter-item expanded "><a href="statistics/statistics.html"><strong aria-hidden="true">7.</strong> Statistics rewind</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="statistics/MITx6.431x.html"><strong aria-hidden="true">7.1.</strong> Probability - The Science of Uncertainty and Data</a></li><li class="chapter-item expanded "><a href="statistics/statinfer.html"><strong aria-hidden="true">7.2.</strong> Improve your statistical inferences</a></li><li class="chapter-item expanded "><a href="statistics/inferentialstat.html"><strong aria-hidden="true">7.3.</strong> Inferential Statistics</a></li><li class="chapter-item expanded "><a href="statistics/python.html"><strong aria-hidden="true">7.4.</strong> Statistics with Python</a></li><li class="chapter-item expanded "><a href="statistics/problems.html"><strong aria-hidden="true">7.5.</strong> Common problems</a></li><li class="chapter-item expanded "><a href="statistics/Infe_stats_python.html"><strong aria-hidden="true">7.6.</strong> Inferential Statistics with Python </a></li></ol></li><li class="chapter-item expanded "><a href="dataviztools/dataviztools.html"><strong aria-hidden="true">8.</strong> Data Viz</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dataviztools/projects.html"><strong aria-hidden="true">8.1.</strong> Dataviz Projects</a></li></ol></li><li class="chapter-item expanded "><a href="english/english.html"><strong aria-hidden="true">9.</strong> English</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="english/speaking.html"><strong aria-hidden="true">9.1.</strong> Speaking English</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Note</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#chapter-1" id="chapter-1">Chapter 1</a></h1>
<h4><a class="header" href="#a-hrefhttpsgithubcomadam-pmarkdown-herewikimarkdown-cheatsheetlinksthis-is-a-link-for-markdown-syntaxa" id="a-hrefhttpsgithubcomadam-pmarkdown-herewikimarkdown-cheatsheetlinksthis-is-a-link-for-markdown-syntaxa"><a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links">This is a link for markdown syntax</a></a></h4>
<h4><a class="header" href="#a-hrefhttpsrust-langgithubiomdbookformatmathjaxhtmlthis-is-a-link-for-mdbook---letax-equation-referencea" id="a-hrefhttpsrust-langgithubiomdbookformatmathjaxhtmlthis-is-a-link-for-mdbook---letax-equation-referencea"><a href="https://rust-lang.github.io/mdBook/format/mathjax.html">This is a link for mdbook - Letax equation reference</a></a></h4>
<h4><a class="header" href="#a-hrefhttpsenwikibooksorgwikilatexmathematicslist_of_mathematical_symbolslatexmathematics-wikibooksa" id="a-hrefhttpsenwikibooksorgwikilatexmathematicslist_of_mathematical_symbolslatexmathematics-wikibooksa"><a href="https://en.wikibooks.org/wiki/LaTeX/Mathematics#List_of_mathematical_symbols">LaTeX/Mathematics Wikibooks</a></a></h4>
<h1><a class="header" href="#how-to-share-local-codes-with-remote-codes-on-github" id="how-to-share-local-codes-with-remote-codes-on-github">How to share local codes with remote codes on github</a></h1>
<h2><a class="header" href="#case-1" id="case-1">Case 1</a></h2>
<p><a href="https://docs.github.com/en/github/importing-your-projects-to-github/importing-source-code-to-github/adding-an-existing-project-to-github-using-the-command-line">A project has already been created in local folder</a> </p>
<ol>
<li>
<p>Create a new repo on github</p>
</li>
<li>
<p>Input command line in terminal:</p>
<p><code>git initial -b main</code>
<code>git remote add origin url</code>
<code>git fetch</code>
<code>git checkout main</code>
<code>git merge master</code>
<code>git git push</code></p>
</li>
</ol>
<h2><a class="header" href="#case-2---start-from-the-begining" id="case-2---start-from-the-begining">Case 2 - start from the begining</a></h2>
<p><a href="https://www.youtube.com/watch?v=TwVuFwyztEE">Advanced Git Tutorial | Google IT Automation with Python</a></p>
<p><strong>Git is a visual control system (VCS), which can save code, configrations, histories, etc.</strong></p>
<ol>
<li>
<p>After installing git, the first thing to do is to tell Git who you are by execute command line <code>git config --global &quot;me@email.com&quot;</code></p>
</li>
<li>
<p>Add new project and repo</p>
<p><code>mkdir project</code></p>
<p><code>cd project</code></p>
<p><code>git init</code> /create a new repo</p>
</li>
<li>
<p>Stage changes and commit</p>
<p><code>git status</code></p>
<p><code>git add</code></p>
<p><code>git commit -m &quot;comments&quot;</code></p>
<p><code>git commit -a -m&quot;message&quot;</code> // A shortcut to stage any changes to tracked files and commit them in one step, only for small changes</p>
</li>
<li>
<p>Show changes in commit</p>
<p><code>git log</code> - only show logs, change author and dates</p>
<p><code>git log -p</code> or <code>git diff -u</code> - show changes with details</p>
<p><code>git show gitlog_id</code></p>
<p><code>git log --stat</code></p>
<p><code>git add -p</code> - review changes before staging them</p>
<p><code>git diff</code> - shows only unstaged changes by default</p>
<p><code>git diff --staged</code> - to show the changes that are staged but not commited</p>
</li>
<li>
<p>Remove or rename the file in the repo</p>
<p><code>git rm filename.py</code> - remove files from repo, stop the file from being tracked by git</p>
<p><code>ls -l</code> - check out the files in the repo</p>
<p><code>ls -la</code> - check out all files including hidden files</p>
<p><code>git mv new_file_name old_file_name</code> - rename the file </p>
<p>.gitigore - inside this file, we can specify rules to tell git which files to skip for the current repo. For example, <code>echo .DS_STORE &gt; .gitignore</code></p>
</li>
<li>
<p>Undo changes before committing</p>
<p><code>git checkout filename</code> - change the file back to previous state has not been staged</p>
<p><code>git reset HAD filename</code> - change the file has been staged but not commited, counterpart to <code>git add</code></p>
</li>
<li>
<p>Amend commit</p>
<p><code>git commit --amend</code> -overwrite previous commit. only for local commit, not for public commit</p>
<p><code>touch filename</code> - create files</p>
</li>
<li>
<p>Rollbacks</p>
<p><code>git revert HEAD</code> - HEAD is regarded as a point to a snapshot</p>
<p><code>git revert commit_id</code></p>
<p><code>git log -p -2</code></p>
<p>Identify a commit by <strong>Commit ID</strong></p>
</li>
</ol>
<h3><a class="header" href="#branch---a-point-to-a-particular-commit" id="branch---a-point-to-a-particular-commit">Branch - a point to a particular commit</a></h3>
<p><em>default branch - master</em></p>
<p><code>git branch</code> - check up the current branch</p>
<p><code>git branch new_branch</code> - create a new branch</p>
<p><code>git checkout new_branch</code>- <strong>check out the latest snapshot for both files and for branches</strong></p>
<p><code>git checkout -b another_branch</code> - create a new branch and swtich to it</p>
<p>When branch was changed, the working directory also changed.</p>
<p><code>git branch -d new_branch</code> - delete the branch</p>
<p><code>git merge new_branch</code> - merge a branch to another</p>
<p><strong>merge conflict</strong></p>
<p><code>git log --graph --oneline</code></p>
<p><code>git merge -abort</code> - stop merging and back to previous status</p>
<h2><a class="header" href="#case-2-return-to-previous-commit" id="case-2-return-to-previous-commit">Case 2 Return to previous commit</a></h2>
<ul>
<li>
<p><code>git lg</code> to check out the history and find out which version I want to return back (commit_ID)</p>
</li>
<li>
<p><code>git reset commit_ID</code> Be causious of using <code>git reset -hard commit_ID</code></p>
</li>
<li>
<p>Then add, commit, push. If leaving a new commit message after <code>git reset</code> operation, it will combine the last few commits that you do not want into a single commint.</p>
</li>
</ul>
<h1><a class="header" href="#visit-with-the-client--setup-overview" id="visit-with-the-client--setup-overview">Visit with the Client &amp; Setup Overview</a></h1>
<p><a href="https://balsamiq.com">layout design page</a>
<a href="https://wwww.google.com/fonts">Google fonts</a></p>
<p>browse-sync:
'browser-syn start --server --director --files &quot;**/<em>&quot;'
'browser-syn start --server --director --files &quot;</em>&quot;'</p>
<h1><a class="header" href="#frontend" id="frontend">Frontend</a></h1>
<h1><a class="header" href="#vue3-and-javascript" id="vue3-and-javascript">Vue3 and Javascript</a></h1>
<h3><a class="header" href="#how-to-create-a-dynamic-router-on-a-page" id="how-to-create-a-dynamic-router-on-a-page">How to create a dynamic router on a page</a></h3>
<p><a href="https://router.vuejs.org/guide/essentials/navigation.html">Vue-router Programmatic Navigation</a></p>
<pre><code class="language-js">const userId = '123'
router.push({ name: 'user', params: { userId } }) // -&gt; /user/123
router.push({ path: `/user/${userId}` }) // -&gt; /user/123
// This will NOT work
router.push({ path: '/user', params: { userId } }) // -&gt; /user
</code></pre>
<p>In my case,</p>
<pre><code class="language-js">// In the component of Tombview
methods: {
    open: function (userId) {
      router.push({name: 'userTomb', params: {userId}})
    }
</code></pre>
<p><a href="https://router.vuejs.org/guide/essentials/dynamic-matching.html#reacting-to-params-changes">Dynamic Route Matching</a></p>
<pre><code class="language-js">// in the &quot;router.js&quot; file
const routes = [
    {name: 'userTomb', path: '/userTomb/:userId', component: userTomb}
]
// : refers to params
</code></pre>
<pre><code class="language-js">// In the new router page
const User = {
  template: '&lt;div&gt;User {{ $route.params.id }}&lt;/div&gt;'
}
</code></pre>
<p>In my case,</p>
<pre><code class="language-js">{{ $route.params.userId }} // use this code to pass dynamic paramters.
</code></pre>
<p>When it is used in <script></script></p>
<pre><code class="language-js">{{ this.$route.params.userId }} 
</code></pre>
<h1><a class="header" href="#css-notes" id="css-notes">CSS Notes</a></h1>
<p><a href="http://www.csszengarden.com">Usefull link for CSS sstyle</a>
Note that you could not change html but only style</p>
<h2><a class="header" href="#1-css-rules" id="1-css-rules">1 CSS rules</a></h2>
<p>what this about</p>
<pre><code class="language-html">&lt;style&gt;
p {
    color: blue;
    font-size: 20px;
    width: 200px;
}

h1 {
    color: green;
    font-size: 36px;
    width: center;
}
&lt;/style&gt;
</code></pre>
<p><code>p</code> is a selector;</p>
<p>In the curly braces, there is a declaration, containing property ´´´color´´´and value ´´´blue´´´
zero or more declarations are allowed.</p>
<p>The collection of these CSS rules is what's called a stylesheet.</p>
<h2><a class="header" href="#2-css-selectors-element-class-and-id-selectors" id="2-css-selectors-element-class-and-id-selectors">2 CSS selectors: Element, Class, and ID Selectors</a></h2>
<h3><a class="header" href="#21-element-selector" id="21-element-selector">2.1 element selector</a></h3>
<p><code>&lt;p&gt; ... &lt;/p&gt;</code></p>
<h3><a class="header" href="#22-class-selector" id="22-class-selector">2.2 class selector</a></h3>
<pre><code class="language-css">.blue {
    color: blue;
}
</code></pre>
<p>In html part:
<code>&lt;p class=&quot;blue&quot;&gt;...&lt;/p&gt;</code></p>
<h3><a class="header" href="#23-id-selector" id="23-id-selector">2.3 id selector</a></h3>
<p>Can only be used once in the HTML document.</p>
<pre><code class="language-css">#name {
    color:blue;
}
</code></pre>
<p><code>&lt;p id=&quot;name&quot;&gt;...&lt;/p&gt;</code></p>
<h3><a class="header" href="#24-grouping-selectors" id="24-grouping-selectors">2.4 grouping selectors</a></h3>
<pre><code class="language-css">div, .blue{
    color: blue;
}
</code></pre>
<h2><a class="header" href="#3-combining-selectors" id="3-combining-selectors">3 Combining Selectors</a></h2>
<h3><a class="header" href="#31-element-with-class-selector" id="31-element-with-class-selector">3.1 Element with Class Selector</a></h3>
<pre><code class="language-css">//Every p that has a class = &quot;big&quot;
p.big{
    font-size: 20 px
}
</code></pre>
<p>An example:</p>
<pre><code class="language-html">&lt;p class=&quot;big&quot;&gt; ... &lt;/p&gt; // font-size: 20px
&lt;div class=&quot;big&quot;&gt; ... &lt;/div&gt;
</code></pre>
<h3><a class="header" href="#32-child-selector" id="32-child-selector">3.2 Child Selector</a></h3>
<pre><code class="language-css">//every p that is a direct child of article
article &gt; p {  
    color: blue;
}
</code></pre>
<pre><code class="language-html">&lt;article&gt;&lt;p&gt;...&lt;/p&gt;&lt;/article&gt; // only this content has blue text.
...
&lt;p&gt;...&lt;/p&gt;
&lt;article&gt;&lt;div&gt;&lt;p&gt;...&lt;/p&gt;&lt;/div&gt;&lt;/article&gt;
</code></pre>
<h3><a class="header" href="#33-descendant-selector" id="33-descendant-selector">3.3 Descendant Selector</a></h3>
<pre><code class="language-css">//every p that is inside (at any level) of article
article p {  
    color: blue;
}
</code></pre>
<pre><code class="language-html">&lt;article&gt;&lt;p&gt;...&lt;/p&gt;&lt;/article&gt; // Blue text
...
&lt;p&gt;...&lt;/p&gt; // Unaffected
&lt;article&gt;&lt;div&gt;&lt;p&gt;...&lt;/p&gt;&lt;/div&gt;&lt;/article&gt; // Blue text
</code></pre>
<h3><a class="header" href="#34-not-limited-to-element-selector" id="34-not-limited-to-element-selector">3.4 Not Limited to element Selector</a></h3>
<pre><code class="language-css">//every p that is inside (at any level) of element with class = colored&quot;&quot;
.colored p {  
    color: blue;
}
//every element with class = &quot;colored&quot; that is a direct child of article element
article &gt; .colored{  
    color: blue;
}
</code></pre>
<h3><a class="header" href="#35-summary" id="35-summary">3.5 Summary</a></h3>
<p>combining selectors</p>
<ul>
<li>Element with class selector: selector.class</li>
<li>Child(direct) selector: selector&gt;selector</li>
<li>Descendent selector: selector selector</li>
</ul>
<h2><a class="header" href="#4-pseudo-class-selector" id="4-pseudo-class-selector">4 Pseudo-Class Selector</a></h2>
<p><code>:link</code>
<code>:visited</code>
<code>:hover</code>
<code>:active</code>
<code>:nth-child</code></p>
<p>Styling links is not exactly as straight forward as styling a regular element, and that's because links have states. And these states can be expressed using our pseudo-classes. 
An example:</p>
<pre><code class="language-css">header li {
    list-style: none
}
// visited means that HTML allows that after you click a particular link that a different style can be applied to that link than an unclicked link
// In our case, however, we don't want to differentiate between the two, so we'll style them both together.
a:link, a:visited {  // &lt;a&gt; tag defines a hyperlink, which is used to link from one page to another.
    text-decoration: none;
    background-color: green;
    border: 1px solid blue;
    display: block;  // &lt;a&gt; tag is an inline element. Here we change it to a block-level element.
    width: 200px;
    text-align: center;
    margin-bottom: 1px;
}
// An active is that state when the user actually clicks on the element but hasn't yet released his click. 
a:hover, a:active {
    background-color: red;
    color: purple;
}
// the nth child pseudo-selector allows you to target a particular element within a list.
header li:nth-child(3) {
    font-size: 24px;
}
// Set every odd member has a gray backgroud.
section div:nth-child(odd) {
    background-color: gray
}
// When the cursor hovers on the 4th member, the 4th member change the color to green.
section div:nth-child(4):hover {
    background-color: green;
    cursor: pointer;
}
</code></pre>
<h2><a class="header" href="#5-style-placement" id="5-style-placement">5 Style placement</a></h2>
<h3><a class="header" href="#51-head-style-stylestyle" id="51-head-style-stylestyle">5.1 Head style <code>&lt;style&gt;...&lt;/style&gt;</code></a></h3>
<p>Head styles are usually there override external ones.</p>
<h3><a class="header" href="#52-place-style-inline" id="52-place-style-inline">5.2 Place style inline</a></h3>
<p>Great for quick testing.
<code>&lt;p style=&quot;text-align: center;&quot;&gt;...&lt;/p&gt; // not recommended</code> </p>
<h3><a class="header" href="#53-external-css-stylesheet" id="53-external-css-stylesheet">5.3 External CSS stylesheet</a></h3>
<p>Mostly-used one in real sites.
<code>&lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot; </code></p>
<h2><a class="header" href="#6-conflict-resolution" id="6-conflict-resolution">6 Conflict resolution</a></h2>
<h3><a class="header" href="#61-origin-precedence" id="61-origin-precedence">6.1 Origin Precedence</a></h3>
<ul>
<li>when in conflict
Simple rule: last declaration wins
It is based on the principal that HTML is processed sequentially top to bottom.</li>
<li>when no conflict
Sample rule: declarations merge</li>
</ul>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Cascade of CSS&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;external.css&quot;&gt; 
&lt;style&gt;

p {
  color: maroon;
}

&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Origin Example&lt;/h1&gt;
&lt;p&gt;The rule is simple: last declaration wins.&lt;/p&gt; // color: maroon
&lt;p style=&quot;color: black;&quot;&gt;If there is no conflict, declarations merge into one rule.&lt;/p&gt; // color: black
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>In external css stylesheet:</p>
<pre><code class="language-css">p {
  font-size: 130%;
  background-color: gray;
  color: white;
}
</code></pre>
<h3><a class="header" href="#62-inheritance" id="62-inheritance">6.2 Inheritance</a></h3>
<p>If you specify some CSS property on some element, all the children and grandchildren and so on and so on of that element will also inherit that property without you having to specify the property for each and every element.</p>
<h3><a class="header" href="#63-specificity" id="63-specificity">6.3 Specificity</a></h3>
<p>Most specific selector combination wines, which can be evaluated by score:</p>
<table><thead><tr><th>1</th><th align="center">1</th><th align="center">1</th><th align="right">1</th></tr></thead><tbody>
<tr><td>style=&quot;...&quot;</td><td align="center">id</td><td align="center">class, pseudo-class, attribute</td><td align="right"># of element</td></tr>
</tbody></table>
<p>For example, 
<code>div p {color: green;}</code> score = 0002
<code>div #myparag {color: blue;}</code> score = 0101
<code>div.big p {color: green;}</code> score = 0012</p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Inheritance in CSS&lt;/title&gt;
&lt;style&gt;
header.navigation p { // score = 0012
  color: blue;
}
p.blurb { // score = 0011 
  color: red;
}
p {
  color: green !important;  // !important will override over specificity.
}
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;header class=&quot;navigation&quot;&gt;
  &lt;p class=&quot;blurb&quot;&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit. Vero soluta enim aut! Nihil nam obcaecati, fugiat sint sit libero voluptate eos incidunt odio neque cum, dignissimos aperiam, magnam nisi debitis.&lt;/p&gt;
&lt;/header&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h2><a class="header" href="#7-styling-text" id="7-styling-text">7 Styling Text</a></h2>
<pre><code class="language-css">.style {
    font-family: Arial, Helvetica, sans-serif;
    color: #0000ff; // first '00': red; middle '00': green: last '00': blue
    font-style: italic;
    font-weight: bold;
    font-size: 24px;
    text-transform: capitalize;
    text-align: center;
    
}

body {
    font-size: 120%; // 120 % by default
}
</code></pre>
<pre><code class="language-html">body {
    font-size: 120%; // default font = 16px; current font = 19px;
}
&lt;div style=&quot;font-size: 2em;&quot;&gt; 2em text // font size is two times the currect font - 38px
&lt;div style=&quot;font-size: 2em;&quot;&gt; 4em text // font size = 76px
&lt;div style=&quot;font-size: .5em;&gt; 2em again! &lt;/div&gt; // font size = 76px
&lt;/div&gt; 
&lt;/div&gt; 
</code></pre>
<h2><a class="header" href="#8-the-box-model" id="8-the-box-model">8 The Box Model</a></h2>
<h3><a class="header" href="#81-box-sizing" id="81-box-sizing">8.1 box-sizing</a></h3>
<p>The box composes of margin, border, and padding.
<code>box-sizing: border-box;</code> The width refers to the whole box, which is hihgly recommended.
or <code>box-sizing: content-box;</code> The width refers to the content only, the default setting.
However, it should be noted that the <code>box-sizing</code> property does not inherit. To solve the problem, we can use <code>*</code> selector, which can apply the CSS style inside to all the elements.</p>
<pre><code class="language-css">* {
    box-sizing:border-box;
}
</code></pre>
<h3><a class="header" href="#82-cumulative-margins" id="82-cumulative-margins">8.2 Cumulative Margins</a></h3>
<ul>
<li>Horizontal margins are cumulative.</li>
<li>Vertical magins from two elements will collapse, and larger margin wins.</li>
</ul>
<h3><a class="header" href="#83-content-overflow" id="83-content-overflow">8.3 Content overflow</a></h3>
<p><code>overflow: auto</code>
<code>overflow: scroll</code>
<code>overflow: hidden</code>
<code>overflow: invisible</code></p>
<h2><a class="header" href="#9-background-properties" id="9-background-properties">9 Background properties</a></h2>
<pre><code class="language-html">&lt;body&gt;
&lt;h1&gt;The background property&lt;/h1&gt;
&lt;div id=&quot;bg&quot;&gt;Wolala&lt;/div&gt;
&lt;/body&gt;
</code></pre>
<pre><code class="language-css">#bg {
    width: 500px;
    height: 500px;
    background-color: blue;
    background-image: url('cat.png') // Use an image as a background.
    background-repeat: no-repeat // repeat images or not.
    background-position: top right // set image position
    // or background: url('cat.png') no-repeat right center blue
}
</code></pre>
<h2><a class="header" href="#10-position-elements" id="10-position-elements">10 Position Elements</a></h2>
<h3><a class="header" href="#101-by-floating" id="101-by-floating">10.1 by Floating</a></h3>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Two Column Design&lt;/title&gt;
&lt;style&gt;

* {
  box-sizing: border-box;
}

div {
  /*background-color: #00FFFF;*/
}
p {
  width: 50%;
  /*border: 1px solid black;*/
  float: left;  // float to the left of the last element.
  padding: 10px;
}

#p1 {
  /*background-color: #A52A2A;*/
}
#p2 {
  /*background-color: #DEB887;*/
}

section {
  clear: left;
}

&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Two Column Design&lt;/h1&gt;

&lt;div&gt;
  &lt;p id=&quot;p1&quot;&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit. Quia distinctio aliquid cupiditate perferendis fuga, sit quasi alias vero sunt non, ratione earum dolores nihil! Consequuntur pariatur totam incidunt soluta expedita.&lt;/p&gt;
  &lt;p id=&quot;p2&quot;&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit. Dicta beatae voluptatibus veniam placeat iure unde assumenda porro neque voluptate esse sit magnam facilis labore odit, provident a ea! Nulla, minima.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Eius nemo vitae, cupiditate odio magnam reprehenderit esse eum reiciendis repellendus incidunt sequi! Autem, laudantium, accusamus. Doloribus tempora alias minima laborum, provident!&lt;/p&gt;
  &lt;section&gt;This is regular content continuing after the the paragraph boxes.&lt;/section&gt;
&lt;/div&gt;


&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h3><a class="header" href="#102-relative-and-absolute-element-positioning" id="102-relative-and-absolute-element-positioning">10.2 Relative and Absolute Element Positioning</a></h3>
<ul>
<li>
<p>Static positioning
Normal document flow. 
Default for all, except html.</p>
</li>
<li>
<p>Relative Positioning
Element is positioned relative to its position in normal document flow.
Positioning CSS(offset) properties are: top, bottom, left, right.
Html positioning is defaulted by relative</p>
</li>
<li>
<p>Absolute Positioning
All offsets(top, bottom, left, right) are relative to the position of the nearst ancestor which has positioning set on it, other than static.</p>
</li>
</ul>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Positioning Elements&lt;/title&gt;
&lt;style&gt;
* {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}
h1 {
  margin-bottom: 15px;
}

div#container {
  background-color: #00FFFF;
  position: relative;
  top: 60px; // equivalent to 'from top'
}
p {
  width: 50px;
  height: 50px;
  border: 1px solid black;
  margin-bottom: 15px;
}
#p1 {
  background-color: #A52A2A;
  position: relative;
  top: 65px;
  left: 65px;
}
#p2 {
  background-color: #DEB887;
}
#p3 {
  background-color: #5F9EA0;
  position: absolute; // the absolute positioning needs a relative or an absolute parent or an ancestor.
  top: 0;
  left: 0;
}
#p4 {
  background-color: #FF7F50;
}

&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Positioning Elements&lt;/h1&gt;

&lt;div id=&quot;container&quot;&gt;
  &lt;p id=&quot;p1&quot;&gt;&lt;/p&gt;
  &lt;p id=&quot;p2&quot;&gt;&lt;/p&gt;
  &lt;p id=&quot;p3&quot;&gt;&lt;/p&gt;
  &lt;p id=&quot;p4&quot;&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;

</code></pre>
<h2><a class="header" href="#11-media-query-syntax" id="11-media-query-syntax">11 Media Query Syntax</a></h2>
<pre><code class="language-css">@media (max-width: 767px){ // media feature (resolves to true or false)
   p {
     color: blue;
   }
</code></pre>
<h3><a class="header" href="#media-query-common-features" id="media-query-common-features">Media Query Common Features</a></h3>
<p><code>@media(max-width: 800px) {...}</code></p>
<p><code>@media(max-width: 800px) {...}</code></p>
<p><code>@media(orientation: portrait){...}</code></p>
<p><code>@media screen{...}</code></p>
<p><code>@media print{...}</code></p>
<h3><a class="header" href="#media-query-common-logical-operators" id="media-query-common-logical-operators">Media Query Common Logical Operators</a></h3>
<ul>
<li>
<p>Devices with width within a range
<code>@media(min-width: 768px) and (max-width: 991px){...}</code></p>
</li>
<li>
<p>Comma is equivalent to OR
<code>@media(max-width: 768px), (min-width: 991px){...}</code></p>
</li>
</ul>
<h3><a class="header" href="#media-query-common-approach" id="media-query-common-approach">Media Query Common Approach</a></h3>
<pre><code class="language-css">p {color: blue;} // base styles
@media(min-witdh: 1200px)
@media(min-width:992px) and (max-width:1199px) 
// Be sure that two sizes are not overlapped.
</code></pre>
<h3><a class="header" href="#an-example-for-how-to-use-media-queries" id="an-example-for-how-to-use-media-queries">An example for how to use media queries</a></h3>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Media Queries&lt;/title&gt;
&lt;style&gt;

/********** Base styles **********/
h1 {
  margin-bottom: 15px;
}

p {
  border: 1px solid black;
  margin-bottom: 15px;
}
#p1 {
  background-color: #A52A2A;
  width: 300px;
  height: 300px;
}
#p2 {
  background-color: #DEB887;
  width: 50px;
  height: 50px;
}

/********** Large devices only **********/
@media (min-width: 1200px){
    #p1 {
        width: 80%; 
    // p1 at width 1200 pixels or wider will take 80% of our screen
    // when it is below 1200px, the p1 will go back to the original size.
    }
    #p2 {
        width: 150px;
        height: 150px;
    }
}

/********** Medium devices only **********/
@media (min-width: 992px) and (max-width: 1199px){
    #p1{
        width: 50%;
    }
    #p2 {
        width: 100px;
        height: 100px;
    }
}


&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Media Queries&lt;/h1&gt;

&lt;p id=&quot;p1&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;p2&quot;&gt;&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h3><a class="header" href="#summary" id="summary">Summary</a></h3>
<ul>
<li>Basic syntax of a media query
** @media(media feature)
** @media(media feature) logical operator (media feature)</li>
<li>Remember not to overlap breakpoints</li>
<li>Usually, you provide base styling. Then change or add to them in each media query.</li>
</ul>
<h2><a class="header" href="#12-responsive-design" id="12-responsive-design">12 Responsive Design</a></h2>
<p>what is a responsive website? 
It is a site that's designed to adapt its layout to the viewing environment by using fluid, proportion-based grids, flexible images, and CSS3 media queries. 
12 columns grid responsive layout
<a href="https://github.com/jhu-ep-coursera/fullstack-course4/blob/master/examples/Lecture24/responsive-before.html">Checkout here for an example.</a></p>
<h3><a class="header" href="#121-introduction-to-bootstrap" id="121-introduction-to-bootstrap">12.1 Introduction to Bootstrap</a></h3>
<p>Bootstrap is the most popular HTML, CSS and JS framework for developing responsive, mobile first projects on the web.
[https://getbootstrap.com/]
bootstrap depends on jQuery
S0 jQuery also needs to be download.</p>
<h3><a class="header" href="#122-bootstrap-grid-system" id="122-bootstrap-grid-system">12.2 Bootstrap grid system</a></h3>
<pre><code class="language-html">&lt;div class=&quot;container&quot;&gt;  // your Bootstrap grid always has to be inside of a container wrapper or .container-fluid.
    &lt;div class=&quot;row&quot;&gt; // The row class also creates a negative margin, to counteract the padding that the container class sets up.
        &lt;div class=&quot;col-md-4&quot;&gt;Col 1&lt;/div&gt;
        ...
    &lt;/div&gt;
&lt;/div&gt;
</code></pre>
<h4><a class="header" href="#1221-column-class-template" id="1221-column-class-template">12.2.1 Column class template</a></h4>
<p><code>col-SIZE-SPAN</code></p>
<ul>
<li>SIZE 
screen width range identifier
columns will collapes (i.e., stack) below that width, unless another rule applies</li>
<li>SPAN
How many columns element should span
values: 1 through 12</li>
</ul>
<pre><code class="language-html">&lt;header class=&quot;container&quot;&gt;  // your Bootstrap grid always has to be inside of a container wrapper.
    &lt;nav class=&quot;row&quot;&gt; // The row class also creates a negative margin, to counteract the padding that the container class sets up.
        &lt;div class=&quot;col-md-4&quot;&gt;Col 1&lt;/div&gt;
        ...
    &lt;/nav&gt;
&lt;/header&gt;
</code></pre>
<h1><a class="header" href="#sql" id="sql">SQL</a></h1>
<h1><a class="header" href="#duke-coursera" id="duke-coursera">Duke-coursera</a></h1>
<pre><code class="language-sql">SELECT TOP 3 * FROM Customers;
</code></pre>
<p>Next is about <strong>JOIN</strong>.</p>
<h1><a class="header" href="#python" id="python">Python</a></h1>
<h1><a class="header" href="#python-course-university-of-michigan" id="python-course-university-of-michigan">Python Course (University of Michigan)</a></h1>
<p>https://www.coursera.org/learn/python/home/welcome</p>
<blockquote>
<p>I took the course in Feb. 2018 without any coding experience before. However, I do not feel I really get Python because of lack of practice/exercise.<br>
My interests start moving to data analysis recently, and I realized Python is a powerful tool in the world of data analytics. Therefore, I tried to pick up the course again and hopefully I could know it better this time (after I finished CS50 last year) </p>
<div style="text-align: right"> — April 2021 </div>
</blockquote>
<br>
<p>Tips for writing Python (or any other code) 
Good names for variables
Comments - documentation</p>
<h2><a class="header" href="#converting-user-input" id="converting-user-input">Converting User Input</a></h2>
<pre><code class="language-python"># convert floor number from Europe system to US system
inp=input('Europe floor?')
usf=int(inp) + 1
print('US floor', usf)

def abc():
</code></pre>
<p>\( \int x dx = \frac{x^2}{2} + C \)</p>
<p>\[ 
\mu = \frac{1}{N} \sum_{i=0} x_i \\
\int_0^\infty \mathrm{e}^{-x},\mathrm{d}x
\]
https://en.wikibooks.org/wiki/LaTeX/Mathematics</p>
<h1><a class="header" href="#learning-in-a-hard-way" id="learning-in-a-hard-way">Learning in a hard way</a></h1>
<h2><a class="header" href="#numpyarray" id="numpyarray">numpy.array</a></h2>
<p>When creates a numpy.array <code>a = numpy.array([1,2,3])</code>. It is 1-dimensional if not specified. The shape of a can be checked out by <code>a.shape</code>, and it will output <code>(3,)</code>.</p>
<p>The number of dimensions will be transformed from 1 to 2 by exploting </p>
<ul>
<li>
<p><code>a1 = a.reshape(1,3)</code>, giving an output <code>array([[1,2,3]])</code></p>
</li>
<li>
<p><code>a2 = a.reshape(3,1)</code>, giving an output </p>
</li>
</ul>
<pre><code class="language-py">array([[1],
       [2],
       [3]])
</code></pre>
<p><strong>array.sum</strong></p>
<ul>
<li>array.sum(axis=0): sum up along the column</li>
</ul>
<pre><code class="language-py">a1.sum(axis=0)
#output: array([1, 2, 3])

a2.sum(axis=0)
#output: array([6])
</code></pre>
<ul>
<li>array.sum(axis=1): sum up along the row</li>
</ul>
<pre><code class="language-py">a1.sum(axis=1)
#output: array([6])

a2.sum(axis=1)
#output: array([1, 2, 3]) -&gt; 1D array?
</code></pre>
<p><code>a.sum(axis=1)</code> and <code>a.sum(axis=0)</code> give the same output <code>array([6])</code> because of only one dimension.</p>
<p><img src="https://user-images.githubusercontent.com/41487483/119228773-d0b20600-bb14-11eb-9fa9-93e697500f1c.png" alt="image" /></p>
<h2><a class="header" href="#easy-coding" id="easy-coding">Easy coding</a></h2>
<ol>
<li>´x = x + 1´ is qual to ´x += 1´</li>
</ol>
</br>
<h2><a class="header" href="#virtualenv---virtual-environment-manager" id="virtualenv---virtual-environment-manager">Virtualenv - virtual environment manager</a></h2>
<p><code>venv</code> for Python 3 or <code>virtualen</code> for Python 2</p>
<p><a href="https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment">Installing packages using pip and virtual environments</a></p>
<p><a href="https://docs.python.org/3/library/venv.html#module-venv">Installing venv</a></p>
<h3><a class="header" href="#this-is-how-i-did-for-my-energy-data-project" id="this-is-how-i-did-for-my-energy-data-project">This is how I did for my &quot;Energy-data&quot; project:</a></h3>
<p>Copy the following code into &quot;init_py.sh&quot; file</p>
<pre><code>#!/bin/bash
set -e

PYTHON_ENV_NAME=venv

pip3 install virtualenv

# or 'sudo pip3 install virtualenv'

virtualenv -p python3 $PYTHON_ENV_NAME

echo &quot;source $(pwd)/$PYTHON_ENV_NAME/bin/activate&quot; &gt; .env

source $(pwd)/$PYTHON_ENV_NAME/bin/activate # activate the local python environment

pip3 install jupyter
pip3 install matplotlib
pip3 install pandas
pip3 install scipy
pip3 install seaborn
pip3 install graphviz
pip3 install scikit-learn

echo -e &quot;\n&quot;
echo &quot;Please run \&quot;$ source $PYTHON_ENV_NAME/bin/activate\&quot; to switch to the python environment.&quot;
echo &quot;Use \&quot;$ deactivate\&quot; anytime to deactivate the local python environment if you want to switch back to your default python.&quot;
echo &quot;Or install autoenv as described on project readme file to make your life much easier.&quot;
</code></pre>
<h3><a class="header" href="#other-easy-ways-to-do" id="other-easy-ways-to-do">Other easy ways to do</a></h3>
<p><a href="https://www.youtube.com/watch?v=N5vscPTWKOk">Video source from Corey Schafer</a></p>
<p><a href="https://www.pythonforbeginners.com/basics/how-to-use-python-virtualenv">Text insruction</a></p>
<h1><a class="header" href="#statistics-rewind" id="statistics-rewind">Statistics rewind</a></h1>
<h1><a class="header" href="#probability---the-science-of-uncertainty-and-data-2021" id="probability---the-science-of-uncertainty-and-data-2021">Probability - The Science of Uncertainty and Data (2021)</a></h1>
<p>Use the course to re-build my statistics knowledge.</p>
<p><a href="https://www.edx.org/course/probability-the-science-of-uncertainty-and-data">The course link</a></p>
<p><a href="https://ece307.cankaya.edu.tr/uploads/files/introduction%20to%20probability%20(bertsekas,%202nd,%202008).pdf">The Book</a></p>
<h2><a class="header" href="#1-sample-space-and-probability" id="1-sample-space-and-probability">1 Sample Space and Probability</a></h2>
<h3><a class="header" href="#11-sample-space---a-set-of-outcomes" id="11-sample-space---a-set-of-outcomes">1.1 Sample space - A set of outcomes</a></h3>
<ul>
<li>
<p>discrete/finite example</p>
</li>
<li>
<p>continuous example</p>
</li>
</ul>
<h3><a class="header" href="#12-probability-axioms" id="12-probability-axioms">1.2 Probability Axioms</a></h3>
<ul>
<li>
<p>Nonnegativity
\(P(A) \geq 0 \)</p>
</li>
<li>
<p>Normalization
\( P( \Omega ) = 1 \), \(\Omega \) is the entire sample space.</p>
</li>
<li>
<p>(finite) Additivity: A and B are disjoint, then the probability of their unions satisfies \(P(A \cup B) = P(A) + P(B)\) (to be strengthened later)</p>
</li>
</ul>
<h4><a class="header" href="#121-simple-consequences-of-the-axioms" id="121-simple-consequences-of-the-axioms">1.2.1 Simple consequences of the axioms</a></h4>
<ol>
<li>
<p>For a sampe space consist of a finite number of disjointed events,
\[ 
P({s_1, s_2, ...., s_n}) = P(s_1) + P(s_2) + ...... P(s_n)
\]</p>
</li>
<li>
<p>\(A \subset B\), then \(P(A) \leq P(B)\)</p>
</li>
<li>
<p>\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</p>
</li>
<li>
<p>\(P(A \cup B) \leq P(A) + P(B))\)</p>
</li>
</ol>
<h3><a class="header" href="#13-probability-calculations" id="13-probability-calculations">1.3 Probability calculations</a></h3>
<h4><a class="header" href="#131-uniform-probability-law" id="131-uniform-probability-law">1.3.1 Uniform Probability Law</a></h4>
<ul>
<li>
<p>Discrete example</p>
<p>If the sample space consists of n possible outcomes which are equally likely (i.e., all single-element events have the same probability),
\[ 
P(A) = \frac{\text{number of elements of A}}{n}
\]</p>
</li>
<li>
<p>continuous example</p>
<p>probability = area</p>
</li>
</ul>
<h4><a class="header" href="#132-discrete-but-infinite-sample-space" id="132-discrete-but-infinite-sample-space">1.3.2 Discrete but infinite sample space</a></h4>
<ul>
<li>
<p>Sample space: {1, 2, 3 ....}</p>
<p>Given \(P(n) = \frac{1}{2^n}\), n = 1, 2, 3....</p>
<p>As \( P(\Omega) = 1 \): \(\frac{1}{2} + \frac{1}{4} + ....=  \sum\limits_{n=1}^\infty \frac{1}{2^n} = \frac{1}{2}\sum\limits_{n=0}^\infty \frac{1}{2^n} = \frac{1}{2}\frac{1}{1-1/2} = 1\) </p>
</li>
</ul>
<h4><a class="header" href="#133-countable-aditivity-axiom" id="133-countable-aditivity-axiom">1.3.3 Countable aditivity axiom</a></h4>
<p><em>Additivity holds only for &quot;<strong>countable</strong>&quot; sequences of events</em></p>
<p>If \(A_1, A_2, A_3 ...\) is an \(\underline{\text{infinite sequence of disjoined events}}\),</p>
<p>\[
P(A_1 \cup A_2 ......) = P(A_1) + P(A_2) + ......
\]</p>
<br>
<h3><a class="header" href="#14-mathematical-background" id="14-mathematical-background">1.4 Mathematical background</a></h3>
<h4><a class="header" href="#141-sets---a-collection-of-distinc-elements" id="141-sets---a-collection-of-distinc-elements">1.4.1 Sets - A collection of distinc elements</a></h4>
<ul>
<li>
<p>finite: e.g. {a, b, c, d}</p>
</li>
<li>
<p>infinite: the reals (R)</p>
</li>
<li>
<p>\( \Omega \) - the universal set</p>
</li>
<li>
<p>Ø - empty set</p>
</li>
</ul>
<p><em>What are reals?</em></p>
<p><em>The reals include rational numbers (terminating decimals and non-terminating recurring decimals and irrational numbers (non-terminating non-reccuring decimals</em></p>
<h4><a class="header" href="#142-unions-and-intersection" id="142-unions-and-intersection">1.4.2 Unions and intersection</a></h4>
<h4><a class="header" href="#143-de-morgans-law" id="143-de-morgans-law">1.4.3 De Morgans' Law</a></h4>
<ul>
<li>
<p>\( (S \cap T)^c = S^c \cup T^c \) and \( (S \cup T)^c = S^c \cap T^c \)</p>
</li>
<li>
<p>\( (S^c \cap T^c)^c = S \cup T \)</p>
</li>
</ul>
<h4><a class="header" href="#144-other-important-mathematical-backgrounds" id="144-other-important-mathematical-backgrounds">1.4.4 Other important mathematical backgrounds</a></h4>
<ul>
<li>
<p>Sequences and their limits </p>
<p><em>squence: an enumerated collection of objects</em></p>
</li>
<li>
<p>When does a sequence converge</p>
<ul>
<li>
<p>if \(a_i \leq a_{i+1}\)</p>
<ul>
<li>
<p>the sequence &quot;converge to \(\infty\)&quot;</p>
</li>
<li>
<p>the sequence converge to some real number a </p>
</li>
</ul>
</li>
<li>
<p>if \(|a_i - a| \leq b\), for \(b_i \to 0\), then \(a_i \to a\)</p>
</li>
</ul>
</li>
<li>
<p>Infinite series</p>
<p><em>series(infinte sums) vs. summation(finite sums)</em></p>
<p>\(\sum\limits_{n=1}^\infty a_i = \lim\limits_{n\to\infty}\sum\limits_{i=1}^n a_i\) </p>
<ul>
<li>
<p>\(a_i \leq 0\): limit exists</p>
</li>
<li>
<p>if term \(a_i\) do not all have the same sign:</p>
<p>a. limit does not exist</p>
<p>b. limit may exist but be different if we sum in a different order</p>
<p>c. <strong>Fact</strong>: limit exists and independent of order of summation if  \(\sum\limits_{n=1}^\infty |a_i| \leq \infty\) </p>
</li>
</ul>
</li>
<li>
<p>Geometric series (等比数列、等比级数)</p>
<p>\(\sum\limits_{i=0}^\infty a^i = 1 + a + a^2 + ...... = \frac{1}{1-a} \text{           |a| &lt; 1} \)</p>
</li>
</ul>
<h3><a class="header" href="#14-sets" id="14-sets">1.4 Sets</a></h3>
<h4><a class="header" href="#141-countable-and-uncountable-infinite-sets" id="141-countable-and-uncountable-infinite-sets">1.4.1 Countable and uncountable infinite sets</a></h4>
<ul>
<li>
<p>Countable</p>
<ul>
<li>
<p>integers, pairs of positive integers, etc.</p>
</li>
<li>
<p>rational numbers q (有理数), with 0 &lt; q &lt; 1</p>
</li>
</ul>
</li>
<li>
<p>Uncountable - <em>continuous numbers</em></p>
<ul>
<li>
<p>the interval [0, 1]</p>
</li>
<li>
<p>the reals, the plane, etc.</p>
</li>
</ul>
<p><em>How to prove the reals are uncountable - &quot;Control's diagonalization argument&quot;</em></p>
</li>
</ul>
<br>
<h2><a class="header" href="#unit-2-conditioning-and-independence" id="unit-2-conditioning-and-independence">Unit 2 Conditioning and independence</a></h2>
<p>Refer to Section 1.3 - 1.5 in the textbook</p>
<h3><a class="header" href="#21-conditional-and-bayes-rules" id="21-conditional-and-bayes-rules">2.1 Conditional and Bayes' Rules</a></h3>
<h4><a class="header" href="#211-the-definition-of-conditional-probability" id="211-the-definition-of-conditional-probability">2.1.1 The definition of conditional probability</a></h4>
<p>P(A|B) = &quot;probability of A, given that B occurred&quot;</p>
<p>\[
P(A|B) = \frac{P(A \cap B )}{P(B)}
\]</p>
<p>defined only when P(B) &gt; 0</p>
<h4><a class="header" href="#212-conditional-probabilities-share-properties-of-ordinary-probabilities" id="212-conditional-probabilities-share-properties-of-ordinary-probabilities">2.1.2 Conditional probabilities share properties of ordinary probabilities</a></h4>
<ul>
<li>
<p>\(P(A|B) \geq 0\)</p>
</li>
<li>
<p>\(P(\Omega|B) = 1\)</p>
</li>
<li>
<p>\(P(B|B) &lt; 0\)</p>
</li>
<li>
<p>If \(A \cap C = Ø\), then \(P(A \cup C|B) = P(A|B) + P(C|B)\) also only applies to countable and finite sequence (countable additivity axioms).</p>
</li>
</ul>
<h4><a class="header" href="#213-models-base-on-conditional-probabilities" id="213-models-base-on-conditional-probabilities">2.1.3 Models base on conditional probabilities</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/117574884-993c6600-b0df-11eb-9125-c5501b77d001.png" alt="image" /></p>
<p><strong>1. The multiplication rule</strong></p>
<pre><code>\\(P(A \cap B) = P(B)P(A|B) = P(A)P(B|A)\\)

\\(P(A^c \cap B \cap C^c) = P(A^c \cap B) P(C^c|A^c \cap B) = P(A^c) P(B|A^c) P(C^c|A^c \cap B)\\)

\\(P(A_1 \cap A_2...\cap A_n) = P(A_1) \prod\limits_{i=2}^n P(A_i|A_1 \cap A_2...\cap A_i)\\)
</code></pre>
<p><img src="https://user-images.githubusercontent.com/41487483/118396328-55051480-b64f-11eb-9c9e-9703528df69e.png" alt="image" /></p>
<p><strong>2. Total probability theorem</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118396377-9e556400-b64f-11eb-8e1a-f5530bb63fd3.png" alt="image" /></p>
<p><strong>3. Bayes' rules</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118396464-eeccc180-b64f-11eb-9080-e7e86ad223bf.png" alt="image" /></p>
<h3><a class="header" href="#22-independence" id="22-independence">2.2 Independence</a></h3>
<h4><a class="header" href="#221-conditional-independence" id="221-conditional-independence">2.2.1 Conditional independence</a></h4>
<p>Independent of two events</p>
<ul>
<li>
<p>Intuitive &quot;definition&quot;: P(B|A) = P(B) </p>
<ul>
<li>Occurence of A provides no new information about B</li>
</ul>
</li>
</ul>
<p>Definition of independence:</p>
<p>\(P(A \cap B) = P(A) \times P(B)\)</p>
<p><em>whether two events disjoined or joined is not associated with independence</em></p>
<p><strong>Independent of events complements</strong></p>
<p>If A and B are independent, then A and \(B^c\) are independent. </p>
<p><strong>Independent of events complements</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118397139-df9b4300-b652-11eb-9e1c-e8b293e070fc.png" alt="image" /></p>
<p><strong>Conditioning may affect independence</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118397436-35241f80-b654-11eb-89cf-d6eacefd924e.png" alt="image" /></p>
<h4><a class="header" href="#222-independence-of-a-collection-of-events" id="222-independence-of-a-collection-of-events">2.2.2 Independence of a collection of events</a></h4>
<ul>
<li>
<p>Intuitive &quot;definition&quot;: Information on some of the events does not change probabilities related to the remaining events</p>
</li>
<li>
<p>Definition: Events \(A_1, A_2,....., A_n\) are called independent if: </p>
<p>\(P(A_i \cap A_j \cap .... \cap A_m) = P(A_i)P(A_j)...P(A_m)\)</p>
</li>
</ul>
<p><strong>Pairwise independence</strong></p>
<p>n = 3:</p>
<p>\(P(A_1 \cap A_2) = P(A_1)P(A_2)\)</p>
<p>\(P(A_1 \cap A_3) = P(A_1)P(A_3)\)</p>
<p>\(P(A_2 \cap A_3) = P(A_2)P(A_3)\)</p>
<p>vs. 3-way indenpendence</p>
<p>\(P(A_1 \cap A_2 \cap A_3) = P(A_1)P(A_2)P(A_3)\)</p>
<p><strong>Independence vs. pairwise independence</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/118398068-4d496e00-b657-11eb-91d3-ade5aa82f245.png" alt="image" /></p>
<h4><a class="header" href="#223-reliability" id="223-reliability">2.2.3 Reliability</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/118398123-7bc74900-b657-11eb-8f00-9313ffc249f6.png" alt="image" /></p>
<h2><a class="header" href="#unit-3-couting" id="unit-3-couting">Unit 3 Couting</a></h2>
<h3><a class="header" href="#31-basic-counting-principle" id="31-basic-counting-principle">3.1 Basic counting principle</a></h3>
<p>r stages and \(n_i\) choices at stage i give the total number of possible choices \( n_1 * n_2 * ....n_r \)</p>
<h3><a class="header" href="#32-permutation" id="32-permutation">3.2 Permutation</a></h3>
<ul>
<li><strong>Permutation</strong> - number of ways of ordering n elements (repetition is prohibited)</li>
</ul>
<p>\[n * (n-1) * (n-2) * ... * 2 * 1 = n!\]</p>
<ul>
<li>Number of subsets of {1, 2, ...n} = \(2^n\)</li>
</ul>
<h3><a class="header" href="#33-combinations" id="33-combinations">3.3 Combinations</a></h3>
<ul>
<li>
<p><strong>combinations</strong> \(\binom{n}{k}\)- number of <em>k</em>-element subsets of a given <em>n</em>-element set</p>
<p><em>How is combination equation derived?</em></p>
<p>Two ways of constructing an <strong>ordered</strong> sequence of <em>k</em> <strong>distinct</strong> items:</p>
<ul>
<li>
<p>choose the <em>k</em> items one at a time: </p>
<p>\[
n (n-1) ... (n-k+1) = \frac{n!}{k!(n-k)!}
\]</p>
</li>
<li>
<p>choose <em>k</em> items, then order them:</p>
<p>\[
\left(
\begin{array}{c}
n \\
k
\end{array}
\right)k!
\]</p>
</li>
</ul>
<p>There we have 
\[
\left(
\begin{array}{c}
n \\
k
\end{array}
\right) = \frac{n!}{k!(n-k)!}
\]</p>
</li>
</ul>
<h3><a class="header" href="#33-binominal-coeffficient" id="33-binominal-coeffficient">3.3 Binominal coeffficient</a></h3>
<ul>
<li>
<p><strong>Binominal coeffficient</strong> \(\binom{n}{k}\) - Binomial probabilities</p>
<p>Toss coins n times and each toss is given independent, P(Head) = p</p>
<p>\[
P(\text{k heads}) = \binom{n}{k}p^k (1-p)^{n-k}
\]</p>
<p>If asking P(k heads without ordered), then </p>
<p>\[
P(\text{k heads}) = p^k (1-p)^{n-k}
\]</p>
<p>Therefore, \(\binom{n}{k}\) is the number of <em>k</em>-head sequence</p>
</li>
</ul>
<h3><a class="header" href="#34-partitions" id="34-partitions">3.4 Partitions</a></h3>
<p><img src="https://user-images.githubusercontent.com/41487483/119098744-73d02600-ba16-11eb-8406-20acdf555e25.png" alt="image" /></p>
<ul>
<li>
<p><strong>multinomial coeffecient</strong> (number of partitions) =</p>
<p>\[
\frac{n!}{n_1! n_2! ... n_r!}
\]</p>
</li>
</ul>
<p>If r = 2, then \(n_1 = k\) and \(n_2 = n - k\). There is \(\frac{n!}{n! (n-k)!}\) which is \(\binom{n}{k}\)</p>
<ul>
<li>A simple example</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/119102618-83516e00-ba1a-11eb-86e6-b87e6ecf3467.png" alt="image" /></p>
<p><img src="https://user-images.githubusercontent.com/41487483/119102964-e8a55f00-ba1a-11eb-9693-d29eb512ac3f.png" alt="image" /></p>
<h2><a class="header" href="#4-discrete-random-variables" id="4-discrete-random-variables">4 Discrete random variables</a></h2>
<h3><a class="header" href="#41-probability-mass-function-pmf" id="41-probability-mass-function-pmf">4.1 Probability mass function (PMF)</a></h3>
<p><strong>Random variable</strong>(r.v.): a function from the sample space to the real numbers, notated as X.</p>
<p><strong>PMF</strong>: probability distribution of X </p>
<p>\[
p_X(x) = P(X = x) = P({w \in \Omega, s.t. X(\omega) = x})
\]</p>
<h3><a class="header" href="#42-discrete-random-variable-examples" id="42-discrete-random-variable-examples">4.2 Discrete Random variable examples</a></h3>
<h4><a class="header" href="#421-bernoulli-random-variables" id="421-bernoulli-random-variables">4.2.1 Bernoulli random variables</a></h4>
<p>with parameter \(p \in [0,1]\)</p>
<p>\[
p_X(x) = \begin{cases} 1, p(x) = p \\ 0, p(x) = 1 - p \end{cases} 
\]</p>
<ul>
<li>
<p>Models a trial that results in either success/failure, Heads/Tails, etc.</p>
</li>
<li>
<p><strong>Indicator random variables</strong> of an event A, \(I_A\) iff A occurs</p>
</li>
</ul>
<h4><a class="header" href="#422-uniform-random-variables" id="422-uniform-random-variables">4.2.2 Uniform random variables</a></h4>
<p>with paramters a,b</p>
<ul>
<li>
<p>Experiment: pick one of a, a+1 .... b at a random; all equally likely</p>
</li>
<li>
<p>Sample space; {a, a + 1, .... b}</p>
</li>
<li>
<p>Random variables X: \(X(\omega) = \omega\)</p>
</li>
</ul>
<h4><a class="header" href="#423-binomial-random-variables" id="423-binomial-random-variables">4.2.3 Binomial random variables</a></h4>
<p>with parameters: pasitive integer \(n; p \in [0,1]\)</p>
<ul>
<li>
<p>Experiment: n independent toses of a coin with P(Heads) = p</p>
</li>
<li>
<p>Sample space: set of sequences of H and T of length n</p>
</li>
<li>
<p>Random variables X: number of Heads observed</p>
</li>
<li>
<p>Model of: number of successes in a given number of independent trials </p>
</li>
</ul>
<p>\[
p_X(k) = \left(\begin{array}{c} n \\ k \end{array}  \right)p^k(1-p)^{n-k}, k = 0, 1 ..., n
\]</p>
<h4><a class="header" href="#424-geometric-random-variables" id="424-geometric-random-variables">4.2.4 Geometric random variables</a></h4>
<p>with parameter p: 0 &lt; p ≤ 1</p>
<ul>
<li>
<p>Experiment: infinitely many independent tosses of a coin: P(Heads) = p</p>
</li>
<li>
<p>Random variable X: number of tosses until the first Heads</p>
</li>
<li>
<p>Model of waiting times; number of tirals until a success</p>
</li>
</ul>
<p>\[
p_X(k) = P(X = k) = P(T...TH) =(1-p)^{k-1}p, k = 1,2,3...<br />
\]</p>
<h3><a class="header" href="#43-expectationmean-of-a-random-variable" id="43-expectationmean-of-a-random-variable">4.3 Expectation/mean of a random variable</a></h3>
<ul>
<li>
<p><strong>Definition</strong>: </p>
<p>\[
E[X] = \sum\limits_{x} xp_X(x)
\]</p>
</li>
<li>
<p>Interpretation: average in large number of independet repetitions of the experiment</p>
</li>
<li>
<p>Elementary properties</p>
<ul>
<li>
<p>If X ≥ 0, then E(X) ≥ 0</p>
</li>
<li>
<p>If a ≤ X ≤ b, then a ≤ E[X] ≤ b</p>
</li>
<li>
<p>If c is a constant, E[c] = c</p>
</li>
<li>
<p>The expected value rule: </p>
<p>\[
E[Y] = \sum\limits_y yp_Y(y) = E[g(X)] = \sum\limits_x g(x)p_X(x)<br />
\]</p>
</li>
<li>
<p>Linearity of expectation: \(E[aX+b] = aE[X] + b\)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#44-variance---a-measure-of-the-spread-of-a-pmf" id="44-variance---a-measure-of-the-spread-of-a-pmf">4.4 Variance - a measure of the spread of a PMF</a></h3>
<h4><a class="header" href="#441-definition-of-variance" id="441-definition-of-variance">4.4.1 Definition of variance:</a></h4>
<p>\[
var(X) = E[(X - \mu)^2] = \sum\limits_x (x - \mu)^2 p_X(x)
\]</p>
<p>standard deviation: \(\sigma_X = \sqrt{var(X)}\)</p>
<h4><a class="header" href="#442-properties-of-the-variance" id="442-properties-of-the-variance">4.4.2 Properties of the variance</a></h4>
<ul>
<li>
<p>Notation: \(\mu = E[X] \)</p>
</li>
<li>
<p>\(var(aX + b) = a^2var(X)\)</p>
</li>
<li>
<p>A useful formula:</p>
<p>\[
var(X) = E(X^2) - (E[X])^2<br />
\]</p>
</li>
</ul>
<p><strong>Summary of Expectation and Variance of Discrete Random Variables</strong></p>
<table><thead><tr><th>Random Variables</th><th align="center">Formula</th><th align="right">E(X)</th><th align="right">var(X)</th></tr></thead><tbody>
<tr><td>Bernoulli (p)</td><td align="center">\(p_X(x) = \begin{cases} 1, p(x) = p \\ 0, p(x) = 1 - p \end{cases} \)</td><td align="right">\(p\)</td><td align="right">\(p(1-p)\)</td></tr>
<tr><td>Uniform (a,b)</td><td align="center">\(p_X(x) = \frac{1}{b-a}, a ≤ x ≤ b\)</td><td align="right">\(\frac{a+b}{2}\)</td><td align="right">\(\frac{1}{12}(b-a)(b-a-2)\)</td></tr>
<tr><td>Binomial \(p \in [0,1]\)</td><td align="center">\(p_X(k) = \left(\begin{array}{c} n \\ k \end{array}  \right)p^k(1-p)^{n-k}, k = 0, 1 ..., n\)</td><td align="right">\( np \)</td><td align="right">\(np(1-p)\)</td></tr>
<tr><td>Geometric  \(0 &lt; p ≤ 1\)</td><td align="center">\(p_X(k) = (1-p)^{k-1}p, k = 1,2,3.... \)</td><td align="right">\(\frac{1}{p}\)</td><td align="right">\(\)</td></tr>
</tbody></table>
<h3><a class="header" href="#45-conditional-pmf-and-expectation-given-an-event" id="45-conditional-pmf-and-expectation-given-an-event">4.5 Conditional PMF and expectation, given an event</a></h3>
<h4><a class="header" href="#451-conditional-pmfs" id="451-conditional-pmfs">4.5.1 Conditional PMFs</a></h4>
<p>\(p_{X|A}(x|A) = P(X = x|A)\), given A = {Y = y}</p>
<p>\[
p_{X|Y}(x|y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}<br />
\]</p>
<h4><a class="header" href="#452-conditional-pmfs-involing-more-than-two-random-variables" id="452-conditional-pmfs-involing-more-than-two-random-variables">4.5.2 Conditional PMFs involing more than two random variables</a></h4>
<ul>
<li>
<p>\(p_{X|Y,Z}(x|y,z) = P(X = x|Y = y, Z = z) = \frac{P(X=x,Y=y,Z=z)}{P(Y=y, Z=z)} = \frac{P_{X,Y,Z}(x,y,z)}{P_{Y,Z}(y,z)} \)</p>
</li>
<li>
<p>Multiplication rules: \(p_{X,Y,Z}(x,y,z) = p_X(x)p_{Y|X}(y|x)p_{Z|X,Y}(z|x,y) \)</p>
</li>
<li>
<p>Total probability and expectation theorems</p>
<p>\(p_X(x) = P(A_1)p_{X|A_1}(x) + ... + P(A_n)p_{X|A_n}(x) \implies p_X(x) = \sum\limits_y p_Y(y)p_{X|Y}(x|y)\)</p>
<p>\(E[X] = P(A_1)E[X|A_1] + ... + P(A_n)E[X|A_n] \implies E[X] = \sum\limits_y p_Y(y) E[X|Y = y]\)</p>
</li>
</ul>
<h3><a class="header" href="#46-multiple-random-variables-and-joint-pmfs" id="46-multiple-random-variables-and-joint-pmfs">4.6 Multiple random variables and joint PMFs</a></h3>
<h4><a class="header" href="#461-joint-pmf" id="461-joint-pmf">4.6.1 Joint PMF</a></h4>
<p>\[
p_{X,Y}(x,y) = P(X = x, Y =y)
\]</p>
<ul>
<li>
<p>\(\sum\limits_x \sum\limits_y p_{X,Y}(x,y) = 1\)</p>
</li>
<li>
<p>Marginal PMFs: \(p_X(x) = \sum\limits_y p_{X,Y}(x,y)\)</p>
<p>\(p_Y(y) = \sum\limits_x p_{X,Y}(x,y)\)</p>
</li>
</ul>
<h4><a class="header" href="#462-functions-of-multiple-random-variables" id="462-functions-of-multiple-random-variables">4.6.2 Functions of multiple random variables</a></h4>
<p>\(Z = g(X,Y)\)</p>
<ul>
<li>
<p>PMF: \(p_Z(z) = P(Z=z) =P(g(X,Y) = z) \)</p>
</li>
<li>
<p>Expected value rules: \(E[g(X,Y)] = \sum\limits_x \sum\limits_y g(x,y) p_{X,Y}(x,y)\)</p>
</li>
<li>
<p>Linearity of expectations</p>
<ul>
<li>
<p>\(E[aX + b] = aE[X] + b\)</p>
</li>
<li>
<p>\(E[X + Y] = E[X] + E[Y]\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#463-independence-of-multiple-random-variables" id="463-independence-of-multiple-random-variables">4.6.3 Independence of multiple random variables</a></h4>
<ul>
<li>
<p>\(P(X = x and Y = y) = P(X = x) \times P(Y = y), for all x, y \)</p>
</li>
<li>
<p>\(P_{X|Y}(x|y) = P_X(x)\) and \(P_{Y|X}(y|x) = P_Y(y)\)</p>
</li>
<li>
<p><strong>Independence and expectations</strong></p>
<ul>
<li>
<p>In general, \(E[g(X,Y)] \neq g(E[X], E[Y])\)</p>
</li>
<li>
<p>If X, Y are independent: \(E[XY] = E[X]E[Y]\)</p>
<p>g(X) and h(Y) are also independent: \(E[g(X)h(Y)] = E[g(X)]E[h(Y)]\)</p>
</li>
</ul>
</li>
<li>
<p><strong>Independence and variances</strong></p>
<ul>
<li>
<p>Always true: \(var(aX) = a^2var(X)\) and \(var(X+a) = var(X)\)</p>
</li>
<li>
<p>In general: \(var(X+Y) \neq var(X) + var(Y)\)</p>
</li>
<li>
<p>If X, Y are independent, \(var(X,Y) = var(X) + var(Y)\)</p>
</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#5-continuous-random-variables" id="5-continuous-random-variables">5 Continuous random variables</a></h2>
<h3><a class="header" href="#51-probability-density-function-pdfs" id="51-probability-density-function-pdfs">5.1 Probability density function (PDFs)</a></h3>
<h4><a class="header" href="#511-definition" id="511-definition">5.1.1 Definition</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/120918805-91f48200-c6b6-11eb-9319-23713295480c.png" alt="image" /></p>
<p><em>PDFs are not probabilities. Their units are probability per unit length.</em></p>
<p><strong>Contiunous random variables</strong>: a random variable is continuous <strong>if it can be described by a PDF</strong>.</p>
<ul>
<li>
<p>\(P(X = a) = 0\)</p>
</li>
<li>
<p>\(f_X(x) \geq 0\)</p>
</li>
<li>
<p>\(\int_{-\infty}^{+\infty}f(x)dx = 1\)</p>
</li>
</ul>
<p><strong>Expectation/Mean</strong></p>
<p>Expection/mean of a continuous random variable: <em>average in large number of independent repetitions of the experiment</em></p>
<p>\[
E[X] = \int_{-\infty}^{+\infty}xf_X(x)dx
\]</p>
<p><strong>Properties of expectations</strong></p>
<ul>
<li>
<p>if X ≥ 0, then \(E[X] ≥ 0\)</p>
</li>
<li>
<p>if a ≤ X ≤ b, then \(a ≤ E[X] ≤ b\)</p>
</li>
<li>
<p>Expected value rule: \(E[g(X)] = \sum\limits_{x} g(x) f_X(x) dx \)</p>
</li>
<li>
<p>Linearity: \(E[aX + b] = aE(X) + b\)</p>
</li>
</ul>
<p><strong>Variance</strong></p>
<p>According to the definition of variance: \(var(X) = E[(X - \mu)^2] \)</p>
<p>\[
var(X) = \int_{-\infty}^{+\infty} (x - \mu)^2 f_X(x) dx
\] </p>
<ul>
<li>
<p>Standard deviation = \(\sigma_X = \sqrt{var(X)} \)</p>
</li>
<li>
<p>\(var(aX + b) = a^2 var(X)\)</p>
</li>
<li>
<p>\(var(X) = E[X^2] - (E[X])^2\)</p>
</li>
</ul>
<p><strong>Summary of Expectation and Variance of continuous random variables</strong></p>
<table><thead><tr><th>Random Variables</th><th align="center">Formula</th><th align="right">E(X)</th><th align="right">var(X)</th></tr></thead><tbody>
<tr><td>Uniform</td><td align="center">\(f(x) = \frac{1}{b-a}, a ≤ x ≤ b\)</td><td align="right">\(\frac{a+b}{2}\)</td><td align="right">\(\frac{(b-a)^2}{12}\)</td></tr>
<tr><td>Exponential \( \lambda &gt; 0 \)</td><td align="center">\(f(x) = \begin{cases} \lambda e^{-\lambda x}, x ≥ 0 \\ 0, x &lt; 0 \end{cases}\)</td><td align="right">\(\frac{1}{\lambda}\)</td><td align="right">\(\frac{1}{\lambda^2}\)</td></tr>
</tbody></table>
<h4><a class="header" href="#512-cumulative-distribution-functions-cdf" id="512-cumulative-distribution-functions-cdf">5.1.2 Cumulative distribution functions (CDF)</a></h4>
<p>CDF defination: \(F_X(x) = P(X ≤ x )\)</p>
<ul>
<li>
<p>Non-decreasing</p>
</li>
<li>
<p>\(F_X(x)\) tends to 1, as \(x \to \infty\)</p>
</li>
<li>
<p>\(F_X(x)\) tends to 0, as \(x \to - \infty\)</p>
</li>
</ul>
<h4><a class="header" href="#513-normalgaussian-random-variables" id="513-normalgaussian-random-variables">5.1.3 Normal(Gaussian) random variables</a></h4>
<ol>
<li>
<p>Standard normal(Gaussian) random variables</p>
<p>Stardard  normal \(N(0,1): f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \)</p>
<ul>
<li>
<p>\(E[X] = 0\)</p>
</li>
<li>
<p>\(var(X) = 1\)</p>
</li>
</ul>
</li>
<li>
<p>General normal(Gaussian) random variables</p>
<p>General  normal \(N(\mu,\sigma^2): f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}, \sigma &gt; 0 \)</p>
<ul>
<li>
<p>\(E[X] = \mu \)</p>
</li>
<li>
<p>\( var(X) = \sigma^2 \)</p>
</li>
</ul>
<pre><code> \\( \sigma^2 \to small\\), the shape of normal distribution becomes more narrow.
</code></pre>
</li>
<li>
<p>Linear functions of a normal random variable</p>
<ul>
<li>
<p>Let \(Y = aX + b, X \sim N(\mu, \sigma^2)\)</p>
<p>\(E[Y] = a\mu + b\)</p>
<p>\(Var(Y) = a^2 \sigma^2 \)</p>
</li>
<li>
<p>Fact: \(Y \sim N(a\mu + b, a^2 \sigma^2)\)</p>
</li>
<li>
<p>Special case: a = 0. There is Y = b, \(N(b, 0)\)</p>
</li>
</ul>
</li>
</ol>
<h4><a class="header" href="#514-calculation-of-normal-probabilities" id="514-calculation-of-normal-probabilities">5.1.4 Calculation of normal probabilities</a></h4>
<ol>
<li>
<p><strong>Standard normal tables</strong> </p>
<p>\(\Phi(y) = F_Y(y) = P(Y \leq y)\) which can be find in the table, where y ≥ 0.</p>
</li>
<li>
<p>Standardizing a random variable</p>
<p>\(X \sim N(\mu, \sigma^2), \sigma^2 &gt; 0 \)</p>
<p>\(Y = \frac{X - \mu}{\sigma}\)</p>
</li>
</ol>
<h3><a class="header" href="#52-conditioning-on-an-event-multiple-continuous-rvs" id="52-conditioning-on-an-event-multiple-continuous-rvs">5.2 Conditioning on an event: multiple continuous r.v.'s</a></h3>
<p>\[
P( X \in B|A) =  \int_B f_{X|A}(x)dx
\]</p>
<h4><a class="header" href="#521-conditional-pdf-of-x-given-that-x-in-a-" id="521-conditional-pdf-of-x-given-that-x-in-a-">5.2.1 Conditional PDf of X, given that \(X \in A \)</a></h4>
<p>\[
f_{X|X \in A}(x) = \begin{cases} 0, if x \notin A \\ \frac{f_X(x)}{P(A)}, if x \in A \end{cases}
\]</p>
<h4><a class="header" href="#522-conditional-expectation-of-x-given-an-event" id="522-conditional-expectation-of-x-given-an-event">5.2.2 Conditional expectation of X, given an event</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121066009-b5f4b800-c7c9-11eb-9e82-cca4ded3a17f.png" alt="image" /></p>
<h4><a class="header" href="#523-memorylessness-of-the-exponential-pdf" id="523-memorylessness-of-the-exponential-pdf">5.2.3 Memorylessness of the exponential PDF</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121071392-2a325a00-c7d0-11eb-8e77-c14ded14fe0c.png" alt="image" /></p>
<h4><a class="header" href="#524-total-probability-and-expectation-theorems" id="524-total-probability-and-expectation-theorems">5.2.4 Total probability and expectation theorems</a></h4>
<ul>
<li>Probability theorem: </li>
</ul>
<p>\[
P(B) = P(A_1)P(B|A_1) + \dotsb + P(A_n)P(B|A_n)
\]</p>
<ul>
<li>For the discrete random variable: </li>
</ul>
<p>\[
p_X(x) = P(A_1)p_{X|A_1}(x) + \dotsb + P(A_n)p_{X|A_n}(x)
\]</p>
<ul>
<li>For CDF: </li>
</ul>
<p>\[
F_X(x) = P(X \leq x) = P(A_1)P(X \leq x | A_1) + \dotsb + P(A_n)P(X \leq x | A_n) 
\\= P(A_1)F_{X|A_1}(x) + \dotsb + P(A_n)F_{X|A_n}(x)
\]</p>
<ul>
<li>For PDF, the derivative of CDF:</li>
</ul>
<p>\[
f_X(x) = P(X \leq x) = P(A_1)f_{X|A_1}(x) + \dotsb + P(A_n)f_{X|A_n}(x)
\]</p>
<ul>
<li>Integral above equation, we will obtain the expectation equation:</li>
</ul>
<p>\[
\int xf_X(x)dx = P(A_1) \int xf_{X|A_1}(x)dx + \dotsb + P(A_n) \int xf_{X|A_n}(x)dx
\]</p>
<p>\[
E[X] = P(A_1)E[X|A_1] + \dotsb + P(A_n)E[X|A_n]<br />
\]</p>
<h3><a class="header" href="#53-mixed-random-varibles" id="53-mixed-random-varibles">5.3 Mixed random varibles</a></h3>
<h4><a class="header" href="#531-mixed-distirbutions" id="531-mixed-distirbutions">5.3.1 Mixed distirbutions</a></h4>
<p>\[
X = \begin{cases} Y, \text{with probability } p \text{ (Y discrete)}\\ Z, \text{with probability } 1-p \text{ (Z continuous)} \end{cases}<br />
\]</p>
<ul>
<li>
<p>do not have PDF or PMF but can be defined with CDF and expectation</p>
<p>\[
F_X(x) = p P(Y \leq x) + (1-p) P(Z \leq x)
\\
=pF_Y(x) + (1-p)F_Z(x)
\\
= E[X] = p E[Y] + (1-p) E[Z]
\]</p>
<p><img src="https://user-images.githubusercontent.com/41487483/121077061-6fa65580-c7d7-11eb-84f5-a7c1d5c36df3.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#532-joint-pdfs" id="532-joint-pdfs">5.3.2 Joint PDFs</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121728365-75f54400-caed-11eb-9935-66e24af94023.png" alt="image" /></p>
<ul>
<li>
<p>Joint PDFs are denoted as \(f_{X,Y}(x,y)\): probaility per unit area</p>
<p>When X = Y, equal to a line, meaning X and Y are not joint PDFs.</p>
</li>
</ul>
<h4><a class="header" href="#533-from-the-joint-to-the-marginal" id="533-from-the-joint-to-the-marginal">5.3.3 From the joint to the marginal</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/121731040-b73b2300-caf0-11eb-8a17-90bb4190224e.png" alt="image" /></p>
<h4><a class="header" href="#534-joint-cdf" id="534-joint-cdf">5.3.4 Joint CDF</a></h4>
<p>\[
F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \int\limits_{-\infty}^{y} \int\limits_{-\infty}^{x} f_{x,y}(s,t)dsdt 
\]</p>
<h3><a class="header" href="#54-conditioning-on-a-random-variable-and-bayers-rule" id="54-conditioning-on-a-random-variable-and-bayers-rule">5.4 Conditioning on a random variable and Bayers rule</a></h3>
<h4><a class="header" href="#541-conditional-pdfs-given-another-rv" id="541-conditional-pdfs-given-another-rv">5.4.1 Conditional PDFs, given another r.v.</a></h4>
<ul>
<li>
<p>\(f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}\), if \(f_y(y) &gt; 0\)</p>
<ul>
<li>
<p>\(f_{X|Y}(x|y) \geq 0\)</p>
</li>
<li>
<p>Think of value of Y as fixed at some y shape of \(f_{X|Y}(\cdot|y)\): slice of the joint</p>
</li>
<li>
<p>multiplication rule: </p>
<p>\[
f_{X|Y}(x,y) = f_Y(y) \cdot f_{X|Y}(x|y)<br />
\]</p>
</li>
</ul>
</li>
<li>
<p>\(P(X \in A | Y = y) = \int_A f_{X|Y}(x/y)dx\)</p>
</li>
</ul>
<h4><a class="header" href="#542-total-probability-and-expectation-theorems" id="542-total-probability-and-expectation-theorems">5.4.2 Total probability and expectation theorems</a></h4>
<ul>
<li>
<p>Anolog to the PMFs of discrete randome varable \(p_X(x) = \sum\limits_y p_Y(y)p_{X|Y}(x|y)\)</p>
<p>For continuous r.v., there is </p>
<p>\[
f_X(x) = \sum_{-\infty}^{\infty} f_Y(y)f_{X|Y}(x|y)dy<br />
\]</p>
</li>
<li>
<p>Anolog to the Expectation of discrete randome varable \(E[X|Y=y] = \sum\limits_x x p_{X|Y}(x|y)\)</p>
<p>For continuous r.v., there is </p>
<p>\[
E[X|Y=y] = \int_{-\infty}^{\infty} xf_{X|Y}(x|y)dx<br />
\]</p>
</li>
<li>
<p>Anolog to the discrete randome varable \(E[X] = \sum\limits_y p_Y(y) E[X|Y=y]\)</p>
<p>For continuous r.v., there is </p>
<p>\[<br />
E[X] = \int_{-\infty}^{\infty} f_Y(y)E[X|Y=y]dy<br />
\\ = \int_{-\infty}^{\infty} xf_X(x)dx<br />
\]</p>
</li>
<li>
<p>Expected value rule</p>
<p>\[
E[g(X)|Y=y] = \int_{-\infty}^{\infty} g(x)f_{X|Y}(x|y)dx<br />
\]</p>
</li>
</ul>
<h4><a class="header" href="#543-independence" id="543-independence">5.4.3 Independence</a></h4>
<p>\[
f_{X,Y}(x,y) = f_X(x)f_Y(y), for all x and y<br />
\]</p>
<ul>
<li>
<p>\(f_{X,Y}(x,y) = f_X(x)\), for all y with \(f_Y(y) &gt; 0\) and all x</p>
</li>
<li>
<p>If X, Y are <strong>independent</strong>:</p>
<p>\[
E[XY] = E[X]E[Y] \\
var(X + Y) = var(X) + var(Y)
\]</p>
<p>g(X) and h(Y) are also independent: \(E[g(X)h(Y)] = E[g(X)] \cdot E[h(Y)]\)</p>
</li>
</ul>
<h4><a class="header" href="#544-the-bayes-rule-----a-theme-with-variations" id="544-the-bayes-rule-----a-theme-with-variations">5.4.4 The Bayes rule --- a theme with variations</a></h4>
<ul>
<li>
<p>For discrete r.v., </p>
<ul>
<li>
<p>\(p_{X|Y}(x|y) = \frac{p_X(x) p_{Y|X}(y|x)}{p_Y(y)}\)</p>
</li>
<li>
<p>\(p_Y(y) = \sum\limits_{x'} p_X(x')p_{Y|X}(y|x')\)</p>
</li>
</ul>
</li>
<li>
<p>For continuous r.v., </p>
<ul>
<li>
<p>\(f_{X|Y}(x|y) = \frac{f_X(x) f_{Y|X}(y|x)}{_Y(y)}\)</p>
</li>
<li>
<p>\(p_Y(y) = \int\limits f_X(x')f_{Y|X}(y|x')\)</p>
</li>
</ul>
</li>
<li>
<p>One discrete and one continuous r.v.</p>
<p><img src="https://user-images.githubusercontent.com/41487483/121778872-456ae400-cb99-11eb-8fc6-d1cb9ea4f3f2.png" alt="image" /></p>
</li>
</ul>
<h2><a class="header" href="#unit-6-further-topics-on-random-variables" id="unit-6-further-topics-on-random-variables">Unit 6 Further topics on random variables</a></h2>
<h3><a class="header" href="#61-derived-distributions" id="61-derived-distributions">6.1 Derived distributions</a></h3>
<h4><a class="header" href="#611-a-linear-function-y---ax--b" id="611-a-linear-function-y---ax--b">6.1.1 A linear function \(Y =  aX + b\)</a></h4>
<ul>
<li>
<p>Discrete r.v. </p>
<p>\(
p_Y(y) = p_X(\frac{y-b}{a})
\)</p>
</li>
<li>
<p>Continuous r.v.</p>
<p>\(
f_Y(y) = \frac{1}{|a|}f_X(\frac{y-b}{a})
\)</p>
<ul>
<li>
<p>A linear function of normal r.v. is normal</p>
<p>If \(X \sim N(\mu, \sigma^2)\), then \(aX + b \sim N(a\mu + b, a^2\sigma^2)\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#612-a-general-function-gx-of-a-continuous-rv" id="612-a-general-function-gx-of-a-continuous-rv">6.1.2 A general function \(g(X)\) of a continuous r.v.</a></h4>
<p><strong>Two-step procedure:</strong></p>
<ul>
<li>
<p>Find the CDF of Y: \(F_Y(y) = P(Y \leq y) = P(g(x) \leq y)\) and the valid range of y</p>
</li>
<li>
<p>Differentiate: \(f_Y(y) = \frac{dF_Y(y)}{dy}\)</p>
</li>
</ul>
<ol>
<li>
<p>A general formula for the PDF of \(Y = g(X)\) when <em>g</em> is monotomic</p>
<p>\[
f_Y(y) = f_X(h(y))\left|\frac{dh(y)}{dy}\right|<br />
\]</p>
<p>\(x = h(y)\) is the inverse function of \(y = g(x)\)</p>
</li>
<li>
<p>A nonmonotonic example \(Y = X^2\)</p>
<ul>
<li>
<p>the discrete case: \(p_Y(y) = p_X(\sqrt{y}) + p_X(-\sqrt{y})\)</p>
</li>
<li>
<p>the continuous case: \(f_Y(y) = f_X(\sqrt{y})\frac{1}{2\sqrt{y}} + p_X(-\sqrt{y})\frac{1}{2\sqrt{y}}\)</p>
</li>
</ul>
</li>
<li>
<p>A function of multiple r.v.'s: \(Z = g(X,Y)\)</p>
<p><img src="https://user-images.githubusercontent.com/41487483/122451427-69b72e00-cfa8-11eb-9aa5-b1c0855886d0.png" alt="image" /></p>
</li>
</ol>
<h3><a class="header" href="#62-sums-of-independent-vadom-variables" id="62-sums-of-independent-vadom-variables">6.2 Sums of independent vadom variables</a></h3>
<h4><a class="header" href="#621-the-distribution-of-x--y-the-discrete-case" id="621-the-distribution-of-x--y-the-discrete-case">6.2.1 The distribution of \(X + Y\): the discrete case</a></h4>
<p>Z = X + Y; X,Y independent, discrete known PMFs</p>
<p>\[
p_Z(z) = \sum\limits_x p_X(x)p_Y(z-x)<br />
\]</p>
<p><strong>Dsicrete convoltion mechanics</strong></p>
<ol>
<li>
<p>Flip the PMF of Y and put it underneath the PMF of X</p>
</li>
<li>
<p>Shift the flipped PMF by z </p>
</li>
<li>
<p>Cross-multiply and add</p>
</li>
</ol>
<h4><a class="header" href="#622-the-distribution-of-x--y-the-continous-case" id="622-the-distribution-of-x--y-the-continous-case">6.2.2 The distribution of \(X + Y\): the continous case</a></h4>
<p>Z = X + Y; X,Y independent, continuous known PDFs</p>
<p>\[
f_Z(z) = \int\limits_x f_X(x)f_Y(z-x)dx<br />
\]</p>
<ul>
<li>
<p>conditional on \(X = x\): </p>
<p>\[
f_{Z|x}(z|x) = f_Y(z-x)<br />
\]</p>
<p>which can then be used to calculate Joint PDF of Z and X and marginal PDF of Z.</p>
</li>
<li>
<p>Same mechanics as in discrete case</p>
</li>
</ul>
<h4><a class="header" href="#623-the-sum-of-independent-normal-rvs" id="623-the-sum-of-independent-normal-rvs">6.2.3 The sum of independent normal r.v.'s</a></h4>
<ul>
<li>
<p>\(X \sim N(\mu_x, \sigma_x^2), Y \sim N(\mu_y, \sigma_y^2\) <strong>Independent</strong></p>
<p>\(Z = X + Y: \sim N(N(\mu_x + \mu_y,  \sigma_x^2 + \sigma_y^2))\)</p>
<p><strong>The sum of finitely many independent normals is normal</strong></p>
</li>
</ul>
<h3><a class="header" href="#63-covariance-协方差" id="63-covariance-协方差">6.3 Covariance (协方差)</a></h3>
<h4><a class="header" href="#631-definition" id="631-definition">6.3.1 Definition</a></h4>
<p>\[
cov(X,Y) = E[(X - E[X]) \cdot (Y - E(Y))]<br />
\]</p>
<ul>
<li>If \(X,Y\) <strong>independent: \(cov(X,Y) = 0 \)</strong> </li>
</ul>
<p><em>convers is not true!</em></p>
<h4><a class="header" href="#632-covariance-properties" id="632-covariance-properties">6.3.2 Covariance properties</a></h4>
<ol>
<li>
<p>\(cov(X,X) = var(X) = E[X^2] - (E[X])^2\)</p>
</li>
<li>
<p>\(cov(aX+b,Y) = a \cdot cov(X,Y)\)</p>
</li>
<li>
<p>\(cov(X,Y+Z) = cov(X,Y) + cov(X,Z)\)</p>
</li>
</ol>
<p><strong>Practical covariance formula:</strong> </p>
<p>\[
cov(X,Y) = E[XY] - E[X]E[Y]
\]</p>
<h4><a class="header" href="#633-the-variance-of-a-sum-of-random-variables" id="633-the-variance-of-a-sum-of-random-variables">6.3.3 The variance of a sum of random variables</a></h4>
<ul>
<li>
<p>two r.v.s</p>
<p>\[
var(X_1 + X_2) = var(X_1) + var(X_2) + 2cov(X_1,X_2)<br />
\]</p>
<p><em>X,Y indepedent, then \(var(X_1 + X_2) = var(X_1) + var(X_2)\)</em></p>
</li>
<li>
<p>multiple r.v.s</p>
<p>\[
var(X_1 + \dots + X_n) = \sum\limits_{i=1}^nvar(X_i) + \sum\limits_{(i,j):i \neq j}^n cov(X_i,X_j)<br />
\]</p>
<p><em>\(\sum\limits_{(i,j):i \neq j}^n \) contains \((n^2 - n)\) terms</em></p>
</li>
</ul>
<h3><a class="header" href="#64-the-correlation-coefficient" id="64-the-correlation-coefficient">6.4 The correlation coefficient</a></h3>
<p>\[
\rho(X,Y) = E\left[\frac{(X - E[X])}{\sigma_X} \cdot \frac{(Y - E[Y])}{\sigma_Y}\right] = \frac{cov(X,Y)}{\sigma_X \sigma_Y}
\]</p>
<h4><a class="header" href="#641-interpretation-of-correlation-coeffecient" id="641-interpretation-of-correlation-coeffecient">6.4.1 Interpretation of correlation coeffecient</a></h4>
<ul>
<li>
<p>Dimensionless version of covariance </p>
</li>
<li>
<p>Measure of the defree of &quot;association&quot; between X and Y</p>
</li>
<li>
<p>Association does not imply causation or influence</p>
</li>
<li>
<p>Correlation often refleces underlying, common, hidden factor</p>
</li>
</ul>
<h4><a class="header" href="#642-key-properties-of-the-correlation-coeffecient" id="642-key-properties-of-the-correlation-coeffecient">6.4.2 Key properties of the correlation coeffecient</a></h4>
<ul>
<li>
<p>\(-1 \leq \rho \leq 1\)</p>
</li>
<li>
<p><strong>Independent</strong> \(\implies \rho = 0\) &quot;uncorrelated&quot; (converse is not true)</p>
</li>
<li>
<p>\(|\rho| = 1 \Leftrightarrow\) linearly related</p>
</li>
<li>
<p>\(cov(aX+b, Y) = a \cdot cov(X,Y) \implies \rho(aX+b,Y) = sigma(a)\rho(X,Y)\)</p>
</li>
</ul>
<h3><a class="header" href="#65-conditional-expectation-and-variance-as-a-random-variable" id="65-conditional-expectation-and-variance-as-a-random-variable">6.5 Conditional expectation and variance as a random variable</a></h3>
<h4><a class="header" href="#651-conditional-expecation" id="651-conditional-expecation">6.5.1 Conditional expecation</a></h4>
<ul>
<li><strong>Definition</strong>: \(g(Y)\) is the <em>random variable</em> that takes the value \(E[X|Y=y]\), if \(Y\) happens to take the value \(y\).</li>
</ul>
<p>\[
E[X|Y] = g(Y)<br />
\]</p>
<ul>
<li><strong>Law of iterated expectations</strong></li>
</ul>
<p>\[
E[E[X|Y]] = E[g(Y)] = E[X]<br />
\]</p>
<h4><a class="header" href="#652-conditional-variance" id="652-conditional-variance">6.5.2 Conditional variance</a></h4>
<ul>
<li>
<p>Variance fundamentals</p>
<p>\[
var(X) = E[(X - E[X])^2] \\
var(X|Y=y) = E[(X - E[X|Y=y])^2|Y=y]
\]</p>
</li>
</ul>
<p><strong>var(X|Y) is the r.v. that takes the value var(X|Y=y), when Y=y</strong></p>
<ul>
<li>
<p><strong>Law of total variance</strong></p>
<p>\[
var(X) = E[var(X|Y)] + var(E[X|Y])<br />
\]</p>
<p><em>var(X) = (average variability within sections) + (variability between sections)</em></p>
</li>
</ul>
<h3><a class="header" href="#66-sum-a-random-number-of-indepedent-rvs" id="66-sum-a-random-number-of-indepedent-rvs">6.6 Sum a random number of indepedent r.v.'s</a></h3>
<p><em>Example of shopping</em></p>
<ul>
<li>
<p>N: number of stores visited (N is a nonnegative integer r.v.)</p>
</li>
<li>
<p>\(X_i\): money spent in store i </p>
<ul>
<li>
<p>\(X_i\) independent, identically distributed</p>
</li>
<li>
<p>independent of N</p>
</li>
</ul>
</li>
<li>
<p>Let \(Y = X_1 + \dots + X_N\)</p>
</li>
</ul>
<h4><a class="header" href="#661-expecatation-of-the-sum" id="661-expecatation-of-the-sum">6.6.1 Expecatation of the sum</a></h4>
<p>Based on the Law of iterated expectations:</p>
<p>\[
E[Y] = E[E[Y|N]] = E[N \cdot E[X]] = \cdot E[X]E[N]<br />
\]</p>
<h4><a class="header" href="#662-variance-of-the-sum" id="662-variance-of-the-sum">6.6.2 Variance of the sum</a></h4>
<p>Based on the Law of total variance: \(var(Y) = E[var(Y|N)] + var(E[Y|N])\):</p>
<p>\[
var(Y) = E[N]var(X) + (E[X])^2var(N) 
\]</p>
<h2><a class="header" href="#unit-7-bayesian-inferences" id="unit-7-bayesian-inferences">Unit 7 Bayesian inferences</a></h2>
<h3><a class="header" href="#71-introduction-to-bayesian-inference" id="71-introduction-to-bayesian-inference">7.1 Introduction to Bayesian inference</a></h3>
<h4><a class="header" href="#711-basic-concepts" id="711-basic-concepts">7.1.1 Basic concepts</a></h4>
<ol>
<li>
<p>Model building versus inferring unobserved variables</p>
<p>\[X = aS + W\]</p>
<p>S: signal; W: noise; a: medium (image a black box where S goes through and output X with W as noise)</p>
<ul>
<li>
<p>Model building: known signal S, observe X -&gt; infer a</p>
</li>
<li>
<p>Variable estimation: known a, observe X -&gt; infer S</p>
</li>
</ul>
</li>
<li>
<p>Hypothesis testing vs. estimation</p>
<ul>
<li>
<p>Hypothesis testing</p>
<ul>
<li>
<p>unknown takes one of few possible values</p>
</li>
<li>
<p>aim at small probability of incorrect decision</p>
</li>
</ul>
</li>
<li>
<p>Estimation</p>
<ul>
<li>
<p>numerical unknown(s)</p>
</li>
<li>
<p>aim at an estimate that is &quot;close&quot; to the true but unknown value</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4><a class="header" href="#712-the-bayescian-inference-framework" id="712-the-bayescian-inference-framework">7.1.2 The Bayescian inference framework</a></h4>
<ul>
<li>
<p>Unknown \(\Theta\) - treated as a random variable <strong>prior distribution: \(p_{\Theta}\) or \(f_{\Theta}\)</strong></p>
</li>
<li>
<p>Observation \(X\) - observation model \(p_{X|\Theta}\) or \(f_{X|\Theta}\)</p>
</li>
<li>
<p>Use appropriate version of the Bayes rule to find \(p_{X|\Theta}(\cdot | X = x)\) or \(f_{X|\Theta} (\cdot| X = x)\)</p>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/124636223-9617e900-de88-11eb-90fa-6c5b17f9f000.png" alt="image" /></p>
<ul>
<li>
<p>The output of Bayesian inference - <strong>posterior distribution</strong></p>
<ul>
<li>
<p><strong>Maximum a posterior probability (MAP)</strong>:</p>
<p>\(p_{\Theta|X}(\theta^*|x) = \max\limits_{\theta} p_{\Theta|X}(\theta|x)\)</p>
<p>\(f_{\Theta|X}(\theta^*|x) = \max\limits_{\theta} f_{\Theta|X}(\theta|x)\)</p>
</li>
<li>
<p><strong>Conditional expectation: \(E[\Theta|X = x]\)</strong> <strong>Least Mean Square (LMS)</strong></p>
</li>
<li>
<p>estimate: \(\hat{\theta} = g(x)\) (number)</p>
</li>
<li>
<p>estimator: \(\hat{\Theta} = g(X)\) (random variable)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#713-four-cases" id="713-four-cases">7.1.3 Four cases</a></h4>
<ol>
<li>
<p><strong>Discrete \(\Theta\), discrete X</strong></p>
<ul>
<li>values of \(\Theta\): alternative hypotheses</li>
</ul>
<p>\[
p_{\Theta|X}(\theta|x) = \frac{p_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}<br />
\]</p>
<p>\[
p_X(x) = \sum\limits_{\theta'}p_{\Theta}(\theta')p_{X|\Theta}(x|\theta')
\]</p>
<ul>
<li>conditional prob of error: <strong>Smallest under the MAP rule</strong></li>
</ul>
<pre><code> \\[
     P(\hat{\theta} \neq \Theta|X = x)
 \\] 
</code></pre>
<ul>
<li>overal probability of error: </li>
</ul>
<pre><code> \\[
     P(\hat{\Theta} \neq \Theta) = \sum\limits_{x} P(\hat{\Theta} \neq \Theta|X = x)p_X(x) = \sum\limits_{\theta}P(\hat{\Theta} \neq \Theta|\Theta = \theta)p_{\Theta}(\theta)
 \\] 
</code></pre>
</li>
<li>
<p><strong>Discrete \(\Theta\), Continuous X</strong></p>
<p>\[
p_{\Theta|X}(\theta|x) = \frac{p_{\Theta}(\theta)f_{X|\Theta}(x|\theta)}{f_X(x)}<br />
\]</p>
<p>\[
f_X(x) = \sum\limits_{\theta'}p_{\Theta}(\theta')f_{x|\Theta}(x|\theta')
\]</p>
<ul>
<li>
<p>the same equation for conditional prob. of error</p>
</li>
<li>
<p>overall probability of error</p>
<p>\[
P(\hat{\Theta} \neq \Theta) = \int\limits_{x} P(\hat{\Theta} \neq \Theta|X = x)f_X(x)dx = \sum\limits_{\theta}P(\hat{\Theta} \neq \Theta|\Theta = \theta)p_{\Theta}(\theta)
\] </p>
</li>
</ul>
</li>
<li>
<p><strong>Continuous \(\Theta\), Discrete X</strong></p>
<p>\[
f_{\Theta|X}(\theta|x) = \frac{p_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}<br />
\]</p>
<p>\[
p_X(x) = \int\limits_{\theta'}f_{\Theta}(\theta')p_{x|\Theta}(x|\theta')d\theta'
\]</p>
<ul>
<li>Inferring the unknown bias of a coin and <strong>the Beta distribution</strong></li>
</ul>
</li>
<li>
<p><strong>Continuous \(\Theta\), Continuous X</strong></p>
<p>\[
f_{\Theta|X}(\theta|x) = \frac{f_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}<br />
\]</p>
<p>\[
f_X(x) = \int\limits_{\theta'}f_{\Theta}(\theta')p_{x|\Theta}(x|\theta')d\theta'
\]</p>
<ul>
<li>
<p>Linear normal models: estimation of a noisy singal</p>
</li>
<li>
<p>Estimating the parameter of a uniform </p>
<p>\(X\): uniform \([0, \Theta]\)</p>
<p>\(\Theta\): uniform \([0, 1]\)</p>
</li>
<li>
<p>Performance evaluation of an estimator \(\hat{\Theta}\)</p>
<p>\(E[(\hat{\Theta} - \Theta)^2|X = x]\)</p>
<p>\(E[(\hat{\Theta} - \Theta)^2]\)</p>
</li>
</ul>
</li>
</ol>
<p>Useful equation: </p>
<p>\[
\int_0^1 \theta^\alpha(1-\theta)^\beta d\theta = \frac{\alpha!\beta!}{(\alpha + \beta + 1)!}<br />
\]</p>
<h3><a class="header" href="#72-linear-models-with-normal-noise" id="72-linear-models-with-normal-noise">7.2 Linear models with normal noise</a></h3>
<h4><a class="header" href="#721-recognizing-normal-pdfs" id="721-recognizing-normal-pdfs">7.2.1 Recognizing normal PDFs</a></h4>
<ul>
<li>
<p>Normal distribution: \(X \sim N(\mu, \sigma^2)\) </p>
<p>\(f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}\)</p>
</li>
<li>
<p>\(f_X(x) = c e^{-(\alpha x^2 + \beta x + \gamma)}\), \(\alpha &gt; 0\) Normal with mean \(-\beta/2\alpha\) and variance \(-1/2\alpha\)</p>
</li>
</ul>
<h4><a class="header" href="#722-estimating-a-normal-random-variable-in-the-presence-of-additive-normal-noise" id="722-estimating-a-normal-random-variable-in-the-presence-of-additive-normal-noise">7.2.2 Estimating a normal random variable in the presence of additive normal noise</a></h4>
<p>\(X = \Theta + W\), \(\Theta, W,N :(0,1), independent\)</p>
<ul>
<li>
<p>\( \hat{\theta} _{MAP} = \hat{\theta} _{LMS} = E[\Theta|X = x] = x/2\)</p>
</li>
<li>
<p>even with general means and variances:</p>
<ul>
<li>
<p>posterior is normal</p>
</li>
<li>
<p>LMS and MAP estimators conincide</p>
</li>
<li>
<p>these estimators are &quot;linear&quot; of the form \(\hat{\Theta} = aX + b\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#723-the-case-of-multiple-observations" id="723-the-case-of-multiple-observations">7.2.3 The case of multiple observations</a></h4>
<p>\(X_i = \Theta + W_1\), \(\Theta \sim N(x_0, \sigma_0^2)\), \(W_i \sim N(x_i, \sigma_i^2), \Theta, W_i\) indepedent</p>
<ul>
<li>
<p>\(\hat{\theta} _{MAP} = \hat{\theta} _{LMS} = E[\Theta|X = x] = \frac{\sum\limits _{i=0}^n\frac{x_i}{\sigma_i^2}}{\sum\limits _{i=0}^n\frac{1}{\sigma_i^2}}\)</p>
</li>
<li>
<p>Key conclusions</p>
<ul>
<li>
<p>posterior is normal</p>
</li>
<li>
<p>LMS and  MAP estimates coincide</p>
</li>
<li>
<p>these estimates are &quot;linear&quot; of the form \(\hat{\theta} = a_0 + a_1x_1 + \dots + a_nx_n\)</p>
</li>
</ul>
</li>
<li>
<p>Interpretations</p>
<ul>
<li>
<p>estimate \(\hat{\theta}\): weighted average of \(x_0\) (prior mean) and \(x_i\) (observations)</p>
</li>
<li>
<p>weights determined by variances</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#724-the-mean-square-error" id="724-the-mean-square-error">7.2.4 The mean square error</a></h4>
<ul>
<li>
<p>Performance measures</p>
<ul>
<li>
<p>\(E[(\Theta - \hat{\Theta})^2|X = x] = E[(\Theta - \hat{\theta})^2|X = x] = var(\Theta|X = x) = \frac{1}{\sum\limits _{i=0}^n \frac{1}{\sigma_i^2}}\)</p>
</li>
<li>
<p>\(E[(\Theta - \hat{\Theta})^2] = \int E[(\Theta - \hat{\Theta})^2|X = x] f_X(x) dx = \frac{1}{\sum\limits _{i=0}^n \frac{1}{\sigma_i^2}}\)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#73-least-mean-squares-lms-estimation" id="73-least-mean-squares-lms-estimation">7.3 Least mean squares (LMS) estimation</a></h3>
<h4><a class="header" href="#731-in-the-absence-of-observations" id="731-in-the-absence-of-observations">7.3.1 In the absence of observations</a></h4>
<ul>
<li>
<p>Least Mean Square formulation: minimize <strong>Mean Squared Error (MSE)</strong> \(E[(\Theta - \hat{\theta})^2]: \hat{\theta} = E[\Theta]\)</p>
</li>
<li>
<p>\(E[(\Theta - E[\Theta])^2]:var(\Theta)\)</p>
</li>
</ul>
<h4><a class="header" href="#732-lms-estimation-of-theta-based-on-x" id="732-lms-estimation-of-theta-based-on-x">7.3.2 LMS estimation of \(\Theta\) based on X</a></h4>
<ul>
<li>Minimize <strong>conditional mean square error</strong>: \(E[(\Theta - \hat{\theta})^2|X = x]: \hat{\theta} = E[\Theta|X = x]\)</li>
</ul>
<h4><a class="header" href="#733-lms-performance-evaluation" id="733-lms-performance-evaluation">7.3.3 LMS performance evaluation</a></h4>
<ul>
<li>
<p>LMS estimate: \(\hat{\theta} = E[\Theta|X=x]\)</p>
</li>
<li>
<p>Estimator: \(\hat{\Theta} = E[\Theta|X]\)</p>
</li>
<li>
<p>Expected performance, once we have a measurement - <strong>Conditional mean square error</strong></p>
<p>\(MSE = E[(\Theta - E[\Theta|X=x])^2|X=x] = var(\Theta|X=x)\)</p>
</li>
<li>
<p>Expected perfornamce of the design:</p>
<p>\(MSE = E[(\Theta - E[\Theta|X])^2] = E[var(\Theta|X)] = \int var(\Theta|X=x) \cdot f_X(x) dx\) <em>Average of conditional variance</em></p>
</li>
<li>
<p>A good example</p>
<p><img src="https://user-images.githubusercontent.com/41487483/124714295-e1baa900-df01-11eb-8618-b411d877495e.png" alt="image" /></p>
<p><img src="https://user-images.githubusercontent.com/41487483/124714412-04e55880-df02-11eb-915d-996236e5958e.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#734-properties-of-the-estimation-error-in-lms-estimation" id="734-properties-of-the-estimation-error-in-lms-estimation">7.3.4 Properties of the estimation error in LMS estimation</a></h4>
<p>Given Estimator: \(\hat{\Theta} = E[\Theta|X]\) and Error: \(\tilde{\Theta} = \hat{\Theta} - \Theta\)</p>
<ul>
<li>
<p>\(E[\tilde{\Theta|X=x}] = 0\)</p>
</li>
<li>
<p>\(cov(\tilde{\Theta},\hat{\Theta}) = 0\)</p>
</li>
<li>
<p>\(var(\Theta) = var(\hat{\Theta}) + var({\tilde{\Theta}})\)</p>
</li>
</ul>
<h3><a class="header" href="#74-linear-least-mean-squares-llms-estimation" id="74-linear-least-mean-squares-llms-estimation">7.4 Linear least mean squares (LLMS) estimation</a></h3>
<p><em>Motivation: Conditional expectation \(E[\Theta|X]\) maybe hard to compute/implement</em></p>
<h4><a class="header" href="#741-llms-formulation" id="741-llms-formulation">7.4.1 LLMS formulation</a></h4>
<p>Consider estimators of \(\Theta\) of the form \(\hat{\Theta} = aX + b\), minimize \(E[(\hat{\Theta} - \Theta)^2] \implies E[(\hat{\Theta} - aX - b)^2] \) </p>
<h4><a class="header" href="#742-llms-solution" id="742-llms-solution">7.4.2 LLMS solution</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/124715231-0cf1c800-df03-11eb-9cf0-ec04d3cf70d4.png" alt="image" /></p>
<p>Minimize \(E[(\hat{\Theta} - \Theta)^2]\), that is \(E[(\Theta - aX - b)^2]\)</p>
<p>\[
\hat{\Theta}_L  = E[\Theta] + \frac{Cov(\Theta,X)}{var(X)}(X - E[X]) = E[\Theta] + \rho \frac{\sigma _\Theta}{\sigma_X}(X - E[X])<br />
\]</p>
<p>\(\rho\) corelation coefficiency</p>
<p><strong>Remarks on the solution and on the error variance</strong></p>
<ul>
<li>
<p>Only means, variances, covariances matter (we do not need to know everything)</p>
<p>\(E[(\hat{\Theta}_L - \Theta)^2] = (1 - \rho^2)var(\Theta)\)</p>
</li>
</ul>
<h4><a class="header" href="#743-llms-with-multiple-observations" id="743-llms-with-multiple-observations">7.4.3 LLMS with multiple observations</a></h4>
<ul>
<li>
<p>Consider the form \(\hat{\Theta} = a_1X_1 + \dots + a_nX_n + b\)</p>
</li>
<li>
<p>Minimize \(E[(a_1X_1 + \dots + a_nX_n + b - \Theta)^2]\)</p>
</li>
<li>
<p>Solve linear system in \(b\) and \(a_i\)</p>
</li>
<li>
<p>if \(E[\Theta|X]\) is linear in X, then \(\hat{\Theta} _{LMS} = \hat{\Theta} _{LLMS}\)</p>
</li>
<li>
<p>suppose general distributions with same mean, variances</p>
<ul>
<li>
<p>\(\hat{\theta} _{MAP} = \hat{\theta} _{LMS} = E[\Theta|X = x] = \frac{\sum\limits _{i=0}^n\frac{x_i}{\sigma_i^2}}{\sum\limits _{i=0}^n\frac{1}{\sigma_i^2}}\)</p>
</li>
<li>
<p>\(\hat{\Theta} _{LMS} = E[\Theta|X] = \frac{\frac{x_0}{\sigma _0^2} + \sum\limits _{i=i}^n\frac{X_i}{\sigma_i^2}}{\sum\limits _{i=0}^n\frac{1}{\sigma_i^2}} = \hat{\Theta} _{LLMS}\)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#75-bayesian-inference-summary" id="75-bayesian-inference-summary">7.5 Bayesian inference summary</a></h3>
<p><img src="https://user-images.githubusercontent.com/41487483/124724545-5eeb1b80-df0c-11eb-964b-82371cd4799c.png" alt="image" /></p>
</br>
<h2><a class="header" href="#unit-8-limit-theorems-and-clasic-statistics" id="unit-8-limit-theorems-and-clasic-statistics">Unit 8 Limit theorems and clasic statistics</a></h2>
<h3><a class="header" href="#81-inequalities-comvergence-and-the-weak-law-of-large-numbers" id="81-inequalities-comvergence-and-the-weak-law-of-large-numbers">8.1 Inequalities, comvergence, and the Weak Law of Large Numbers</a></h3>
<h4><a class="header" href="#811-markov-and-chebyshev-inequality" id="811-markov-and-chebyshev-inequality">8.1.1 Markov and Chebyshev inequality</a></h4>
<ol>
<li>
<p><strong>Markov inequality</strong></p>
<p>&quot;If \(X \geq 0\) and \(E[X]\) is small, then X is unlikely to be very large&quot;</p>
<p>\[
P(X \geq a) \leq \frac{E[X]}{a} \text{, for all }  a &gt; 0 \text{ and }  X \geq 0
\]</p>
</li>
<li>
<p><strong>Chebyshev inequality</strong></p>
<p>&quot;If the variance is small, then X is unlikely to be too far from the mean&quot;</p>
<p>\[
P(|X - \mu| \geq c) \leq \frac{\sigma^2}{c^2} 
\text{, for all }  c &gt; 0 \text{ and }  X \text{ is a random variable with mean } \mu \text{ and variance } \sigma^2
\]</p>
</li>
</ol>
<h4><a class="header" href="#812-the-weak-law-of-large-numbers-wlln" id="812-the-weak-law-of-large-numbers-wlln">8.1.2 The Weak Law of Large Numbers (WLLN)</a></h4>
<p>\(X_1, X_2, \dots\) i.i.d.: infinite mean \(\mu\) and variance \(\sigma^2\)</p>
<p>\[
\text{Sample mean: } M_n = \frac{X_1 + \dots + X_n}{n}
\]</p>
<ul>
<li>
<p>\(E[M_n] = \mu\)</p>
</li>
<li>
<p>\(Var(M_n) = \frac{\sigma^2}{n}\)</p>
</li>
<li>
<p><strong>WLLN:</strong> for \(\varepsilon &gt; 0\),</p>
<p>\[
P(|M_n - \mu|) \geq \varepsilon = P \left( \left| \frac{X_1 + \dots + X_n}{n} - \mu\right| \geq \varepsilon \right) \to 0 \text{, as n} \to \infty
\]</p>
</li>
<li>
<p>Interpreting the WLLN </p>
<ul>
<li>
<p><strong>Sample mean</strong> \(M_n\) is unlikely to be far off from <strong>true mean</strong> \(\mu\)</p>
</li>
<li>
<p><strong>Sample mean</strong> \(M_n\) is the <strong>emperical frequency</strong> of even \(A\), with \(p = P(A)\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#813-convergence-in-probability" id="813-convergence-in-probability">8.1.3 Convergence in Probability</a></h4>
<p>Sequence of random variables \(Y_n\), not necessarily independent</p>
<p><strong>Definition</strong>: A sequence <a style="color:red;">\(Y_n\)</a> <strong>converges in probability</strong> to a certain number <a style="color:red;">a</a> if:</p>
<p>\[
\lim_\limits{n \to \infty} P(|Y_n - a| \geq \varepsilon) = 0<br />
\]</p>
<p><em>Almost all of the PMF/PDF of \(Y_n\) eventually gets concentrated (arbitrarily) close to a</em></p>
<ul>
<li>
<p>Some properties - suppose that \(X_n \to a, Y_n \to b\)</p>
<ol>
<li>
<p>if <em>g</em> is continuous, then \(g(X_n) \to g(a)\)</p>
</li>
<li>
<p>\(X_n + Y_n \to a + b\)</p>
</li>
<li>
<p>\(E[X_n]\) <b>need not converge to a</b></p>
</li>
</ol>
</li>
</ul>
</br>
<h3><a class="header" href="#82-the-central-limit-theorem-clt" id="82-the-central-limit-theorem-clt">8.2 The Central Limit Theorem (CLT)</a></h3>
<p><img src="https://user-images.githubusercontent.com/41487483/125804339-0461c83e-e397-488a-81ff-bfc51017512b.png" alt="image" /></p>
<h4><a class="header" href="#822-what-exactly-does-the-clt-say" id="822-what-exactly-does-the-clt-say">8.2.2 What exactly does the CLT say?</a></h4>
<ol>
<li>
<p><strong>Theory</strong></p>
<p>\(Z_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}\) and \(Z \sim N(0,1)\)</p>
<ul>
<li>
<p>CDF of Z<sub>n</sub> converges to normal <strong>CDF</strong></p>
</li>
<li>
<p>results for convergence of PDFs or PMFs (with more assumptions)</p>
</li>
<li>
<p>results without assuming that X<sub>i</sub> are identically distributed</p>
</li>
<li>
<p>results under &quot;weak dependence&quot;</p>
</li>
</ul>
<p><strong>In short, CLT applies to a sequence of random variables that do not need to be i.i.d.</strong> </p>
</li>
<li>
<p><strong>Practice</strong></p>
<ul>
<li>
<p>The practiec of normal approximations:</p>
<ul>
<li>
<p>treat Z<sub>n</sub> as if it were normal</p>
</li>
<li>
<p>treat S<sub>n</sub> as if normal: \(N(n\mu, n\sigma^2)\) as \(S_n = \sqrt{n}\sigma Z_n + n\mu\)</p>
</li>
</ul>
</li>
<li>
<p>Can we use the CLT when n is &quot;moderate&quot;? </p>
<ul>
<li>
<p>usually, yes</p>
</li>
<li>
<p>symmetry and unimodality help</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</br>
<h3><a class="header" href="#83-an-introduction-to-classical-statistics" id="83-an-introduction-to-classical-statistics">8.3 An introduction to classical statistics</a></h3>
<h4><a class="header" href="#831-overview" id="831-overview">8.3.1 Overview</a></h4>
<ul>
<li>
<p>Inference using the Bayes rule:</p>
<p>unknown \(\Theta\) and observation \(X\) are both random variables: Find \(p_{\Theta|X}\)</p>
</li>
<li>
<p>Classical statistics: unknown <strong>constant</strong> \(\theta\)</p>
<ul>
<li>
<p>Problem types in classical statistics</p>
<ul>
<li>
<p>Hypothesis testing: \(H_0: \theta = 1/2 \text{ vs. } H_1: \theta = 3/4\)</p>
</li>
<li>
<p>Composite hypotheses: \(H_0: \theta = 1/2 \text{ vs. } H_1: \theta \neq 1/2\)</p>
</li>
<li>
<p>Estimation: design an estimator \(\hat{\Theta}\), to keep estimation error \((\hat{\Theta} - \theta)\) small.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#832-the-sample-mean-and-some-terminology" id="832-the-sample-mean-and-some-terminology">8.3.2 The sample mean and some terminology</a></h4>
<ul>
<li>
<p>Estimating a mean</p>
<ul>
<li>
<p>\(X_1, \dots, X_n\): i.i.d, mean \(\theta\), variance \(\sigma^2\)</p>
</li>
<li>
<p><strong>Sample mean</strong> \(= \hat{\Theta}_n = M_n = \frac{X_1 + \dots + X_n}{n}\) </p>
</li>
</ul>
</li>
<li>
<p>Properties and terminology</p>
<ul>
<li>
<p>\(E[\hat{\Theta}_n] = \theta\)  <a style="color:red;">(unbiased)</a> for all \(\theta\)</p>
</li>
<li>
<p>WLLN: \(E[\hat{\Theta}_n] \to \theta\)  <a style="color:red;">(consistency)</a> for all \(\theta\)</p>
</li>
<li>
<p>Mean square error <a style="color:red;">(MSE)</a>: \(E[(\hat{\Theta}_n - \theta)^2] = var(\hat{\Theta}_n) = \frac{\sigma^2}{n}\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#833-on-the-mean-square-error-of-an-estimator" id="833-on-the-mean-square-error-of-an-estimator">8.3.3 On the mean square error of an estimator</a></h4>
<p>\[
E[(\hat{\Theta} - \theta)^2] = var(\hat{\Theta} - \theta) + (E[\hat{\Theta} - \theta])^2 = var(\hat{\Theta}) + (bias)^2<br />
\]</p>
<ul>
<li>
<p>Sample mean estimator (\(\hat{\Theta}_n = M_n\)): \(MSE = \frac{\sigma^2}{n} + 0\)</p>
</li>
<li>
<p>Zero estimator (\(\hat{\Theta} = 0\)): \(MSE = 0 + \theta^2\)</p>
</li>
<li>
<p>\(\sqrt{var(\hat{\Theta})}\) is the <a style="color:red"> standard error </a>. </p>
<p><em>Standard Error refers to sampling distribution, whereas standard deviation refers to sample distribution</em></p>
</li>
</ul>
<h4><a class="header" href="#834-confidence-intervals-cis" id="834-confidence-intervals-cis">8.3.4 Confidence intervals (CIs)</a></h4>
<p>An \(1 - \alpha\) confidence interval is an interval \([\hat{\Theta}^-, \hat{\Theta}^+]\), for all \(\theta\)</p>
<p>\[
P(\hat{\Theta}^- \leq \theta \leq \hat{\Theta}^+)<br />
\]</p>
<ul>
<li>
<p><strong>CI for the estimation of the mean</strong></p>
<ul>
<li>
<p>\(X_1, \dots, X_n\): i.i.d, mean \(\theta\), variance \(\sigma^2\)</p>
</li>
<li>
<p><strong>Sample mean</strong> \(= \hat{\Theta}_n = M_n = \frac{X_1 + \dots + X_n}{n}\) </p>
</li>
<li>
<p>95% CI: \(\Phi(1.96) = 0.975 = 1 - 0.025\)</p>
<p>\[
P \left( \frac{|\hat{\Theta}_n - \theta|}{\sigma/\sqrt{n}}\right) \leq 1.96 \approx 0.95 \text{ (CLT) } \implies P \left(\hat{\Theta}_n - \frac{1.96\sigma}{\sqrt{n}} \leq \theta \leq \hat{\Theta}_n + \frac{1.96\sigma}{\sqrt{n}}\right)
\]</p>
</li>
</ul>
</li>
<li>
<p><strong>CI for the mean when \(\sigma\) is unknown</strong></p>
<ol>
<li>
<p>use upper bound on \(\sigma\)</p>
<ul>
<li>for \(X_i\) Bernoulli: \(\sigma \leq 1/2\)</li>
</ul>
</li>
<li>
<p>use ad hoc estimate of \(\sigma\)</p>
<ul>
<li>for \(X_i\) Bernoulli: \(\sigma = \sqrt{\hat{\Theta}_n(1 - \hat{\Theta}_n)}\)</li>
</ul>
</li>
<li>
<p>use sample mean estimate of the variance</p>
<p>\(\sigma^2 = E[(X_i - \theta)^2] \implies \frac{1}{n} \sum\limits_{i = 1}^n (X_i - \hat{\Theta}_n)^2 \to \sigma^2\)</p>
</li>
</ol>
</li>
</ul>
<hr>
<ul>
<li>
<p>Two approximations involved here:</p>
<ul>
<li>
<p>CLT: approximately normal</p>
</li>
<li>
<p>using estimate of \(\sigma\)</p>
</li>
</ul>
</li>
<li>
<p>correction for second approximation <a style="color:red">(t-tables)</a> used when <em>n</em> is small.</p>
</li>
</ul>
<hr>
<h4><a class="header" href="#835-other-natural-estimators" id="835-other-natural-estimators">8.3.5 Other natural estimators</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/125932005-aaf38a3c-d0e4-41e8-9be0-7fd8c9896573.png" alt="image" /></p>
<h4><a class="header" href="#836-maximum-likelihood-ml-estimation" id="836-maximum-likelihood-ml-estimation">8.3.6 Maximum Likelihood (ML) estimation</a></h4>
<ul>
<li>
<p>Pick <a style="color:red">\(\theta\)</a> that &quot;makes data most likely&quot;</p>
<p>\[
\hat{\theta}_ {ML} = arg \max\limits_{\theta} p_X(x;\theta)<br />
\]</p>
<p><i>compare to maximum a posterior probability Bayesian posterior \(p_{\Theta|X}(\theta^*|x) = \max\limits_{\theta}p_{\Theta|X}(\theta|x)\)</i></p>
</li>
</ul>
<h2><a class="header" href="#unit-9-the-bernoulli-and-poisson-process" id="unit-9-the-bernoulli-and-poisson-process">Unit 9 The Bernoulli and Poisson process</a></h2>
<h3><a class="header" href="#91-the-bernoulli-process" id="91-the-bernoulli-process">9.1 The Bernoulli process</a></h3>
<h4><a class="header" href="#911-definition" id="911-definition">9.1.1 Definition</a></h4>
<ul>
<li>
<p>A sequence of independent Bernoulli tirals, \(X_i\)</p>
</li>
<li>
<p>At each trial, i:</p>
<p>\(P(X_i = 1) = P(\text{success at the ith trial}) = p\)</p>
<p>\(P(X_i = 0) = P(\text{failure at the ith trial}) = 1 - p\)</p>
</li>
<li>
<p>Properties</p>
<ul>
<li>
<p>\(E[X_i] = p\)</p>
</li>
<li>
<p>\(var(X_i) = p*(1-p)\)</p>
</li>
</ul>
</li>
<li>
<p>Key assumption</p>
<ul>
<li>
<p>Independence</p>
</li>
<li>
<p>Time-homogeneity</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#912-stochastic-processes" id="912-stochastic-processes">9.1.2 Stochastic processes</a></h4>
<ul>
<li>
<p>A sequence of random variables \(X_1, X_2, \dots\)</p>
</li>
<li>
<p>Sample space: \(\Omega = \text{a set of infinite sequence of 0's and 1's}\) </p>
</li>
</ul>
<h4><a class="header" href="#913-number-of-successesarrivals-s-in-n-time-slots-binomial-distribution" id="913-number-of-successesarrivals-s-in-n-time-slots-binomial-distribution">9.1.3. Number of successes/arrivals S in n time slots (Binomial distribution)</a></h4>
<ul>
<li>
<p>\(S = X_1 + X_2 + \dots + X_n\)</p>
</li>
<li>
<p>\(P(S=k) = \binom{n}{k}p^k(1-p)^{n-k}\), k = 0, 1, 2 ....</p>
</li>
<li>
<p>\(E[S] = np\)</p>
</li>
<li>
<p>\(var(S) = np(1-p)\)</p>
</li>
</ul>
<h4><a class="header" href="#914-time-until-the-first-successarrival-geometric-distribution" id="914-time-until-the-first-successarrival-geometric-distribution">9.1.4 Time until the first success/arrival (Geometric distribution)</a></h4>
<ul>
<li>
<p>\(T_i = min \{i: X_i=1 \}\)</p>
</li>
<li>
<p>\(P(T_1 = k) = (1-p)^(k-1)p\), k = 1,2,...</p>
</li>
<li>
<p>\(E[T_1] = \frac{1}{p}\)</p>
</li>
<li>
<p>\(var(T_1) = \frac{1-p}{p^2}\)</p>
</li>
</ul>
<h4><a class="header" href="#915-independence-memorylessness-and-fresh-start-properties" id="915-independence-memorylessness-and-fresh-start-properties">9.1.5 Independence, memorylessness, and fresh-start properties</a></h4>
<ul>
<li>
<p>Fresh-start after time n (slots), after time T1</p>
</li>
<li>
<p>Fresh-start after a random time N</p>
<ul>
<li>
<p>N = time of 3rd sucess</p>
</li>
<li>
<p>N = first time that 3 successes in a row have been observed</p>
</li>
</ul>
</li>
<li>
<p>The process \(X_{N+1}, X_{N+2}\), ... is </p>
<ul>
<li>
<p>A Bernoulli process</p>
</li>
<li>
<p>independent of N, \(X_1, X_2, \dots, X_N\)</p>
</li>
</ul>
<p><em>as long as N is determined &quot;casually&quot;</em></p>
</li>
</ul>
<h4><a class="header" href="#916-time-of-the-kth-successarrival" id="916-time-of-the-kth-successarrival">9.1.6 Time of the kth success/arrival</a></h4>
<ul>
<li>
<p>\(Y_k\) = time of kth arrival</p>
</li>
<li>
<p>\(T_k\) = kth inter-arrival time = \(Y_k - Y_{k-1} \text{, } k \geq 2 \)</p>
</li>
<li>
<p>\(Y_k = T_1 + \dots + T_k\)</p>
<ul>
<li>
<p>The process starts fresh after time T1</p>
</li>
<li>
<p>T2 is independent of T1: Geometric(p)</p>
</li>
<li>
<p>\(E[Y_k] = \frac{k}{p}\)</p>
</li>
<li>
<p>\(var(Y_k) = \frac{k(1-p)}{p^2}\)</p>
</li>
<li>
<p>PMF: \(p_{Y_k}(t) = \binom{t-1}{k-1}p^k(1-p)^{t-k} \text{, } t = k, k +1, ..\).</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#917-merging-of-independent-bernoulli-processes" id="917-merging-of-independent-bernoulli-processes">9.1.7 Merging of independent Bernoulli processes</a></h4>
<ul>
<li>
<p>\(X_i\): Bernoulli(p)</p>
</li>
<li>
<p>\(Y_i\): Bernoulli(q)</p>
</li>
<li>
<p>Merged process: \(Z_i: g(X_i, Y_i)\) Bernoulli(p + q - pq)</p>
</li>
</ul>
<h4><a class="header" href="#917-splitting-of-a-bernoulli-process" id="917-splitting-of-a-bernoulli-process">9.1.7 Splitting of a Bernoulli process</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/126913885-cf7863e9-8053-4a4c-90d4-c6afc0585538.png" alt="image" /></p>
<h4><a class="header" href="#918-poisson-approximation-to-binomial" id="918-poisson-approximation-to-binomial">9.1.8 Poisson approximation to binomial</a></h4>
<ul>
<li>
<p>Interesting regime: large n, small p, <a style='color:red'> moderate λ = np</a></p>
</li>
<li>
<p>Number of arrivals S in n slots: \(p_S(k) \to \frac{\lambda^k}{k!}e^{-\lambda}\) (For fixed k = 0, 1...)</p>
</li>
</ul>
</br>
<h3><a class="header" href="#92-the-poison-process" id="92-the-poison-process">9.2 The Poison process</a></h3>
<h4><a class="header" href="#921-definition" id="921-definition">9.2.1 Definition</a></h4>
<p>Poisson process is similar to Bernoulli process, but in a continuous time interval.</p>
<ul>
<li>Numbers of arrivals in disjoint time intervals are independent</li>
</ul>
<pre><code>\\(P(k, \tau)\\) = Prob. of *k* arrivals in interval of duration \\(\tau\\)
</code></pre>
<ul>
<li>
<p>Small interval probabilities - For VERY small \(\delta\):</p>
<p>\[
P(k, \delta) = \begin{cases} 1-\lambda\delta + O(\delta^2) &amp; \quad \text{if } k = 0 \\
\lambda\delta + O(\delta^2)  &amp; \quad \text{if } k=1 \\
0 + O(\delta^2) &amp; \quad \text{if } k&gt;1 \end{cases}
\]</p>
<p>\[
P(k, \delta) \approx \begin{cases} 1-\lambda\delta &amp; \quad \text{if } k = 0 \\
\lambda\delta &amp; \quad \text{if } k=1 \\
0 &amp; \quad \text{if } k&gt;1 \end{cases}
\]</p>
<p><strong><a style='color:red'>λ: &quot;Arrival rates&quot; </a></strong></p>
</li>
</ul>
<h4><a class="header" href="#922-the-poisson-pmf-for-the-number-of-arrivals" id="922-the-poisson-pmf-for-the-number-of-arrivals">9.2.2 The Poisson PMF for the number of arrivals</a></h4>
<ul>
<li>
<p>\(N_{\tau}:\text{ arrivals in }[0, \tau]\) </p>
</li>
<li>
<p>\(N_\tau \approx Binomial(n,p)\), \(n = \frac{\tau}{\delta}\), \(p = \lambda\delta + O(\delta^2)\)</p>
</li>
<li>
<p>\[
P(k, \tau) = P(N_\tau =k) = \frac{(\lambda\tau)^ke^{-\lambda\tau}}{k!}, \text{k = 0, 1, 2,...}
\]</p>
</li>
<li>
<p>\(E[N_\tau] \approx np \approx \lambda\tau\)</p>
</li>
<li>
<p>\(var(N_\tau) \approx np(1-p) \approx \lambda\tau\)</p>
</li>
</ul>
<h4><a class="header" href="#923-the-time-t_1-until-the-first-arrival" id="923-the-time-t_1-until-the-first-arrival">9.2.3 The time \(T_1\) until the first arrival</a></h4>
<p>Find the CDF: \(P(T_1 \leq t) = 1 - P(T_1 &gt; t) = 1 - P(0,t) = 1 - e^{-\lambda t}\)</p>
<p>\[
f_{T_1}(t) = \lambda e^{-\lambda t} \text{, for } t \geq 0<br />
\]</p>
<p><strong><a style='color:red'>Exponential(λ) </a></strong></p>
<h4><a class="header" href="#924-the-time-y_k-of-the-kth-arrival" id="924-the-time-y_k-of-the-kth-arrival">9.2.4 The time \(Y_k\) of the kth arrival</a></h4>
<p>Two ways to derive: </p>
<ul>
<li>
<p>Through CDF: \(P(Y_k \leq y) = \sum\limits_{n=k}^{\infty}P(n, y)\)</p>
</li>
<li>
<p>More intuitive argument</p>
<p>\[
f_{Y_k}(y)\delta \approx P(y \leq Y_k \leq y + \delta) \approx P(k-1, y)\lambda\delta
\]</p>
</li>
</ul>
<p><strong><a style='color:red'>Erlang distribution</a></strong></p>
<p>\[
f_{Y_k}(y) = \frac{\lambda^k y^{k-1} e^{-\lambda y} }{(k-1)!} \text{, } y \geq 0<br />
\]</p>
<h4><a class="header" href="#925-memorylessness-and-the-fresh-start-property" id="925-memorylessness-and-the-fresh-start-property">9.2.5 Memorylessness and the fresh-start property</a></h4>
<ul>
<li>
<p>If we start watching at time <em>t</em>, we see Poisson process, independent of the history until time <em>t</em>. Then, time until next arrival follows exp(λ)</p>
</li>
<li>
<p>Time between first and second arrival, \(T_2 = Y_2 - Y_1\) follows exp(λ)</p>
</li>
<li>
<p>Similar for all \(T_k = Y_k - Y_{k-1} \text{, } k \geq 2\)</p>
</li>
<li>
<p>\(Y_k = T_1 + \dots + T_k\) is sum of i.i.d. exponentials</p>
<ul>
<li>
<p>\(E[Y_k] = \frac{k}{\lambda}\)</p>
</li>
<li>
<p>\(var(Y_k) = \frac{k}{\lambda^2}\)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#926-bernoullipoisson-relation" id="926-bernoullipoisson-relation">9.2.6 Bernoulli/Poisson relation</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/126948921-e39258a2-2396-4efb-8e13-4dbf605356f4.png" alt="image" /></p>
<table><thead><tr><th></th><th align="center">Poisson</th><th align="center">Bernoulli</th></tr></thead><tbody>
<tr><td>Times of Arrival</td><td align="center">Continuous</td><td align="center">Discrete</td></tr>
<tr><td>Arrival Rate</td><td align="center">λ per unit time</td><td align="center">p per trial</td></tr>
<tr><td>PMF of # of arrivals</td><td align="center">\[P(k,\tau) = \frac{(\lambda\tau)^ke^{-\lambda\tau}}{k!} \\E[N_\tau] \approx \lambda\tau \\ var(N_\tau) \approx \lambda\tau\]</td><td align="center">\[P_S(k) = \binom{n}{k}p^k(1-p)^{(n-k)} \\ \to \frac{\lambda^k}{k!}e^{-\lambda} \\ E[S] = np \\ var(S) = np(1-p) \]</td></tr>
<tr><td>Interarrival Time Distr.</td><td align="center">\[f_{T1}(t) = \lambda e^{-\lambda t}\] Exponential </br> \[E[T_1] = 1/\lambda \\ var(T_1) = 1/\lambda^2\]</td><td align="center">\[P_{T1} = (1-p)^{n-1}p\] Geometric </br> \[E[T_1] = 1/p \\ var(T_1) = \frac{1-p}{p^2}\]</td></tr>
<tr><td>Time to k-th arrival</td><td align="center">\[f_{Y_k}(y) = \frac{\lambda^k y^{k-1} e^{-\lambda y}}{(k-1)!}\] Erlang </br> \[E[Y_k] = k/\lambda \\ var(Y_k) = k/\lambda^2\]</td><td align="center">\[p_{Y_k}(t) = \binom{t-1}{k-1}p^k(1-p)^{t-k}\] Pascal</td></tr>
</tbody></table>
</br>
<h3><a class="header" href="#93-more-on-the-poisson-process" id="93-more-on-the-poisson-process">9.3 More on the Poisson process</a></h3>
<h4><a class="header" href="#931-the-sum-of-independent-poisson-random-variables" id="931-the-sum-of-independent-poisson-random-variables">9.3.1 The sum of independent Poisson random variables</a></h4>
<p>\[
P(k, \tau) = \frac{(\lambda\tau)^k e^{-\lambda\tau}}{k!}
\]</p>
<p>We call it a Poisson random variable with parameters \(\lambda\tau\)</p>
<p><a style='color:red'>The sum of independent Poisson random variables, with means/parameters \(\mu\) and \(\nu\) is Poisson with mean/parameter \(\mu + \nu\)</a></p>
<h4><a class="header" href="#932-merging-independent-poisson-processes" id="932-merging-independent-poisson-processes">9.3.2 Merging independent Poisson processes</a></h4>
<table><thead><tr><th align="center"></th><th align="center">0 </br> \(1 - \lambda_1\delta\)</th><th align="center">1 </br> \(\lambda_1\delta\)</th><th align="center">≥ 2 </br> \(O(\delta^2)\)</th></tr></thead><tbody>
<tr><td align="center"><strong>0</strong> \(1 - \lambda_2\delta\)</td><td align="center">\((1-\lambda_1\delta)(1-\lambda_2\delta)\)</td><td align="center">\(\lambda_1\delta(1-\lambda_2\delta)\)</td><td align="center">-</td></tr>
<tr><td align="center"><strong>1</strong> \(\lambda_2\delta\)</td><td align="center">\(\lambda_2\delta(1-\lambda_1\delta)\)</td><td align="center">\(\lambda_1\lambda_2\delta^2\)</td><td align="center">-</td></tr>
<tr><td align="center"><strong>≥ 2</strong> \(O(\delta^2)\)</td><td align="center">-</td><td align="center">-</td><td align="center">-</td></tr>
</tbody></table>
<ul>
<li>
<p>0 Arrivals \(\approx 1 - (\lambda_1 + \lambda_2)\delta\)</p>
</li>
<li>
<p>1 Arrivals \(\approx (\lambda_1 + \lambda_2)\delta\)</p>
</li>
<li>
<p>≥ 2 Arrivals \(O(\delta^2)\)</p>
</li>
</ul>
<p><a style='color:red'>Merging independent Poisson(λ1) and Poisson(λ1) result in Poisson(λ1 + λ2))</a></p>
<h4><a class="header" href="#933-the-time-the-firstlast-light-bulb-burns-out---minxyz-and-maxxyz-problem" id="933-the-time-the-firstlast-light-bulb-burns-out---minxyz-and-maxxyz-problem">9.3.3 The time the first(last) light bulb burns out - min{X,Y,Z} and max{X,Y,Z} problem</a></h4>
<p>Three lightbulbs have independent lifetimes X, Y, Z exponential(λ)</p>
<ol>
<li>
<p>The expected time until first lightbulb burnout:</p>
<ul>
<li>
<p>X, Y, Z: first arrivals in independent Poisson processes</p>
</li>
<li>
<p>Merged process: Poisson(3λ)</p>
</li>
<li>
<p>min{X, Y, Z}: 1st arrival in merged process \(\to E[min] = 1/3\lambda\)</p>
</li>
</ul>
</li>
<li>
<p>The expected time until the last lightbulb burnout:</p>
<ul>
<li>Merged process in different intervals</li>
</ul>
<p>\[
E[max] = \frac{1}{3\lambda} + \frac{1}{2\lambda} + \frac{1}{\lambda}
\]</p>
</li>
</ol>
<h4><a class="header" href="#934-splitting-of-a-poisson-process" id="934-splitting-of-a-poisson-process">9.3.4 Splitting of a Poisson process</a></h4>
<p>Split arrivals into two streams using independent coin flips of a coin with bias <em>q</em></p>
<p><em>Assume that coin flips are independent from the original Poisson process</em></p>
<ul>
<li>
<p><a style='color:red'>Resulting streams are Poisson with rate \(\lambda q, \lambda (1-q)\)</a></p>
</li>
<li>
<p>The splitted Poisson processes are <a style='color:red'>independent!</a></p>
</li>
</ul>
<h4><a class="header" href="#935-random-incidence-in-the-poisson-process" id="935-random-incidence-in-the-poisson-process">9.3.5 'Random incidence' in the Poisson process</a></h4>
<ol>
<li>
<p>Analysis</p>
<p><img src="https://user-images.githubusercontent.com/41487483/126982230-c8be5322-3130-43d5-8909-c9df592315a1.png" alt="image" /></p>
</li>
<li>
<p>Random incidence &quot;Paradox&quot; is not special to the Poisson process</p>
<ul>
<li>
<p>Example: interarrival times, i.i.d., equally likely to be 5 or 10 mins. Then expected value of <a style='color:red'>k-th interarrival time </a> = 7.5</p>
</li>
<li>
<p>Show up at a <a style='color:red'>&quot;random time&quot;</a> </p>
<ul>
<li>
<p>P(arrival duaring a 5-minute interarrival interval) = 1/3</p>
</li>
<li>
<p>Expected length of interarrival interval during which you arrive ≈ 8.3</p>
</li>
</ul>
</li>
<li>
<p><a style='color:red'>Sampling method matters</a> - <em>Different sampling methods can give different results</em></p>
<ul>
<li>
<p>Average family size? (3 families with one person, 1 family with 6 persons)</p>
<ul>
<li>
<p>look at a random family: 3/4x1 + 1/4x6</p>
</li>
<li>
<p>looat at a random persons's family: 3/9x1 + 6/9x6</p>
</li>
</ul>
</li>
<li>
<p>Average bus occupancy?</p>
</li>
<li>
<p>Average class size?</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3><a class="header" href="#94-additional-theoretical-background" id="94-additional-theoretical-background">9.4 Additional theoretical background</a></h3>
<h4><a class="header" href="#941-poisson-versus-normal-approximation-to-the-binomial" id="941-poisson-versus-normal-approximation-to-the-binomial">9.4.1 Poisson versus normal approximation to the binomial</a></h4>
<p>We have seen that a binomial random variable with parameters <em>n</em> and <em>p</em> can be approximated by a normal random variable (central limit theorem) but also by a Poisson random variable. Are these two facts contradictory? Fortunately not; the two approximations apply to different regimes:</p>
<ol>
<li>
<p>if we fix <em>p</em> and let \(n \to \infty)\), we are in the setting where the <strong>central limit theorem</strong> applies.</p>
</li>
<li>
<p>If we let \(n \to \infty)\), \(p \to 0)\), while keeping the product <em>np</em> fixed, the Poisson approximation applies.</p>
</li>
<li>
<p>If <em>p</em> is very small but <em>np</em> is very large, then two approximations agree.</p>
</li>
</ol>
<h4><a class="header" href="#942-sums-of-a-binomial-and-a-poisson-distributed-number-of-bernoulli-rvs" id="942-sums-of-a-binomial-and-a-poisson-distributed-number-of-bernoulli-rvs">9.4.2 Sums of a binomial and a Poisson-distributed number of Bernoulli r.v.'s</a></h4>
<p>Let \(X_1,X_2,...\) be independent Bernoulli random variables with parameter <em>p</em>, and <em>N</em> be a random variable that takes integer values and is independent of \(X_i, i = 1,2, \dots\) Let \(Y=X_1+X_2+ \dots +X_N\) for positive values of <em>N</em>, and let \(Y =0\)  when \(N=0\).</p>
<ul>
<li>
<p>If <em>N</em> is binomial with parameters <em>m</em> and <em>q</em>, then <em>Y</em> is binomial with parameters <em>m</em> and <em>pq</em>.</p>
<p><img src="https://user-images.githubusercontent.com/41487483/127369301-0884ba53-e79c-4cd9-8545-368f7ef5c03f.png" alt="image" /></p>
</li>
<li>
<p>If <em>N</em> is poisson with parameters \(\lambda\), then <em>Y</em> is Poisson with parameter \(\lambda\).</p>
<p><img src="https://user-images.githubusercontent.com/41487483/127369385-7d06f2f1-2866-4949-b342-933fe8bfe6d2.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#943-sums-of-a-geometrically-distributed-number-of-geometric-and-exponential-rvs" id="943-sums-of-a-geometrically-distributed-number-of-geometric-and-exponential-rvs">9.4.3 Sums of a geometrically-distributed number of geometric and exponential r.v.'s</a></h4>
<p>Let <em>N</em> be a geometric random variable with parameter <em>q</em>, and let \(X_1, X_2, \dots\) be random variables that are independent and independent of <em>N</em>. Let \(Y=X_1+\dots+X_N\).</p>
<ul>
<li>
<p>If \(X_i\) is geometric with parameter <em>p</em>, then <em>Y</em> is geometric with parameter <em>pq</em></p>
<p><img src="https://user-images.githubusercontent.com/41487483/127370021-c2299c8b-8107-4826-b1df-048ec9bdde81.png" alt="image" /></p>
</li>
<li>
<p>If \(X_i\) is exponential with parameter \(\lambda\), then <em>Y</em> is exponential with parameter \(\lambda q\)</p>
<p><img src="https://user-images.githubusercontent.com/41487483/127370100-fc323f23-7d82-4b50-aa98-2fc0bb17c31c.png" alt="image" /></p>
</li>
</ul>
<h1><a class="header" href="#improve-your-statistical-inferences" id="improve-your-statistical-inferences">Improve your statistical inferences</a></h1>
<h2><a class="header" href="#week-1" id="week-1">Week 1</a></h2>
<h3><a class="header" href="#11-frequentism-likelihoods-bayesian-statistics" id="11-frequentism-likelihoods-bayesian-statistics">1.1 Frequentism, Likelihoods, Bayesian statistics</a></h3>
<h3><a class="header" href="#12-what-is-p-value" id="12-what-is-p-value">1.2 What is p-value?</a></h3>
<h3><a class="header" href="#121-p-value" id="121-p-value">1.2.1 p-value</a></h3>
<p><strong>A p-value is the probability of getting the observed or more extreme data, assuming the null hypothesis is true.</strong></p>
<p>A p-value is the probability of the data, NOT the probability of a theory.</p>
<p>\(P(D*|H) \neq P(H|D)\)</p>
<p><em>Bayesian statistics is the only approach that will allow you to make statements about the probability that a theory is true.</em></p>
<p>If a p &gt; 0.05, the data we have observed is not surprising. It does not mean there is NO true effect. You need large samples to detect small effects.</p>
<h3><a class="header" href="#122-using-p-value-orrectly" id="122-using-p-value-orrectly">1.2.2 Using p-value orrectly</a></h3>
<p>Use p-value as a rule to guide behavior in the long run.</p>
<h1><a class="header" href="#inferential-statistics" id="inferential-statistics">Inferential Statistics</a></h1>
<p><a href="https://www.coursera.org/learn/inferential-statistics-intro/home/welcome">Here is the course link!</a></p>
<p><a href="https://www.notion.so/Inferential-Statistics-5dd1ac55e304409898a5877aadff7ecc">Summary of leaning objectives for each section</a></p>
<h2><a class="header" href="#1-clt-and-sampling" id="1-clt-and-sampling">1 CLT and Sampling</a></h2>
<h3><a class="header" href="#11-sampling-variability-and-clt" id="11-sampling-variability-and-clt">1.1 Sampling Variability and CLT</a></h3>
<h4><a class="header" href="#111-sample-distribution-and-sampling-distribution" id="111-sample-distribution-and-sampling-distribution">1.1.1 Sample distribution and sampling distribution</a></h4>
<ul>
<li>
<p>Sample distribution: sample mean and sample variability (standard deviation)</p>
</li>
<li>
<p>Sampling distribution</p>
<p>population mean (\(\mu \)) and population standard deviation (\(\sigma\))</p>
<p>\[
\mu = \frac{x_1 + x_2 + ... + x_N}{N}
\]</p>
<p>\[
\sigma = \sqrt{\frac{\sum\limits_{i=1}^{N}(x_i - \bar{x})^2}{N}}
\]</p>
<p><em>Most of time, population standard deviation \(\sigma\) is not known. Thus, \(\sigma\) is usually replaced by sampling standard deviation s</em></p>
<ul>
<li>
<p><strong>mean</strong>(\(\bar{x}) \approx \mu \) </p>
</li>
<li>
<p><strong>standard error:</strong>  \(SE = \frac{\sigma}{\sqrt{n}}\) &lt; \(\sigma\)</p>
</li>
</ul>
</li>
</ul>
<p><a href="https://gallery.shinyapps.io/CLT_mean/">The link to check up the shape of population distribution</a></p>
<h4><a class="header" href="#112-central-limit-theorem-clt" id="112-central-limit-theorem-clt">1.1.2 Central Limit Theorem (CLT)</a></h4>
<p><strong>The distribution of sample statistics is nearly normal, centered at the population mean, and with a standard error equal to the population standard deviation divided by square root of the sample size.</strong></p>
<p>\[
\bar{x} \sim N(mean = \mu, SE = \frac{\sigma}{\sqrt{n}}) 
\]</p>
<p><em>\(N\) refers to the shape of distribution, meaning normal distribution.</em></p>
<p><em>\(\sigma\) is usually unknown, so s is used to replace \(s\) - sampling standard deviation</em></p>
<h4><a class="header" href="#113-other-important-concepts-and-rules" id="113-other-important-concepts-and-rules">1.1.3 Other important concepts and rules</a></h4>
<ul>
<li>
<p><strong>standard deviation (\(\sigma\)) vs. standard error (SE)</strong></p>
<ul>
<li>
<p>\(\sigma\) measures the variability in the data</p>
</li>
<li>
<p>SE measures the variability in the sample mean (point estimates)</p>
</li>
</ul>
</li>
<li>
<p>sample size increases -&gt; SE decreases <em>(either from conceptual or mathematically \(SE = \frac{\sigma}{\sqrt{n}}\) point of view)</em></p>
</li>
<li>
<p>To reduce skewness, either increase sample size (observations) or number of samples</p>
</li>
<li>
<p><strong>Sampling distribution will be nearly normal only if (the condition of CLT)</strong></p>
<ul>
<li>
<p>the sample size is sufficiently large (n ≥ 30 or even larger if the data are considerably skewed) or the population is known to have a normal distribution</p>
</li>
<li>
<p>the observations in the sample are independent: random sample/assignment and n &lt; 10% of population if sampling without replacement</p>
</li>
</ul>
</li>
</ul>
</br>
<h3><a class="header" href="#12-confidential-intervals" id="12-confidential-intervals">1.2 Confidential Intervals</a></h3>
<h4><a class="header" href="#121-confidential-intervals" id="121-confidential-intervals">1.2.1 Confidential Intervals</a></h4>
<p><strong>confidence interval</strong> is defined as the plausible range of values for a population parameter. </p>
<p><strong>confidence level</strong> is defined as the percentage of random samples which  yield confidence intervals that capture the true population parameter.</p>
<p><strong>confidence interval for a population mean:</strong></p>
<p>\[
\bar{x} \pm z\frac{s}{\sqrt{n}}<br />
\] </p>
<p><strong>margin of error</strong> (ME) = \(z\frac{s}{\sqrt{n}} \)</p>
<ul>
<li>for 95% CI: \(\bar{x} \pm 2SE\) i.e., \(ME = 2SE\)</li>
</ul>
<p><em>conditions for this confidence interval is the same as conditions for CLT (independent and sample size)</em></p>
<p><img src="https://user-images.githubusercontent.com/41487483/119108985-0fff2a80-ba21-11eb-85e6-edf4997e6839.png" alt="image" /></p>
<p><a href="http://www.z-table.com/">z-table</a></p>
<h4><a class="header" href="#122-z-score-not-covered-in-the-course" id="122-z-score-not-covered-in-the-course">1.2.2 z-score (not covered in the course)</a></h4>
<ul>
<li>
<p><a href="https://www.usablestats.com/lessons/zarea">z-score for a single value</a> </p>
<p>Given we know the population parameters (\(\mu\) and \(\sigma\)), calculate z-score for any individual in the population:</p>
<p>\[
z = \frac{(x - \mu)}{\sigma}<br />
\]</p>
<p>Using z-table, the probability can be calculated.</p>
</li>
<li>
<p><a href="https://www.usablestats.com/lessons/1samplez">z-score for a sample mean</a></p>
<p>\[
z = \frac{(\bar{x} - \mu)}{\frac{\sigma}{\sqrt{n}}}<br />
\]</p>
</li>
<li>
<p><strong>Empirical rule</strong></p>
<ul>
<li>
<p>68% of values fall within 1 SE of the mean</p>
</li>
<li>
<p>95% fall within 2 SE of the mean</p>
</li>
<li>
<p>99% fall within 3 SE of the mean</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#123-accuracy-vs-precision" id="123-accuracy-vs-precision">1.2.3 Accuracy vs. Precision</a></h4>
<ul>
<li>
<p><strong>Accuracy</strong>: whether or not the CI contains the true population paramter.</p>
</li>
<li>
<p><strong>Precision</strong>: the width of a confidence intervals.</p>
</li>
</ul>
<p>Increasing CL, accuracy increases but precision decreases.</p>
<ul>
<li>To get a higher precision and high accuracy - increase sample size</li>
</ul>
<h3><a class="header" href="#123-required-sample-size-for-me" id="123-required-sample-size-for-me">1.2.3 Required sample size for ME</a></h3>
<p>\[
ME = z \frac{s}{\sqrt{n}}    \rightarrow n = \Bigg(\frac{z s}{ME}\Bigg)^2
\]</p>
<h2><a class="header" href="#13-r-vs-sampling-distribution" id="13-r-vs-sampling-distribution">1.3 R vs. sampling distribution</a></h2>
<ol>
<li>Load the package and dataset(ames)</li>
</ol>
<pre><code class="language-r">    library(statsr)
    library(dplyr)
    library(shiny)
    library(ggplot2)

    data(ames)
</code></pre>
<ol start="2">
<li>Distribution of areas of homes and summary statistics</li>
</ol>
<pre><code class="language-r">ames %&gt;%
    summarise(mu = mean(area), pop_med = median(area), 
        sigma = sd(area), pop_iqr = IQR(area),
        pop_min = min(area), pop_max = max(area),
        pop_q1 = quantile(area, 0.25),  # first quartile, 25th percentile
        pop_q3 = quantile(area, 0.75))  # third quartile, 75th percenti
</code></pre>
<ol start="3">
<li>Sample randome 50 houses and calculate the average area</li>
</ol>
<pre><code class="language-r">samp1 &lt;- ames %&gt;%
    sample_n(size=50)

samp1 %&gt;%
    summarise(x_bar = mean(area))

# or combine above two code chunks into one
samp1 &lt;- ames %&gt;%
    sample_n(size=50) %&gt;%
        summarise(x_bar = mean(area))
</code></pre>
<ol start="4">
<li>Estimate population mean by using sampling distribution</li>
</ol>
<p>Take 15,000 samples of size 50 from the population (<code>rep_sample_n</code>), calculate the mean of each sample, and store each result in a vector called 'sample_means50'.</p>
<pre><code class="language-r">sample_means50 &lt;- ames %&gt;%
  rep_sample_n(size = 50, reps = 15000, replace = TRUE) %&gt;%
    summarise(x_bar = mean(area))

ggplot(data = sample_means50, aes(x = x_bar)) +
  geom_histogram(binwidth = 20)
</code></pre>
<p>To get the summary statistics of 15,000 sample means, analyze the statistics from the 'sample_means50', which is actually a dataset containing 15,000 observations(x_bar).</p>
<pre><code class="language-r">sample_means50 %&gt;%
    summarise(sampling_x_bar = mean(x_bar))
</code></pre>
<h2><a class="header" href="#14-python-vs-sampling-distribution" id="14-python-vs-sampling-distribution">1.4 Python vs. sampling distribution</a></h2>
<ol>
<li>Load packages and import dataset</li>
</ol>
<pre><code class="language-py">import pandas as pd
import numpy as np
import random as random
import math
import matplotlib.pyplot as plt

ames = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/ames.csv&quot;)
#ames.head
#ames.columns
</code></pre>
<ol start="2">
<li>Distribution of population</li>
</ol>
<pre><code class="language-py">mu = np.average(ames[&quot;Lot.Area&quot;])
sigma = np.std(ames[&quot;Lot.Area&quot;])

plt.hist(ames[&quot;Lot.Area&quot;],30, range=[0, mu+5*sigma])
plt.show()
#right skewed distribution
</code></pre>
<ol start="3">
<li>Randomly take 10 samples</li>
</ol>
<pre><code class="language-py">samp1 = ames.sample(n=10,replace=True)
</code></pre>
<ol start="4">
<li>Take 1000 samples with size 200</li>
</ol>
<pre><code class="language-py">size = 200
num_samp = 1000
samp_mean = []

for m in range(num_samp):
  samp = ames.sample(n=size,replace=True)
  x_bar_samp = np.average(samp[&quot;Lot.Area&quot;])
  samp_mean.append(x_bar_samp)
  m += 1

x_bar_samp_mean = np.average(samp_mean)
x_bar_samp_se = np.std(samp_mean)/(math.sqrt(size))

print(x_bar_samp_mean)
print(x_bar_samp_se)

plt.hist(samp_mean, 20, range=[5000,15000])
plt.show()
</code></pre>
</br>
<h2><a class="header" href="#2-hypothesis-testing-and-significance" id="2-hypothesis-testing-and-significance">2 Hypothesis testing and significance</a></h2>
<h3><a class="header" href="#21-hypothesis-testing-for-a-mean" id="21-hypothesis-testing-for-a-mean">2.1 Hypothesis testing (for a mean)</a></h3>
<ul>
<li>
<p>Null hypothesis - \(H_0\) </p>
</li>
<li>
<p>Alternative hypothesis - \(H_A\)</p>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/119312937-3e744400-bc73-11eb-9396-704d391112eb.png" alt="image" /></p>
<p><em>The hypothesis is always about pop.parameters, never about sample statistics (because the sample statistics is certain).</em></p>
<ul>
<li>
<p><strong>p-value</strong> - P(observed or more extreme outcome | \(H_0\) true)</p>
<p>Given \(n = 50, \bar{x} = 3.2, s = 1.74, SE = 0.246\) </p>
<p>We are looking for \(P(\bar{x} &gt; 3.2 | H_0 : \mu = 3) \)</p>
<p>Since we believe that null hypothesis is true, \(\bar{x} \sim N(\mu = 3, SE = 0.246)\) based on the CLT.</p>
<p><em>test statistics</em>: z-score = (3.2-3)/0.246 = 0.81, which is used to calculate the p-value (the probability of observing data at least favorable to the alternative hypothesis as our current data set, if the null hypothesis was true)</p>
<p>p-value = P(z &gt; 0.81) = 0.209</p>
</li>
</ul>
<p><strong>Decision based on the p-value</strong></p>
<ul>
<li>
<p>p-value &lt; the <strong>significant level</strong>, \(\alpha\) (usually 5%): it is unlikely to observe the data if the null hypothesis is true: <a style="color:red">Reject \(H_0\)</a></p>
</li>
<li>
<p>p-value ≥ \(\alpha\): it is likely to occur even if the null hypothesis were true: <a style="color:red"> Do no reject \(H_0\)</a></p>
</li>
</ul>
<p><strong>two-sided(tailed) tests</strong></p>
<p>In the same case, \(P(\bar{x} &gt; 3.2 \text{  or  } \bar{x} &gt; 2.8| H_0 : \mu = 3) \)</p>
<p>p-value = \(P(z &gt; 0.81) + P(z &lt; -0.81) = 0.418 \) --- fail to reject \(H_0\).</p>
<p><img src="https://user-images.githubusercontent.com/41487483/119316226-ff47f200-bc76-11eb-944a-1c63ee237ebd.png" alt="image" /></p>
<h3><a class="header" href="#22-significance" id="22-significance">2.2 Significance</a></h3>
<h4><a class="header" href="#221-inference-for-other-estimators" id="221-inference-for-other-estimators">2.2.1 Inference for other estimators</a></h4>
<ul>
<li>
<p>point estimates: </p>
<p><em>\(\hat{\theta}\): \(\hat{\theta}_{LMS}\) or (\(\hat{\theta} _{MAP}\)) the concept might be different from MIT statistics course</em></p>
<ol>
<li>
<p>sample mean</p>
</li>
<li>
<p>difference between sample means</p>
</li>
<li>
<p>sample proportion \(\hat{p}\)</p>
</li>
<li>
<p>difference between two proportions</p>
</li>
</ol>
</li>
<li>
<p>two requirements:</p>
<ul>
<li>
<p>nearly normal sampling distribution</p>
</li>
<li>
<p>unbiased estimator assumption: <strong>point estimates</strong> are unbiased, i.e., the sampling distribution of the estimate is centered at the true population parameter it estimates.</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#22-decision-errors" id="22-decision-errors">2.2 Decision errors</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/125154284-93074c00-e159-11eb-940d-bbec5059b97d.png" alt="image" /></p>
<p><strong>Decrease significance level (\(\alpha\)) decrease Type I error rate</strong></p>
<p>\(P(\text{Type I error}|H_0 \text{ true}) = \alpha\)</p>
<ul>
<li>
<p><strong>Choosing \(\alpha\)</strong></p>
<ul>
<li>
<p>if Type I error is dangerous or costly, choose a small significance level (e.g. 0.01)</p>
</li>
<li>
<p>if Type II error is dangerous or costly, choose a high significance level (e.g. 0.10)</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/125154728-c054f980-e15b-11eb-962c-f200d24dedd0.png" alt="image" /></p>
<p>\(\beta\) depends on the <strong>effect size \(\delta\)</strong> - difference between point estimate and null value.</p>
<h4><a class="header" href="#223-significance-level-vs-confidence-level" id="223-significance-level-vs-confidence-level">2.2.3 Significance level vs. confidence level</a></h4>
<ul>
<li>
<p>complement each other depending on one-sided or two -sided tests</p>
<ul>
<li>two-sided tests: Significance level = 1 - confidence level </li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/125154861-791b3880-e15c-11eb-98d8-55756cf27eaa.png" alt="image" /></p>
<ul>
<li>
<p>one-sided tests: Significance level ≠ confidence level</p>
<p>CL = 1 - 2 x alpha</p>
<p><img src="https://user-images.githubusercontent.com/41487483/125154873-8cc69f00-e15c-11eb-9701-9d1498add575.png" alt="image" /></p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#224-statistical-vs-practical-significance" id="224-statistical-vs-practical-significance">2.2.4 Statistical vs. practical significance</a></h4>
<ul>
<li>
<p>practical significance </p>
<p>Real difference between point estimator and null value are easier to detect with larger samples (effect size)</p>
</li>
<li>
<p>statistical significance </p>
<p>very large samples will result in statistical significance even for tiny differences between sample mean and the null value (effect size), even when the difference is not practically significant. </p>
</li>
</ul>
<h2><a class="header" href="#3-inference-for-comparing-means" id="3-inference-for-comparing-means">3 Inference for Comparing Means</a></h2>
<h3><a class="header" href="#31-t-distribution-and-comparing-two-means" id="31-t-distribution-and-comparing-two-means">3.1 t-distribution and comparing two means</a></h3>
<h4><a class="header" href="#311-t-distribution" id="311-t-distribution">3.1.1 t-distribution</a></h4>
<p>What purpose does a large sample serve?</p>
<p>As long as observations are independent, and the population distribution is not extremely skewed, a large sample would ensure that</p>
<ul>
<li>
<p>the sampling distribution of the mean is nearly normal.</p>
</li>
<li>
<p>the estimate of the standard error is reliable: \(\frac{s}{\sqrt{n}}\)</p>
</li>
</ul>
<p><strong>t-distribution</strong></p>
<ul>
<li>
<p>when <strong>σ</strong> unknown(almost always), use the t-distribution to address the uncertainty of the standard error estimate</p>
</li>
<li>
<p>bell shaped but thicker tails than the normal</p>
<ul>
<li>
<p>observations more likely to fall beyond 2 SDs from the mean</p>
</li>
<li>
<p>extra thick tails helpful for mitigating the effect of a less reliable estimate for the standard error of the sampling distribution</p>
</li>
</ul>
</li>
<li>
<p>always centered as 0</p>
</li>
<li>
<p>only has one parameter <strong>degress of freedom(df)</strong> to determine the thickness of tails: higher df, less thick the tail</p>
<p><em>the normal distribution has two parameters: mean and SD</em></p>
</li>
<li>
<p>for inference on a mean where <strong>σ</strong> unknown, the calculation is the same way as normal distribution </p>
<p>\[
T = \frac{\text{obs - null}}{SE}
\] </p>
<ul>
<li>find p-value (one or two tail area, based on \(H_A\))</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#312-inference-for-a-mean" id="312-inference-for-a-mean">3.1.2 Inference for a mean</a></h4>
<p><a style="color:red">estimating the mean = point estimate ± margin of error </a></p>
<p>\[
\bar{x} \pm t_{df}^*SE_{\bar{x}} 
\\ SE_{\bar{x}} = \frac{s}{\sqrt{n}} 
\]</p>
<p><strong>degrees of freedome for t statistic for inference on one sample mean</strong></p>
<p>\[
df = n - 1<br />
\]</p>
<h4><a class="header" href="#313-inference-for-comparing-two-independent-means" id="313-inference-for-comparing-two-independent-means">3.1.3 Inference for comparing two independent means</a></h4>
<p>estimating the mean = point estimate ± margin of error</p>
<p>\[
(\bar{x_1} - \bar{x_2}) \pm t_{df}^*SE_{(\bar{x_1} - \bar{x_2})} 
\]</p>
<ul>
<li>
<p>SE of difference between two independent means</p>
<p>\[
SE_{(\bar{x_1} - \bar{x_2} )}= \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\]</p>
</li>
<li>
<p>DF for t statistics for inference on difference of two means</p>
<p>\[
df = min(n_1-1, n_2-1)
\]</p>
</li>
</ul>
<ul>
<li>
<p>Conditions for inference for comparing two independent means</p>
<ol>
<li>independence: </li>
</ol>
<pre><code>  - within groups: 

      - random sample/assignment

      - if samping without replacement, n &lt; 10% of population

  - between groups: not paired
</code></pre>
<ol start="2">
<li>Sample size/skew: the more skew in the population distributions, the higher the sample size needed.</li>
</ol>
</li>
</ul>
<h4><a class="header" href="#314-inference-for-comparing-two-paried-means" id="314-inference-for-comparing-two-paried-means">3.1.4 Inference for comparing two paried means</a></h4>
<p>When two sets of observations have a special correspondence(not independent), they are said to be <strong>paired</strong>.</p>
<p>Two analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations.</p>
<ul>
<li>
<p>Parameter of interest: \(\mu_{diff}\) - average difference between the reading and writing scores of <strong>all</strong> high school students</p>
</li>
<li>
<p>Point estimate: \(\bar{x}_{diff}\) - average difference between the reading and writing scores of <strong>sampled</strong> high school students</p>
</li>
<li>
<p>\(SE = \frac{s_{diff}}{n}\)</p>
</li>
</ul>
<p><strong>Summary</strong></p>
<ul>
<li>
<p>paired data (2 var.) \(\to\) differences (1 var.)</p>
</li>
<li>
<p>most often: \(H_0:\mu_{diff} = 0\)</p>
</li>
<li>
<p>same individuals: pre-post studies, repeated measures, etc.</p>
</li>
<li>
<p>different but dependent individuals: tiwns, partners, etc.</p>
</li>
</ul>
<h4><a class="header" href="#315-power" id="315-power">3.1.5 Power</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/127775746-38903dad-60bf-4393-b8dc-360b78157370.png" alt="image" /></p>
<p><strong>Power</strong> of a test is the probability of correctly rejecting H0, and the probability is \(1-\beta\)</p>
<ul>
<li>Practical problem 1: calculate power for a range of sample sizes and choose target power</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/128233817-c00b3558-5c95-4a10-9e4a-9bfdc107bc85.png" alt="image" /></p>
<ul>
<li>Practical problem 2: calculate required sample size for a desired level of power </li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/127775861-e1c96076-91d9-42a3-9431-9d482ee92888.png" alt="image" /></p>
</br>
<h3><a class="header" href="#32-anova-and-bootstrapping" id="32-anova-and-bootstrapping">3.2 ANOVA and Bootstrapping</a></h3>
<h4><a class="header" href="#321-comparing-more-than-two-means----f-distribution" id="321-comparing-more-than-two-means----f-distribution">3.2.1 Comparing more than two means -- F distribution</a></h4>
<p><strong>ANOVA (analysis of variance) test</strong></p>
<ul>
<li>
<p>\(H_0\): the mean outcome is the same across all categories</p>
</li>
<li>
<p>\(H_A\): at least one pair of means are different from each other</p>
</li>
</ul>
<table><thead><tr><th align="center">t-test</th><th align="center">ANOVA</th></tr></thead><tbody>
<tr><td align="center">compute a test statistic (a ratio)</td><td align="center">Compute a test statistic (a ratio)</td></tr>
<tr><td align="center">\[t = \frac{(\bar{x_1}-\bar{x_2})-(\mu_1-\mu_2)}{SE_{(\bar{x_1}-\bar{x_2})}}\]</td><td align="center">\[F = \frac{\text{variability bet. groups}}{\text{variability within groups}}\]</td></tr>
</tbody></table>
<ul>
<li>
<p>In order to be able to reject \(H_0\), we need a small p-value, which requires a large F statistic.</p>
</li>
<li>
<p>Obtaining a large F statistic requires that the variability between sample means is greater than the variability within the samples.</p>
</li>
</ul>
<h4><a class="header" href="#322-anova" id="322-anova">3.2.2 ANOVA</a></h4>
<ul>
<li>
<p>variability partitioning </p>
<p align="center"><img src="https://user-images.githubusercontent.com/41487483/128079731-462101ec-5e23-4543-8760-e50cf28639ff.png"  width="500" height="180"></p>
</li>
<li>
<p>ANOVA Output</p>
<p><b><div style="text-align: center"> ANOVA Output table</div></b></p>
<p align="center">
  <img src="https://user-images.githubusercontent.com/41487483/128228631-b9e3b806-e2cb-48bf-b005-725ecdb55434.png" width="600" height="150">
  </p>
<ul>
<li>
<p>The first row is about between group variability (<u>Group row</u>) and the second the row is the within group variability (<u>Error row</u>)</p>
</li>
<li>
<p>Sum square error</p>
</li>
</ul>
<pre><code>  - Total: sum of squares total (SST) measures the total variability in the response variable. The caculation is very similar to that of variance except for no dividing by the sample size.

      \\[
          SST = \sum\limits_{i=1}^n (y_i-\bar{y})^2    
      \\]

      \\(y_i\\): value of the response variable for each observation

      \\(\bar{y}\\): grand mean of the response variable

  - Group: sum of squares groups (SSG) measures the variability between groups. &lt;u&gt;Explained variability&lt;/u&gt;: squared deviation of group means from overall mean, weighted by sample size. 

      \\[
          SSG = \sum\limits_{j=1}^k n_j(\bar{y_j}-\bar{y})^2    
      \\]

      \\(n_j\\): number of observations in group *j*

      \\(y_j\\): mean of the response variable for group *j*

      \\(\bar{y}\\): grand mean of the response variable

  - Error: sum of squares error (SSE) measures the variability within groups. &lt;u&gt;Unexplained variability&lt;/u&gt;: unexplained by the group variable due to other reasons

      \\[
          SSE = SST - SSG  
      \\]
</code></pre>
<ul>
<li>
<p>DF: degree of freedom</p>
</li>
<li>
<p>Mean square error: average variability between and within groups, calculated as the total variability (sum of squares) scaled by the associated degrees of freedom.</p>
<ul>
<li>
<p>group: MSG = SSG/DF<sub>G</sub></p>
</li>
<li>
<p>error: MSE = SSE/DF<sub>E</sub></p>
</li>
</ul>
</li>
<li>
<p>F statistics: ratio of the average between group and within group variabilities</p>
<p>\[
F = \frac{MSG}{MSE}<br />
\]</p>
</li>
<li>
<p>Calculate p-value according to F statistics, and remember F always positive we only calculate one-tail.</p>
<ul>
<li>
<p>if p-value is small (less than \(\alpha\)): reject H0</p>
<p>The data provide convincing evidence that at least one pair of population means are different from each other (but we cannot tell which one)</p>
</li>
<li>
<p>if p-value is large (larger than \(\alpha\)): fail to reject H0</p>
<p>The data do not provide convincing evidence that at least one pair of population means are different from each other; the observed difference in sample means are attributable to sampling variability (or chance)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#323-anova-conditions" id="323-anova-conditions">3.2.3 ANOVA conditions</a></h4>
<ol>
<li>
<p>Independence: between groups and within groups</p>
</li>
<li>
<p>Approximate normality: distributions should be nearly normal within each group</p>
</li>
<li>
<p>constant variance: groups should have roughly equal variability </p>
<p><em>side-by-side boxplot is helpful to check constant variance condition</em></p>
</li>
</ol>
<h4><a class="header" href="#324-multiple-comparisons" id="324-multiple-comparisons">3.2.4 Multiple comparisons</a></h4>
<ul>
<li>
<p><strong>Bonferroni correction</strong>: adjust \(\alpha\) by the number of comparison being considered K</p>
<p>\[
K = \frac{k(k-1)}{2} \\
\alpha^* = \alpha/K<br />
\]</p>
</li>
<li>
<p>Pairwise comparisons:</p>
<ul>
<li>
<p>constant variance \(to\) use consistent standard error and degrees of freedom for all tests</p>
</li>
<li>
<p>compare p-values from each test to the modified significance level</p>
</li>
<li>
<p>Standard error for multiple pairwise comparisons:</p>
<p>\[
SE = \sqrt{\frac{MSE}{n_1}+\frac{MSE}{n_2}}
\]</p>
<p>compared to t test between two independent groups \(SE = \sqrt{\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2}}\)</p>
</li>
<li>
<p>Degrees of freedom for multiple pairwise comparisons: df = df<sub>E</sub></p>
<p>compared to t test: df = min(n<sub>1</sub> - 1, n<sub>2</sub> - 1)</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#325-bootstrapping" id="325-bootstrapping">3.2.5 Bootstrapping</a></h4>
<ul>
<li>
<p><strong>Bootstrapping scheme:</strong></p>
<ol>
<li>
<p>take a bootstrap sample - a random sample taken with replacement from the original sample, of the same size as the original sample</p>
</li>
<li>
<p>calculate bootstrap statistic - mean, median, proportion, etc. computed on the bootstrap samples.</p>
</li>
<li>
<p>repeat steps 1 and 2 many times to create a bootstrap distribution - a distribution of bootstrap statistics.</p>
</li>
</ol>
</li>
<li>
<p>calculate confidence interval:</p>
<ol>
<li>
<p>percentile method</p>
</li>
<li>
<p>standard error method</p>
</li>
</ol>
</li>
<li>
<p>limitations</p>
<ul>
<li>
<p>not as rigid conditions as CLT based methods</p>
</li>
<li>
<p>if bootstrap distribution is extremely skewed or sparse, the bootstrap interval might be unreliable</p>
</li>
<li>
<p>A representative sample is still needed - if the sample is biased, the estimates resulting from this sample will also be bias.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Bootstrap vs. sampling distribution</strong></p>
<ul>
<li>
<p>sampling distribution: created using sampling with replacement from the <u>population</u></p>
</li>
<li>
<p>Bootstrap distribution: created using sampling with replacement from the <u>sample</u></p>
</li>
<li>
<p>Both are distributions of sample statistics</p>
</li>
</ul>
</br>
<h2><a class="header" href="#4-inference-for-proportion" id="4-inference-for-proportion">4 Inference for Proportion</a></h2>
<p><strong>Categorical variables</strong> oppose to <em>numerical variables</em></p>
<ul>
<li>
<p>one categorical variable: </p>
<ul>
<li>
<p>two levels: success-failure</p>
</li>
<li>
<p>more than two levels</p>
</li>
</ul>
</li>
<li>
<p>two categorical variables: </p>
<ul>
<li>
<p>two levels: success-failure</p>
</li>
<li>
<p>more than two levels</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#41-inference-for-proportions" id="41-inference-for-proportions">4.1 Inference for proportions</a></h3>
<h4><a class="header" href="#411-sampling-variability-and-clt-for-proportions" id="411-sampling-variability-and-clt-for-proportions">4.1.1 Sampling Variability and CLT for Proportions</a></h4>
<p>For numerical variables, sample statistic from sampling distribution is <u>mean</u></p>
<p>For categorial variables, sample statistic from sampling distribution is <b>proportion</b></p>
<p><strong>CLT for proportions</strong>: The distribution of sample proportion is nearly normal, centered at the population proportion, and with a standard error inversely proportional to the sample size.</p>
<p>\[
\hat{p} \sim N \left( mean=p, SE=\sqrt{\frac{p(1-p)}{n}}\right)
\]</p>
<ul>
<li>
<p>Conditions for the CLT</p>
<ul>
<li>
<p>Independence</p>
</li>
<li>
<p>Sample size/skew: there should be at least 10 successes and 10 failures in the sample: np ≥ 10 and n(1-p) ≥ 10.</p>
</li>
</ul>
</li>
<li>
<p>What if the success-failure condition is not met:</p>
<ul>
<li>
<p>the center of the sampling distribution will still be around the true population proportion</p>
</li>
<li>
<p>the spread of the sampling distribution can still be approximated using the same formula for the standard error</p>
</li>
<li>
<p>the shape of the distribution will depend on whether true population proportion is close to 0 (righ skew) or to 1 (left skew).</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#412-confidence-interval-for-a-proportion" id="412-confidence-interval-for-a-proportion">4.1.2 Confidence interval for a proportion</a></h4>
<ul>
<li>
<p>parameter of interest: \(p\)</p>
</li>
<li>
<p>point estimate: \(\hat{p}\) sample proportion</p>
</li>
<li>
<p>estimating a proportion: point estimate ± margin of error</p>
<p>\[
\hat{p} = z^* SE_{\hat{p}}<br />
\]</p>
<ul>
<li>
<p>SE for  a proportion for calculating a confidence interval: </p>
<p>\[
SE_{\hat{p}} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} 
\]</p>
</li>
<li>
<p>calculating the required sample size for desired ME</p>
<ul>
<li>
<p>use \(\hat{p}\) from previous study</p>
</li>
<li>
<p>if no previous study, use \(\hat{p} = 0.05\) as it gives the most conservative estimate - highest possible sample size</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#413-hypothesis-testing-for-a-proportion" id="413-hypothesis-testing-for-a-proportion">4.1.3 Hypothesis testing for a proportion</a></h4>
<ol>
<li>
<p>set the hypothesis</p>
</li>
<li>
<p>calculate the point estimate \(\hat{p}\)</p>
</li>
<li>
<p>Check conditions</p>
</li>
<li>
<p>Draw sampling distribution, shade p-value, calculate test statistic.</p>
</li>
<li>
<p>Make a decision based on the research context.</p>
</li>
</ol>
<p><em>Null hypothesis always contains a &quot;=&quot; sign.</em></p>
<h4><a class="header" href="#414-estimating-the-difference-between-two-proportions" id="414-estimating-the-difference-between-two-proportions">4.1.4 Estimating the Difference Between Two Proportions</a></h4>
<p><em>calculating a confidence interval for the difference between the two population proportions that are unknown using data from our sample</em></p>
<ul>
<li>
<p>Estimating the difference between two proportions:</p>
<p>\[
(\hat{p}_1 - \hat{p} _2) \pm z^* SE _{(\hat{p}_1 - \hat{p}_2)} \\
SE = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}<br />
\]</p>
</li>
<li>
<p>conditions for inference for comparing two independent proportions</p>
<ol>
<li>
<p>Independent: within groups and between groups</p>
</li>
<li>
<p>Sample size/kew: each sample should meet the success-failure condition</p>
</li>
</ol>
</li>
</ul>
<h4><a class="header" href="#415-hypothesis-test-for-comparing-two-proportions" id="415-hypothesis-test-for-comparing-two-proportions">4.1.5 Hypothesis Test for Comparing Two Proportions</a></h4>
<p><strong>pooled proportion</strong> \(H_0: p_1 = p_2 = \) pooled proportion</p>
<p>\[
\hat{p}_{pool} = \frac{\text{total successes}}{\text{total n}}<br />
\]</p>
<p><img src="https://user-images.githubusercontent.com/41487483/128409785-f2615fab-55af-4fa7-b9b4-c4e4f1b0e64b.png" alt="image" /></p>
<p><img src="https://user-images.githubusercontent.com/41487483/128409969-9afd3ef0-f2e1-4ff9-8e15-906ea24cf9e1.png" alt="image" /></p>
<h4><a class="header" href="#42-simulation-based-inference-for-proportions-and-chi-square-testommg" id="42-simulation-based-inference-for-proportions-and-chi-square-testommg">4.2 Simulation based inference for proportions and chi-square testommg</a></h4>
<h4><a class="header" href="#421-small-sample-proportion" id="421-small-sample-proportion">4.2.1 Small sample proportion</a></h4>
<p>which does not meet success-failure condition</p>
<ul>
<li>
<p>inference via simulation</p>
</li>
<li>
<p>setting up a simulation assuming H0 true</p>
<ul>
<li>
<p>the ultimate goal of a hypothesis test is a p-value</p>
</li>
<li>
<p>devise a simulation scheme that assumes the null hypothesis is true</p>
</li>
<li>
<p><u>repeat the simulation many times and record relevant sample statistic</u> <em>CLT?</em></p>
</li>
<li>
<p>calculate p-value as the proportion of simulations that yield a result favorable to the alternative hypothesis</p>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/128587428-e8e5f4f8-6368-47b5-a79d-82ec357ae12e.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#422-comparing-two-small-sample-proportions" id="422-comparing-two-small-sample-proportions">4.2.2 Comparing Two Small sample proportions</a></h4>
<img src="https://user-images.githubusercontent.com/41487483/128587776-777b7026-908e-48cb-832b-7d3247cccde4.png" width="300" height="300">
<p>For comparing two proportions with hypothesis test, the <strong>pooled proportion</strong> should be used.</p>
<img src="https://user-images.githubusercontent.com/41487483/128587831-1fe22540-94c5-4fc0-89a9-b3cff4d4e8dc.png" width="250" height="250">
<p><img src="https://user-images.githubusercontent.com/41487483/128587856-fe835c70-4c71-4014-955d-f51abcbb8865.png" alt="image" /></p>
<h4><a class="header" href="#423-chi-square-gof-test" id="423-chi-square-gof-test">4.2.3 Chi-Square GOF test</a></h4>
<p>Deals with data with one category with more than two levels to a hypothesis distribution</p>
<div style="color:blue">goodness of fit</div> test is to evaluate how well the observed data fit the expected distribution
<ul>
<li>
<p>Conditions for the Chi-Sqaure test:</p>
<ul>
<li>
<p>Independence</p>
<ul>
<li>
<p>random sample/assignment</p>
</li>
<li>
<p>if sampling without replacement, n &lt; 10% of population</p>
</li>
<li>
<p>each case only contributes to one cell in the table</p>
</li>
</ul>
</li>
<li>
<p>Sample size: each particular scenarior (i.e., cell) must have at least 5 expected cases.</p>
</li>
</ul>
</li>
<li>
<p>Anatomy of a test statistic</p>
<p>General form of a test statistic </p>
<p>\[
\frac{\text{point estimate - null value}}{\text{SE of point estimate}}<br />
\]</p>
<ol>
<li>
<p>identifying the difference between a point estimate and an expected value if the null hypothesis were true</p>
</li>
<li>
<p>standardizing that difference using the standard error of the point estimate</p>
</li>
</ol>
</li>
<li>
<p><strong>Chi-Square \(\chi\)) statistic</strong>: dealing with counts and investigating how far the observed counts are from the expected counts</p>
<p>\[
\chi^2 = \sum\limits_{i=1}^{k}\frac{(O-E)^2}{E}<br />
\]</p>
<p>O: observed</p>
<p>E: expected</p>
<p>k: number of cells</p>
</li>
<li>
<p><strong>Chi-Square \(\chi\)) distribution</strong>: has just one parameter</p>
<ul>
<li>degrees of freedom (df): influence the shape, center and spread</li>
</ul>
<img src="https://user-images.githubusercontent.com/41487483/128588353-db8fc2ef-87ce-4caf-a55b-129b29c9a318.png" width="450" height="300">
</li>
<li>
<p>p-value</p>
<ul>
<li>
<p>p-value for a chi-square test is defined as the tail area above the calculated test statistic</p>
</li>
<li>
<p>the test statistic is always <strong>positive</strong>, and a higher test statistic means a higher deviation from the null hypothesis</p>
</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#424-chi-square-independence-test" id="424-chi-square-independence-test">4.2.4 Chi-Square independence test</a></h4>
<p>Deals with two categorial variables at least one with &gt; 2 levels</p>
<p><strong>Chi-square independence test</strong> is to evaluate the relationship between two categorical variables</p>
<p>\[
\chi^2 = \sum\limits_{i=1}^{k}\frac{(O-E)^2}{E}<br />
\]</p>
<ul>
<li>
<p><strong>df = (#rows - 1)x(#columns - 1)</strong></p>
</li>
<li>
<p>the same conditions as chi-square GOF test</p>
</li>
</ul>
<h1><a class="header" href="#statistics-with-python" id="statistics-with-python">Statistics with Python</a></h1>
<h2><a class="header" href="#1-introduction-to-probability-and-statistics" id="1-introduction-to-probability-and-statistics">1 Introduction to Probability and Statistics</a></h2>
<h3><a class="header" href="#11-probability-theory" id="11-probability-theory">1.1 Probability Theory</a></h3>
<p>Use Python to simulate coin tossing problem,</p>
<pre><code class="language-py"># Generate the sum of k coin flips, repeat that n times
def generate_counts(k=1000, n=100):
  X=2*(np.random.rand(k,n)&gt;0.5)-1 # generate a kXn matrix of +-rando
  S=np.sum(X, axis=0)
  return S
coins_flip = generate_counts()

# plot a histogram
plt.style.use('ggplot')
plt.hist(coins_flip, 10, range = [-400, 400])
plt.show
</code></pre>
<p><code>np.random.rand(k,n)&gt;0.5</code> generate a True/False (k,n) matrix, but it transforms to an integer matrix with <code>2*</code></p>
<ul>
<li>
<p>In most cases, we can <strong>approximate</strong> probabilities using simulations (Monte-Carlo simulations)</p>
</li>
<li>
<p>However, calculating the probabilities is better because it provides a precise answer and is much faster than Monte-Carlo simulations.</p>
</li>
</ul>
<h3><a class="header" href="#12-what-is-statistics" id="12-what-is-statistics">1.2 What is statistics?</a></h3>
<p><strong>Statistics is about analyzing real-world data and drawing conclusions.</strong></p>
<p><strong>The logical of Statistic inference</strong></p>
<p>To answer the question &quot;whether the coin is biased given 570 heads after tossing 1000 times&quot;,</p>
<ol>
<li>
<p>Suppose that the coin is fair</p>
</li>
<li>
<p>Use <strong>probability theory</strong> to compute the probability of getting at least 570 (or 507) heads</p>
</li>
<li>
<p>If this probability is very small, then we can reject with confidence the hypothesis that the coin is fair.</p>
<p><em>Given \(x_i = -1\) for tails and \(x_i = +1\), we looked at the sum \(S_k = \sum_{i=1}^{k} x_i\)</em></p>
<p><em>If number of heads = 570, then \(S_{1000} = 140\)</em></p>
<p><em>It is known that it is unlikely that \(|S_k| &gt; 4 \sqrt{k}\), that is \(|S_{1000}| &gt; 4 sqrt{1000} \approx 126.5\)</em></p>
<pre><code class="language-py">from math import sqrt
4*sqrt(1000)
</code></pre>
<p><em>Therefore, it is very unlikely that the coin is unbiased. -&gt; the coin is probably biased.</em></p>
</li>
</ol>
<h3><a class="header" href="#13-three-card-puzzle" id="13-three-card-puzzle">1.3 Three card puzzle</a></h3>
<p><strong>Three cards in a hat</strong></p>
<p>Suppose we have three cards in a hat:</p>
<ul>
<li>
<p>'R''B' - one card is painted blue on one side and red on the other</p>
</li>
<li>
<p>'R''R' - one card is painted blue on both sides</p>
</li>
<li>
<p>'B''B' - one card is painted red on both sides</p>
</li>
</ul>
<p>I pick one of the three cards at random, flip it to a random side, and place it on the table. If the other side of the card has a different color I pay you $1; if not you pay me $1.</p>
<p><strong>Monte Carlo simulation</strong></p>
<pre><code class="language-py">
red_bck=&quot;\x1b[41m%s\x1b[0m&quot; 
blue_bck=&quot;\x1b[44m%s\x1b[0m&quot;
red=red_bck%'R'
blue=blue_bck%'B'
Cards=[(red,blue),(red,red),(blue,blue)]
counts={'same':0,'different':0}

for j in range(50):
    i=int(np.random.rand()*3.) # Generate a random integer in an array [0,1,2] indicating three cards
    side=int(np.random.rand()*2.) # Generate either 0 or 1 indicating the color
    C=Cards[i]
    if(side==1): # select which side to be &quot;up&quot; ('red' is &quot;up&quot;)
        C=(C[0],C[1]) # two sides of the selected cards
    same = 'same' if C[0]==C[1] else 'different' # count the number of times the two sides are the same or different.
    counts[same]+=1
    print(''.join(C)+' %-9s'%same, end='')
    if (j+1)%5==0:
    print()
print()
print(counts)
</code></pre>
<h2><a class="header" href="#2-elements-sets-and-membership" id="2-elements-sets-and-membership">2 Elements, sets and membership</a></h2>
<h3><a class="header" href="#21-basic-concepts" id="21-basic-concepts">2.1 Basic concepts</a></h3>
<p><strong>Common sets</strong></p>
<ul>
<li>
<p>Intergers {..., -2, -1, 0, 1, 2, ...} \(Z\)</p>
</li>
<li>
<p>Naturals {..., 0, 1, 2, 3, ...} \(N\)</p>
</li>
<li>
<p>Positives {1, 2, 3, ...} \(P\)</p>
</li>
<li>
<p>Rationals {interger ratios m/n, \(n \neq 0\)} \(Q\)</p>
</li>
<li>
<p>Reals {...Google...} \(R\)</p>
</li>
</ul>
<ul>
<li>
<p>The <strong>order</strong> and <strong>repetition</strong> do no matter: </p>
<ul>
<li>
<p>{0,1} = {1,0}</p>
</li>
<li>
<p>{0,1,1,1} = {0,1}</p>
</li>
</ul>
</li>
</ul>
<p><strong>Special sets</strong></p>
<ul>
<li>
<p>Empty set: \(x \notin \varnothing\)</p>
</li>
<li>
<p>Universal set: \(\forall x \in \Omega\)</p>
</li>
</ul>
<ul>
<li>
<p>Define a set in python</p>
<ul>
<li>
<p>Define a set: <code>set1={1,2}</code> or <code>set2=set({2,3})</code></p>
</li>
<li>
<p>Define an empty set: <code>set()</code> or <code>set({})</code></p>
</li>
</ul>
</li>
<li>
<p>Membership - <code>in</code> and <code>not in</code></p>
</li>
<li>
<p>Test empty - <code>not</code></p>
</li>
</ul>
<pre><code class="language-py">S = set()
not S
#Output: True
</code></pre>
<ul>
<li>Set size - <code>len()</code></li>
</ul>
<h3><a class="header" href="#22-basic-sets" id="22-basic-sets">2.2 Basic sets</a></h3>
<h4><a class="header" href="#221-sets-within-sets" id="221-sets-within-sets">2.2.1 Sets within Sets</a></h4>
<p>{\(x \in A | .... \)} = {elements in A such that}</p>
<ul>
<li>
<p>Integer Intervals</p>
<p>\(N = \{ x \in Z | x \geq 0 \}\), \(P = \{ x \in Z | x &gt; 0 \} \)</p>
</li>
<li>
<p>Real intervals</p>
<p>\([a,b] = \{ x \in R | a \leq x \leq b \}\)</p>
<p>\((a,b) = \{ x \in R | a &lt; x &lt; b \}\)</p>
</li>
<li>
<p>Divisibility</p>
<p><img src="https://user-images.githubusercontent.com/41487483/119231729-a49d8180-bb22-11eb-9a44-901751904204.png" alt="image" /></p>
<p><strong>Sets of Multiples</strong> </p>
<p>\(m \in Z\), \(_m Z = \{ i \in Z : m| i \}\)</p>
<p><em>Even numbers</em>: \(_2 Z = \{ ..., -4, -2, 0, 2, 4, ... \} = E\) </p>
</li>
</ul>
<p><strong>Python syntax</strong></p>
<ul>
<li>
<p>{0,...,n-1}: <code>range(n)</code></p>
</li>
<li>
<p>{m,...,n-1}: <code>range(m,n)</code></p>
</li>
<li>
<p>{m, m+d, m+2d, ...} &lt; n-1: <code>range(m, n, d)</code></p>
</li>
</ul>
<pre><code class="language-py">print(set(range(3)))
#Output: {0, 1, 2}

print(set(range(2,5)))
#Output: {2, 3, 4}

print(set(range(2,12,3)))
#Output: {2, 5, 8, 11}
#Return type range, but conver to set if print
</code></pre>
<h4><a class="header" href="#222-visualization---venn-diagram" id="222-visualization---venn-diagram">2.2.2 Visualization - Venn Diagram</a></h4>
<pre><code class="language-py">import matplotlib.pyplot as plt
import matplotlib_venn as venn
S = {1, 2, 3}
T = {0, 2, -1, 5}
venn.venn2([S, T], set_labels=('S','T'))
plt.show()

#for 3 sets: venn.venn3([S,T,U], set_labels=(’S’,’T’,'U'))
</code></pre>
<h3><a class="header" href="#23-relations" id="23-relations">2.3 Relations</a></h3>
<h4><a class="header" href="#231-number-relations" id="231-number-relations">2.3.1 Number relations</a></h4>
<ul>
<li>
<p>Equality - = or ≠</p>
</li>
<li>
<p>Intersection - two sets share at least one common element</p>
<p>Disjoint - no shared elements</p>
</li>
<li>
<p>Subsets - \(A \subseteq B\) </p>
<p>superset - \(B \supseteq A\) </p>
<p>\[
P \subseteq N \subseteq Z \subseteq Q \subseteq R
\]</p>
<p>strict subset - if \(A \subseteq B\) and \(A \neq B\), A is a strict subset of B, denote \(A \subset B\); conversely, A is a strict superset of B, \(B \supset A\)</p>
</li>
</ul>
<h4><a class="header" href="#232-belongs-to--in--vs-subsets-of-subseteq-" id="232-belongs-to--in--vs-subsets-of-subseteq-">2.3.2 Belongs to (\( \in \)) vs. Subsets of (\(\subseteq \))</a></h4>
<ul>
<li>
<p>\( x \in A \): element x belongsto set A</p>
<p>\( 0 \in \{0,1\} \)</p>
</li>
<li>
<p>(\( A \subseteq B \)): A is a subset of B</p>
<p>\( \{ 0 \} \subseteq \{0,1\} \)</p>
</li>
</ul>
<h4><a class="header" href="#233-python-syntaxt" id="233-python-syntaxt">2.3.3 Python syntaxt</a></h4>
<ul>
<li>
<p>Check equality and disjoint</p>
<p><code>==</code>, <code>!=</code>, <code>.isjointed()</code></p>
<pre><code class="language-py">S1={0,1}; S2=set({0,1}); S3={1,0,1}; T={0,2}

# Equality
S1 == T
#Output: False
S1 == S2
S1 == S3
#Output: True

# Inequality
S1 != S2

# Disjoint 
S1.isdisjoint(T)
S1.isdisjoint({2})

</code></pre>
</li>
<li>
<p>Check subsets and supersets</p>
<p><code>&lt;=</code> or <code>issubset</code> for \(\subseteq \) and <code>&lt;</code> for \(\subset \) </p>
<p><code>&gt;=</code> or <code>issuperset</code> for \(\supseteq \) </p>
<pre><code class="language-py">zero = {0}; zplus = {0,1}; zminus = {0, -1}

print(zminus &lt;= zplus)
#Output: False
print(zminus &gt;= zplus)
#Output: False

zero.issubset(zminus)
#Output: True
</code></pre>
</li>
</ul>
<h3><a class="header" href="#24-operations" id="24-operations">2.4 Operations</a></h3>
<h4><a class="header" href="#241-intersection-and-complement" id="241-intersection-and-complement">2.4.1 Intersection and complement</a></h4>
<ul>
<li>
<p>Commutative: \(A \cap B = B \cap A\), \(A \cup B = B \cap A\)</p>
</li>
<li>
<p>Associative: \((A \cap B) \cap C = A \cap (B \cap C)\), \((A \cup B) \cup C = A \cup (B \cup C)\)</p>
</li>
<li>
<p>Distributive: \(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\), \(A \cup (B \cap C) = (A \cup B) \cap (A \cup C)\) </p>
</li>
<li>
<p>De Morgan \((A \cap B)^c = A^c \cup B^c\), \((A \cup B)^c = A^c \cap B^c\)</p>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/120278375-7d2e7d80-c2b5-11eb-8206-7da3043f30d1.png" alt="image" /></p>
<h4><a class="header" href="#242-set-difference-a-b" id="242-set-difference-a-b">2.4.2 Set Difference A-B</a></h4>
<ul>
<li>\(A-B = \{ x: x \in A \wedge x \notin B \} = A \cap B^c\) </li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/120279116-5ae92f80-c2b6-11eb-8c10-01913e85e304.png" alt="image" /></p>
<ul>
<li>
<p>Symmetric Difference</p>
<p>The symmetric differene of two sets is the set of elements in exactly one set. </p>
<p>\(A bigtriangleup B = \{x: x \in A \wedge x \notin B  \vee x \in B \wedge x \notin A \} \)</p>
<p><img src="https://user-images.githubusercontent.com/41487483/120279841-3772b480-c2b7-11eb-851c-a33544dd75f7.png" alt="image" /></p>
</li>
</ul>
<h4><a class="header" href="#243-python-syntax" id="243-python-syntax">2.4.3 Python Syntax</a></h4>
<p><strong>Union and Intersection</strong></p>
<ul>
<li>Union \(\cup\): <code>|</code> or <code>union</code></li>
</ul>
<pre><code class="language-py">A = {1,2}
B = {2,3}

print(A|B)

C = A.union(B)
print(C)
</code></pre>
<ul>
<li>Intersection \(\cap\): <code>&amp;</code> or <code>intersection</code></li>
</ul>
<pre><code class="language-py">print(A&amp;B)

C = A.intersection(B)
print(C)
</code></pre>
<p><strong>Set- and Symmetric-Difference</strong></p>
<ul>
<li>
<p>Set difference: <code>-</code> or <code>difference</code></p>
<pre><code class="language-py">A = {1,2}
B = {2,3}

A - B

C = B.difference(A)
print(C)
</code></pre>
</li>
<li>
<p>Symmetric difference: <code>^</code> or <code>symmetric_difference</code></p>
<pre><code class="language-py">A^B

C = B.symmetric_difference(A)
print(C)
</code></pre>
</li>
</ul>
<h4><a class="header" href="#244-caetesian-products" id="244-caetesian-products">2.4.4 Caetesian products</a></h4>
<ul>
<li>
<p><strong>Set</strong>: Order and repetition do not matter {a,b,c} = {b,a,c}</p>
</li>
<li>
<p><strong>Tuple</strong>: Both order and reperition matter (a,b,c) ≠ (b,a,c) and (a,a,a) ≠ (a)</p>
<ul>
<li>
<p><strong>n-tuple</strong>: Tuple with n elements</p>
</li>
<li>
<p><strong>2-tuple</strong>: Ordered pair (a,b)</p>
</li>
</ul>
</li>
</ul>
<h5><a class="header" href="#cartesian-products" id="cartesian-products"><strong>Cartesian products</strong></a></h5>
<p>The cartesian product of A and B is the set AxB of ordered pairs (a,b) where a \(\in A\) and b \(\in B\)</p>
<p>\[
A \times B = \{(a,b): a \in A, b \in B \}<br />
\]</p>
<ul>
<li>
<p>\(A \times A\) denotes \(A^2\)</p>
</li>
<li>
<p>\(R^2 = \{(x,y): x,y \in R\} \) - <strong>Cartesian Plane</strong></p>
</li>
<li>
<p>\(A, B \subseteq R \) then \(A \times B \subseteq R^2 \) - <strong>Rectangle</strong></p>
</li>
<li>
<p>\(A \times B = \{(x,y): x \in [0,2], y \in [1,4] \}\), where A = [0,2] and B = [1,4]</p>
<p><img src="https://user-images.githubusercontent.com/41487483/120438263-6fdec500-c381-11eb-9064-8bdb146ea448.png" alt="image" /></p>
</li>
</ul>
<ol>
<li><strong>Discrete sets</strong></li>
</ol>
<p><img src="https://user-images.githubusercontent.com/41487483/120438592-dcf25a80-c381-11eb-9f05-c377799e2def.png" alt="image" /></p>
<ol start="2">
<li><strong>Tables</strong></li>
</ol>
<p><em>Tables are Cartesian products</em></p>
<p><img src="https://user-images.githubusercontent.com/41487483/120439120-73bf1700-c382-11eb-9e5f-e3c4ced0d0a9.png" alt="image" /></p>
<ol start="3">
<li><strong>Cartesian product of 3 sets</strong> </li>
</ol>
<p>A x B - 2D</p>
<p>A x B x C - 3D</p>
<ol start="4">
<li><strong>Sequence</strong> </li>
</ol>
<pre><code>Sequence is tuples just without '()' and some times without ','
</code></pre>
<h5><a class="header" href="#cartesian-products-with-python" id="cartesian-products-with-python"><strong>Cartesian products with Python</strong></a></h5>
<pre><code class="language-py">from itertools import product

Faces = set({'J', 'Q', 'K'})
Suits = {'♢','♡'}
for i in product(Faces, Suits):
print(i)
</code></pre>
<h4><a class="header" href="#245-russells-paradox" id="245-russells-paradox">2.4.5 Russell's Paradox</a></h4>
<p><img src="https://user-images.githubusercontent.com/41487483/120651186-ca118000-c47e-11eb-9f1f-d1e85e4d090b.png" alt="image" /></p>
<h2><a class="header" href="#3-counting" id="3-counting">3. Counting</a></h2>
<h3><a class="header" href="#31-set-size" id="31-set-size">3.1 Set Size</a></h3>
<h4><a class="header" href="#311-basic-concepts" id="311-basic-concepts">3.1.1 Basic concepts</a></h4>
<p>The number of elements in a set S is called its <strong>size</strong>, or <strong>cardinality</strong> (基数), denoted |B| or # S.</p>
<p>in Python </p>
<ul>
<li>
<p>Size: <code>len</code>, i.e., <code>len({-1, 1})</code></p>
</li>
<li>
<p>Sum: <code>sum</code>, i.e., <code>sum({-1, 1})</code></p>
</li>
<li>
<p>minimum: <code>min</code>, i.e., <code>min({-1, 1})</code></p>
</li>
<li>
<p>maximum: <code>max</code>, i.e., <code>max({-1, 1})</code></p>
</li>
</ul>
<h4><a class="header" href="#312-disjoint" id="312-disjoint">3.1.2 Disjoint</a></h4>
<ul>
<li>
<p>Additional rule (for disjoint): </p>
<p>\(A \cap B = \varnothing\): \(|A| + |B| = |A \cup B\)|</p>
</li>
<li>
<p>Subtraction rule (for complement):</p>
<p>\(A \subseteq B \implies B = A \cup (B - A) \implies |B| = |A| + |B - A|\) </p>
</li>
</ul>
<h4><a class="header" href="#313-general-unions" id="313-general-unions">3.1.3 General Unions</a></h4>
<p><strong>Principle of Inclusion-Exclusion (PIE)</strong></p>
<ul>
<li>Two sets</li>
</ul>
<p>\[
|A \cup B| = |A| + |B| - |A \cap B |
\] </p>
<ul>
<li>
<p>Three sets
\[
|A \cup B \cup C| = |A| + |B| + |C| - |A \cap B |- |A \cap C| - |B \cap C| + |A \cap B \cap C| 
\]</p>
</li>
<li>
<p>n sets</p>
</li>
</ul>
<h4><a class="header" href="#314-cartesian-products" id="314-cartesian-products">3.1.4 Cartesian Products</a></h4>
<p><strong>Product Rule</strong> - the size of a Cartesian Product is the product of the set sizes. (multiplication)</p>
<p>\[
|A \times B| = |A| \times |B|
\]</p>
<h4><a class="header" href="#315-cartesian-powers" id="315-cartesian-powers">3.1.5 Cartesian Powers</a></h4>
<p>Applications: </p>
<ul>
<li>
<p>Binary strings: \(\{0,1\}^n = |\{0,1\}|^n = 2^n\)</p>
</li>
<li>
<p>Subsets</p>
<p>The <em>power set</em> of S, \(P(S)\), is the collection of all subsets of S.</p>
<p>\[
P(\{ a, b \}) = \{ \{ \}, \{ a \}, \{ b \}, \{a, b \} \}<br />
\]</p>
<p>The size of the power set is the power of the set size.</p>
<p>\[
|P(S)| = |\{0,1\}|^{|S|} = 2^{|S|} 
\]</p>
<p>\(P(P(S))\) - set of subsets of P(S)</p>
<p>\[
|P(P(S))| = 2^{|P(S)|} = 2^{2^{|S|}}
\]</p>
</li>
<li>
<p>Functions</p>
<p>Functions from A to B: \(B^A\), # = \(|B|^{|A|}\)</p>
<ul>
<li>
<p>Binary functions</p>
<p>Binary functions of n binary variables: Functions from \(\{0 ,1 \}^n \) to \( \{0 ,1 \} \). That is \( \{0,1 \}^{{ \{0,1\} }^{n}} \)</p>
<p>#= \(2^{2^n}\) <strong>Double exponntial</strong></p>
</li>
</ul>
<p><em>Exponential Growth</em></p>
<ul>
<li>
<p>\(A^k\): <code>itertools.product(A, repeat = k)</code></p>
</li>
<li>
<p>\(n^k\): <code>n**k</code></p>
</li>
</ul>
<pre><code class="language-py">import itertools 
set(itertools.product({1,2, 3}, repeat = 2))

#Exponent
print(3**2)
</code></pre>
</li>
</ul>
<h3><a class="header" href="#32-variations" id="32-variations">3.2 Variations</a></h3>
<p>Variable length </p>
<p>Take an example of PIN: #3-5 digit PINs</p>
<h3><a class="header" href="#33-counting-trees" id="33-counting-trees">3.3 Counting trees</a></h3>
<ul>
<li>Cartesian products as Trees</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/120917627-800fe080-c6b0-11eb-8fe2-8edc7186f27c.png" alt="image" /></p>
<ul>
<li>Trees are more general products</li>
</ul>
<p>For example, in a university, there are 3 departments, and each department has 2 different courses. Therefore there are 6 courses in total. </p>
<p><strong>Path from Sources to Destination</strong></p>
<p><img src="https://user-images.githubusercontent.com/41487483/120917920-0da00000-c6b2-11eb-9016-1182865eb950.png" alt="image" /></p>
<h2><a class="header" href="#4-permutations-and-combinations" id="4-permutations-and-combinations">4 Permutations and combinations</a></h2>
<h3><a class="header" href="#41-permutations" id="41-permutations">4.1 Permutations</a></h3>
<h4><a class="header" href="#411-basic-concept-and-application" id="411-basic-concept-and-application">4.1.1 Basic concept and application</a></h4>
<ul>
<li>
<p><strong>n factorial = n!</strong></p>
</li>
<li>
<p>0! = 1</p>
</li>
<li>
<p><strong>Stirling's approximation</strong></p>
<p>\[
n! \sim \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n 
\]</p>
</li>
</ul>
<h4><a class="header" href="#412-partial-permutations" id="412-partial-permutations">4.1.2 Partial Permutations</a></h4>
<ul>
<li>
<p>permutations of k out of n objects: k-permutaitons of n</p>
<p>\(n \cdot (n-1) \cdot (n-2) \cdot \dotsb \cdot(n-k+1) = \frac{n!}{(n-k)!} \newcommand*{\defeq}{\stackrel{\text{def}}{=}} (n)^{\underline{k}}\)</p>
<p>kth falling power of n, also denoted \(P(n,k)\)</p>
</li>
</ul>
<h3><a class="header" href="#42-combinations" id="42-combinations">4.2 Combinations</a></h3>
<ul>
<li>
<p>Sequences with k 1's</p>
<p>\(\binom{[n]}{k} \) - collection of k-subsets of [n] = {1,2,...,n}</p>
<p>corresponds to n-bit sequences with k 1's</p>
<p>two interpretations</p>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/41487483/121245332-3554bb00-c8a0-11eb-82e7-5af3f606d8a9.png" alt="image" /></p>
<ul>
<li>Number of n-bit sequences with k 1's: \(\binom{n}{k}\) </li>
</ul>
<h4><a class="header" href="#421-binomial-coefficients" id="421-binomial-coefficients">4.2.1 Binomial coefficients</a></h4>
<p>\[
\binom{n}{k} = \frac{n^{\underline{k}}}{k!} = \frac{n!}{k!(n-k)!}
\] </p>
<ul>
<li>
<p>\(\binom{n}{k} = \binom{n}{n-k}\)</p>
</li>
<li>
<p>recursive: \(\binom{n}{k} = \frac{n}{k} \cdot \binom{n-1}{k-1}\)</p>
<p>\[
\binom{n}{k} \cdot k = n \cdot \binom{n-1}{k-1}<br />
\]</p>
</li>
<li>
<p>\(\sum\limits_{i=0}^{n} \binom{n}{i} = 2^n\)</p>
</li>
</ul>
<h4><a class="header" href="#422-binomial-theorem" id="422-binomial-theorem">4.2.2 Binomial Theorem</a></h4>
<ul>
<li>Pascal's identity</li>
</ul>
<p>\[
\binom{n+1}{k} = \binom{n}{k} + \binom{n}{k-1}<br />
\]</p>
<ul>
<li>
<p>Pascal's triangle</p>
<p><img src="https://user-images.githubusercontent.com/41487483/121404277-187dbd80-c95c-11eb-9214-7cdbdbe4dfc1.png" alt="image" /></p>
</li>
<li>
<p><strong>Binomial Theorem</strong></p>
<p>\[
(a+b)^n = \sum\limits_{i=0}{n} \binom{n}{i}a^{n-i}b^i
\]</p>
<p>For example, \((a+b)^4 = a^4 + 4a^3b + 6a^2b^2 + 4ab^3 + b^4\)</p>
<p>Think of select # b from n set of {a,b}: </p>
<p>\[
(a+b)^n = \binom{n}{0}a^n + \binom{n}{1}a^{n-1}b + \dots + \binom{n}{n}b^n = \sum\limits_{i=0}^{n}\binom{n}{i}a^{n-i}b^i<br />
\]</p>
<ul>
<li>
<p>Polynomial coefficient</p>
<p>\[
(1+x)^n = \sum\limits_{i=0}^{n}\binom{n}{i}x^i<br />
\]</p>
</li>
<li>
<p>Taylor expansion</p>
<p>\[
e^x = \sum\limits_{i=0}^{\infty} \frac{x^i}{i!}
\]</p>
<p>derived from \((1 + \frac{x}{n})^n = \sum\limits_{i=0}{n} \binom{n}{i} \left(\frac{x}{n}\right)^i\)</p>
</li>
<li>
<p>Binomial distribution</p>
<p>\[
\sum\limits_{i=0}^{n} \binom{n}{i} p^{n-1}(1-p)^i = (p + (1 - p))^n = 1^n = 1<br />
\]</p>
</li>
</ul>
<h4><a class="header" href="#423-multinomial-coefficients" id="423-multinomial-coefficients">4.2.3 Multinomial coefficients</a></h4>
<p>\[
\frac{n!}{k_1! \cdot k_2! \cdot k_3!} \triangleq \binom{n}{k_1, k_2, k_3}, (k_1 + k_2 + k_3 = n)
\]</p>
<ul>
<li>Multinomial theorem</li>
</ul>
<p>\[
(a_1 + a_2 + \dots + a_m)^n = \sum\limits_{k_1 + k_2 + \dots + k_m = n \\ k_1, k_2, \dots, k_m \geq 0} \binom{n}{k_1,k_2,\dots, k_m}  \prod\limits_{t=1}^{m} a_t^{k_t}
\]</p>
<ul>
<li>Sum of Multinomialas</li>
</ul>
<p>\[
m^n = (1 + 1 + \dots + 1)^n = \sum\limits_{k_1 + k_2 + \dots + k_m = n \\ k_1, k_2, \dots, k_m \geq 0} \binom{n}{k_1,k_2,\dots, k_m}
\]</p>
</li>
</ul>
<h3><a class="header" href="#43-stars-and-bars" id="43-stars-and-bars">4.3 Stars and bars</a></h3>
<h4><a class="header" href="#431-basic-applications" id="431-basic-applications">4.3.1 Basic applications</a></h4>
<ul>
<li>
<p>k terms adding to n</p>
<p>#ways to write n as a sum of k positive integers, when order matters: \(\binom{n-1}{k-1}\)</p>
</li>
<li>
<p>Any Sum to n</p>
<p>#ways to write n as a sum of (any # of) positive integers: \(2^{n-1} = \sum\limits_{i=0}^{n-1}\binom{n-1}{i}\)</p>
</li>
<li>
<p>Nonnegative terms</p>
<p>#ways to write n as a sum of k nonnegative integers: \(\binom{n+k-1}{k-1}\)</p>
</li>
<li>
<p>Simple example</p>
<p>4-letter words (order doesn't matter): #a + #b + ... + #z = 4 \(\implies \binom{4+26-1}{26-1} = \binom{29}{25} = \binom{29}{4}\)</p>
</li>
</ul>
<h4><a class="header" href="#432-more-applications" id="432-more-applications">4.3.2 More applications</a></h4>
<ul>
<li>
<p>#k positive adding to n = #k nonnegative adding to n-k</p>
<p>\[
\binom{n-1}{k-1} = \binom{n-k+(k-1)}{k-1}<br />
\]</p>
</li>
<li>
<p>#k nonnegative adding to ≤ n = #k+1 nonnegative adding to n</p>
<p>\[
\binom{n+k}{k} = \binom{n+(k+1)-1}{(k+1)-1}
\]</p>
<p><em>need to use Pascal's triangle?</em></p>
</li>
</ul>
<h3><a class="header" href="#44-a-hrefhttpscolabresearchgooglecomdrive14mdazpq1neftfvubrfmxl-bs9bea52wppython-notebooka" id="44-a-hrefhttpscolabresearchgooglecomdrive14mdazpq1neftfvubrfmxl-bs9bea52wppython-notebooka">4.4 <a href="https://colab.research.google.com/drive/14mdAzPq1NeftFVUBRFmxl-bs9bEa52wp"><strong>Python Notebook</strong></a></a></h3>
<ul>
<li>
<p>Permutation: <code>itertools.permutations(A)</code></p>
</li>
<li>
<p>Partial permutation: <code>itertools.permutations(A, k)</code></p>
</li>
<li>
<p>Factorial: <code>factorial(len(A))</code> using the factorial function in math <code>from math import factorial</code></p>
</li>
<li>
<p>Combinations: <code>itertools.combinations(A,k)</code></p>
</li>
<li>
<p>Week exercise</p>
<p><em>Use Python to generate a k-composition of an integer n, i.e., a k-tuple of positive integers that sum to n</em></p>
<ul>
<li>
<p>The simpler way: <code>int(binom(n-1,k-1))</code></p>
</li>
<li>
<p>To obtain all the tuples in the composition by define a function:</p>
</li>
</ul>
<pre><code class="language-py">import sys
import numpy as np
# not clear what the following packages used for
import scipy as sp
from scipy.special import *

def compositions(k, n):
  if k == 1:
      return {(n,)} # (n,) means a tuple containg a single value

  comp = set()
  # comp = [] will generate a list instead of a set.

  for i in range(1, n):  # 1,2,....,n
      for t in compositions(k - 1, n - i): #recursively
          comp.add((i,) + t)

  return comp
</code></pre>
</li>
</ul>
<h2><a class="header" href="#5-topic-5-probability-introduction" id="5-topic-5-probability-introduction">5 Topic 5 Probability Introduction</a></h2>
<h3><a class="header" href="#51-basic-concept" id="51-basic-concept">5.1 Basic concept</a></h3>
<p>Random value of outcome, denoted by X.</p>
<p>Probability of random outcome x denoted by P(x) or P(X=x)</p>
<p><strong>Probability distribution function (PDF)</strong></p>
<ul>
<li>
<p>uniform probability space</p>
<p>Toss an unbiased coin or die...</p>
</li>
<li>
<p>non-uniform probability space</p>
</li>
</ul>
<h3><a class="header" href="#52-three-axioms" id="52-three-axioms">5.2 Three Axioms</a></h3>
<ul>
<li>
<p><strong>Non-negativity</strong> \(P(A) \geq 0\)</p>
</li>
<li>
<p><strong>Unitarity</strong> \(P(\Omega) = 1\)</p>
</li>
<li>
<p><strong>Addition rule</strong>: A,B disjoint \(P(A \cup B)= P(A) + P(B)\)</p>
</li>
</ul>
<h1><a class="header" href="#common-problems" id="common-problems">Common problems</a></h1>
<h2><a class="header" href="#1-problems-about-counting" id="1-problems-about-counting">1 Problems about counting</a></h2>
<p><strong>Probability that a five-card hand contains</strong></p>
<p><em>a standard 52-card deck with four suits (Clubs, Diamonds, Hearts, and Spades) and thirteen ranks (2,..., 10, jack, Queen, King, and Ace)</em></p>
<p>#ways of selecting 5 cards from 52 cards: \(\binom{52}{5}\)</p>
<ol>
<li>
<p>the ace of diamonds</p>
<p>#ways that the ace of diamonds was selected in 5 cards (i.e., select four other cards from the remaining 51 cards): \(1 \times \binom{51}{4}\)</p>
<p>\(P = \frac{1 \times \binom{51}{4}}{\binom{52}{5}}\)</p>
</li>
<li>
<p>at least an ace</p>
<p>Which is easier to calculate the compensate - counting #ways of no ace: \(\binom{48}{5}\)</p>
<p>\(P = 1 - \frac{\binom{48}{5}}{\binom{52}{5}}\)</p>
</li>
<li>
<p>at least a diamond</p>
<p>#ways of no diamond: \(\binom{39}{5}\)</p>
<p>\(P = 1 - \frac{\binom{39}{5}}{\binom{52}{5}}\)</p>
</li>
<li>
<p>the probability that two cards drawn from a standard deck without replacement have the same rank</p>
<p>#ways of selecting two cards: \(\binom{52}{2}\)</p>
<p>#ways of selecting two cards in the same rank: \(\binom{13}{1}\binom{4}{2}\)</p>
<p>\(P = \frac{\binom{13}{1}\binom{4}{2}}{\binom{52}{2}}\)</p>
</li>
</ol>
<h1><a class="header" href="#inferential-statistics-with-python" id="inferential-statistics-with-python">Inferential Statistics with Python</a></h1>
<p><em>Notes from the 2nd course in &quot;Statistics with Python Specialization&quot; on Coursera</em></p>
<p>Commonly used Python library for inferential statistics</p>
<pre><code class="language-py">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
</code></pre>
<h2><a class="header" href="#1-confidence-interval" id="1-confidence-interval">1 Confidence interval</a></h2>
<p><a href="https://colab.research.google.com/drive/18jGIoV-b3UWxA_BZwviKUrQnN1747yo-#scrollTo=2seqqn_-Th5a">Example</a> using NHANES dataset.</p>
<h3><a class="header" href="#11-one-proportion-categorical-variables" id="11-one-proportion-categorical-variables">1.1 One proportion (categorical variables)</a></h3>
<p>\[
\text{confidence interval} = \text{best estimate} \pm \text{mutiplier}*\text{standard error} 
\]</p>
<ul>
<li>
<p>\(\text{standard error} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</p>
</li>
<li>
<p>\(\text{Best estimate} = \text{sample proportion}\)</p>
</li>
</ul>
<h4><a class="header" href="#method-1-using-cross-table-output" id="method-1-using-cross-table-output">Method 1 using cross table output</a></h4>
<ol>
<li>
<p>Cross table</p>
<p>One looking at two proportions from two groups, crosstable might be useful <code>pd.crosstable(dx[col1], dx[col2])</code>.</p>
<p><em>NB: the column names of the cross table are not a list, and thus needs to be renamed by <code>dx.columns = ['col1','col2']</code> in some cases</em></p>
</li>
<li>
<p>Proportion calculation</p>
<pre><code class="language-py">    dz = dx.groupby(['RIAGENDRx']).agg({'SMQ020x': [lambda x: np.mean(x==&quot;yes&quot;), np.size]})
    dz.columns = ['Proportion', 'Total_n']
</code></pre>
<p>Then, calculate p, n, se, respectively</p>
</li>
</ol>
<h4><a class="header" href="#method-2-using-sm-library" id="method-2-using-sm-library">Method 2 using sm library</a></h4>
<p><code>sm.stats.proportion_confint(prop*n, n, alpha = 0.05)</code></p>
<h3><a class="header" href="#12-two-proportion-from-two-independent-variables" id="12-two-proportion-from-two-independent-variables">1.2 Two proportion from two independent variables</a></h3>
<p>\[
\text{confidence interval} = \text{best estimate} \pm \text{mutiplier}*\text{se_diff}<br />
\]</p>
<ul>
<li>
<p>\(\text{SE}_1 = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}}\)</p>
</li>
<li>
<p>\(\text{SE}_2 = \sqrt{\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\)</p>
</li>
<li>
<p>\(\text{se_diff} = \sqrt{SE_1^2 + SE_2^2} = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\)</p>
</li>
<li>
<p>\(\text{Best estimate} = p_1 - p_2\)</p>
</li>
</ul>
<p>Calculate lower confidence interval and upper confidence interval respectively. <em>No sm method so far.</em></p>
<h3><a class="header" href="#13-confidence-interval-for-one-mean-quantative-variable" id="13-confidence-interval-for-one-mean-quantative-variable">1.3 Confidence interval for one mean (quantative variable)</a></h3>
<ul>
<li>
<p>\(\text{Best estimate} = \bar{x}\)</p>
</li>
<li>
<p>\(\text{Standard error} = \frac{s}{\sqrt{n}}\) </p>
<ul>
<li>
<p><em>s</em> is the sample standard deviation <code>np.std(data, ddof=1)</code></p>
</li>
<li>
<p>population standard deviation <code>np.std(data, ddof=0)</code></p>
</li>
</ul>
</li>
<li>
<p>multiplier depends on the significant level and distribution (z or t). If t distribution, the shape depends on the degree of freedom.</p>
</li>
<li>
<p>z distribution: <code>sm.stats.DescrStatsW(bmi_female).zconfint_mean()</code></p>
</li>
</ul>
<h3><a class="header" href="#14-confidence-interval-for-two-means-from-two-independent-populations" id="14-confidence-interval-for-two-means-from-two-independent-populations">1.4 Confidence interval for two means from two independent populations</a></h3>
<h4><a class="header" href="#141-unpooled-approach-sigma_1-neq-sigma_2" id="141-unpooled-approach-sigma_1-neq-sigma_2">1.4.1 Unpooled approach (\(\sigma_1 \neq \sigma_2\))</a></h4>
<ul>
<li>
<p>\(\text{Best estimate} = \bar{x}_1 - \bar{x}_2\)</p>
</li>
<li>
<p>\(\text{se_diff} = \sqrt{SE_1^2 + SE_2^2} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}\)</p>
</li>
<li>
<p>\(df = min(n_1 - 1, n_2 - 1)\) which is a very conservative way <em>or using Welch's approximation</em></p>
</li>
</ul>
<h4><a class="header" href="#142-pooled-approach-sigma_1--sigma_2" id="142-pooled-approach-sigma_1--sigma_2">1.4.2 Pooled approach (\(\sigma_1 = \sigma_2\))</a></h4>
<ul>
<li>
<p>\(\text{Best estimate} = \bar{x}_1 - \bar{x}_2\)</p>
</li>
<li>
<p>\(\text{se_diff} = \sqrt{SE_1^2 + SE_2^2} = \sqrt{\frac{(n_1-1)*s_1^2 + (n_2-1)*s_2^2}{n_1+n_2 - 2}}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)</p>
</li>
<li>
<p>\(df = n_1 + n_2 - 2\) </p>
</li>
</ul>
<h2><a class="header" href="#2-hypothesis-test" id="2-hypothesis-test">2 Hypothesis test</a></h2>
<p><strong>General steps:</strong></p>
<ul>
<li>
<p>Set up a Hypothesis \(H_0\) and significant level \(\alpha\)</p>
</li>
<li>
<p>Check conditions:</p>
<ul>
<li>
<p>simple random sample?</p>
</li>
<li>
<p>nearly normal distribution or sample size large enough </p>
</li>
<li>
<p>calculate test statistics (z-score or t)</p>
</li>
</ul>
<p>\[
z = \frac{\text{Best estimate} - \text{hypothesized estimate}}{standard error of estimate}<br />
\]</p>
<ul>
<li>find p-value and compare to \(\alpha\) and make conclusion - reject \(H_0\) or fail to reject \(H_0\)</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#21-test-on-a-population-proportion" id="21-test-on-a-population-proportion">2.1 Test on a population proportion</a></h3>
<ul>
<li>
<p>set null hypothesis</p>
<ul>
<li>
<p>\(H_0: p_0\)</p>
</li>
<li>
<p>\(H_A: \hat{p}\)</p>
</li>
</ul>
</li>
<li>
<p>Check conditions: \(np \geq 10, n(1-p) \geq 10\)</p>
</li>
<li>
<p>calculate test statistics and p value</p>
<p>\[
z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}<br />
\]</p>
<p><em>se is based on null hypothesis</em></p>
<ul>
<li>
<p>traditional method for p value: <code>p_val = 2*dist.norm.cdf(-np.abs(test_stat))</code></p>
</li>
<li>
<p><code>sm.stats.proportions_ztest()</code></p>
</li>
<li>
<p><code>sm.stats.binom_test()</code></p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#22-test-on-difference-in-population-proportions" id="22-test-on-difference-in-population-proportions">2.2 Test on difference in population proportions</a></h3>
<ul>
<li>
<p>set null hypothesis</p>
<p>\(H_0: p_1 - p_2 = 0\)</p>
<p>\(H_A: p_1 - p_2 \neq 0\)</p>
</li>
<li>
<p>Check conditions: \(n_1p_1 \geq 10, n_1(1-p_1) \geq 10, n_2p_2 \geq 10, n_2(1-p_2) \geq 10\)</p>
</li>
<li>
<p>calculate test statistics</p>
<p>\[
z = \frac{\hat{p}_1-\hat{p_2}_1 - 0}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1}+\frac{1}{n_2})}}<br />
\]</p>
<p><em>se is based on combined the proportion</em> \(p = \frac{(n_1p_1 + n_2p_2)}{(n_1+n_2)}\)</p>
<ul>
<li>
<p>traditional method for p value: <code>p_val = 2*dist.norm.cdf(-np.abs(test_stat))</code></p>
</li>
<li>
<p>t test: <code>sm.stats.ttest_ind(population1,population2)</code></p>
</li>
<li>
<p>z score: <code>sm.stats.ztest(population1,population2)</code></p>
</li>
</ul>
</li>
<li>
<p>Alternative approaches</p>
<ol>
<li>
<p>Chi-square text: different hypothesis and two-side hypothesis</p>
</li>
<li>
<p>Fisher's Exact test</p>
<ul>
<li>
<p>allow one-side hypothesis</p>
</li>
<li>
<p>typically for small sample size</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3><a class="header" href="#23-test-on-one-population-mean" id="23-test-on-one-population-mean">2.3 Test on one population mean</a></h3>
<ul>
<li>
<p>set null hypothesis</p>
<p>\(H_0: \mu = ?\)</p>
<p>\(H_A: \mu \neq ?, \mu &gt; ?, \mu &lt; ?\) depending on the research questions</p>
</li>
<li>
<p>Exam results, check assumptions, summarize data (boxplot, QQplot, Histogram)</p>
</li>
<li>
<p>calculate test statistics</p>
<p>\[
t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}<br />
\]</p>
<ul>
<li>
<p>s: sample standard deviation <code>np.std(x, ddof=1)</code></p>
</li>
<li>
<p><code>sm.stats.ztest()</code></p>
</li>
</ul>
</li>
<li>
<p>What if normality doesn't hold</p>
<ul>
<li>
<p>non-parametric test: </p>
<p>e.g. Wilcoxon signed Rank test (use median to do test statistics)</p>
</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#24-test-on-a-difference-on-population-means-based-on-paired-data" id="24-test-on-a-difference-on-population-means-based-on-paired-data">2.4 Test on a difference on population means based on paired data</a></h3>
<ul>
<li>
<p>set null hypothesis</p>
<p>\(H_0: \mu_d = 0\)</p>
<p>\(H_A: \mu_d \neq 0\) </p>
</li>
<li>
<p>Exam results, check assumptions, summarize data (boxplot, QQplot, Histogram)</p>
</li>
<li>
<p>calculate test statistics</p>
<p>\[
t = \frac{\bar{x}_d - 0}{\frac{s_d}{\sqrt{n}}}<br />
\]</p>
<ul>
<li>
<p><code>sm.stats.ztest()</code> or <code>sm.stats.ttest_ind()</code></p>
</li>
<li>
<p>should be in line with the confidence interval inference: </p>
</li>
</ul>
<p>\[
\bar{x}_d \pm t* \frac{s_d}{\sqrt{n}}
\]</p>
</li>
<li>
<p>Normality doesn't hold? - Wilcoxon signed rank est</p>
</li>
</ul>
<h3><a class="header" href="#24-test-on-a-difference-on-population-means-based-on-independent-data" id="24-test-on-a-difference-on-population-means-based-on-independent-data">2.4 Test on a difference on population means based on independent data</a></h3>
<ul>
<li>
<p>set null hypothesis</p>
<p>\(H_0: \mu_d = 0 \text{ or } \mu_1 = \mu_2\)</p>
<p>\(H_A: \mu_d \neq 0 \text{ or } \mu_1 \neq \mu_2\) </p>
</li>
<li>
<p>Exam results, check assumptions, summarize data (boxplot, QQplot, Histogram)</p>
</li>
<li>
<p>calculate test statistics</p>
<p>\[
t = \frac{(\bar{x}_1 - \bar{x}_2) - 0}{se}<br />
\]</p>
<ul>
<li>
<p>pooled approach (\(\sigma_1^2 \approx \sigma_2^2\)) variance</p>
<p>\[
se = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2 - 2}}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}<br />
\]</p>
<p>\(df = n_1 + n_2 - 2\)</p>
</li>
<li>
<p>unpooled approach (\(\sigma_1^2 \approx \sigma_2^2\) is not needed)</p>
<p>\[
se = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\]</p>
<p>\(df = min(n_1-1,n_2-1)\)</p>
</li>
<li>
<p><code>sm.stats.ztest()</code> or <code>sm.stats.ttest_ind()</code></p>
</li>
<li>
<p><code>sm.stats.CompareMeans(bmi_female, bmi_male).ztest_ind(usevar='pooled')</code></p>
<p>The argument <code>bmi_female</code> should be the output of <code>sm.stats.DescrStatsW(data)</code></p>
</li>
</ul>
</li>
</ul>
<h1><a class="header" href="#data-viz" id="data-viz">Data Viz</a></h1>
<h1><a class="header" href="#a-list-of-all-the-projects-i-have-done-in-the-courses" id="a-list-of-all-the-projects-i-have-done-in-the-courses">A list of all the projects I have done in the courses</a></h1>
<br>
<ol>
<li>Data Visualization and Communication with Tableau
by Duke University. <strong><a href="https://public.tableau.com/profile/jing.ai#!/vizhome/TableauFinalproject_16017395213490/Presentation">The link</a></strong></li>
</ol>
<br>
<ol start="2">
<li>Increasing Real Estate Management Profits: Harnessing Data Analytics by Duke University <strong><a href="https://public.tableau.com/profile/jing.ai#!/vizhome/DukeCapstoneDashboard_16177196005810/Dashboard1">This link</a></strong></li>
</ol>
<br>
<ol start="3">
<li>Google Data Analytics Certificate <strong><a href="https://public.tableau.com/profile/jing.ai#!/vizhome/shared_bike_202105/Dashboard1">This link</a></strong></li>
</ol>
<h1><a class="header" href="#english" id="english">English</a></h1>
<h1><a class="header" href="#speaking-english" id="speaking-english">Speaking English</a></h1>
<h1><a class="header" href="#50-important-phrasal-verbs" id="50-important-phrasal-verbs">50 Important Phrasal Verbs</a></h1>
<p><a href="https://www.youtube.com/watch?v=nIlnxm8m2ec">From Speak English with Vanessa</a></p>
<ol>
<li>
<p><strong>to add up</strong></p>
<p><em>something that makes sense; usually negative</em></p>
<p>Her story didn't <strong>add up</strong>. I think she's lying. </p>
</li>
<li>
<p><strong>to back (sb) up</strong></p>
<p><em>to support someone</em></p>
<p>My parents <strong>backed me up</strong> when I decided to apply for graduate school.</p>
<p>I've got your back. (I will support you) </p>
</li>
<li>
<p><strong>to blow up</strong></p>
<p><em>to become suddenly angry</em></p>
<p>When I told her I couldn't come to her party, she <strong>blew up</strong>.</p>
</li>
<li>
<p><strong>to bring it on</strong></p>
<p><em>to accept a challenge with confidence</em></p>
<p>Fifty new phrasal verbs? Yeah, <strong>bring it on</strong>!</p>
</li>
<li>
<p><strong>to bring up</strong></p>
<p><em>to mention in conversation</em></p>
<p>You shouldn't <strong>bring up</strong> politics in this house unless you're ready for a long discussion.</p>
</li>
<li>
<p><strong>to call off</strong></p>
<p><em>to cancel something that has been planned</em></p>
<p>Instead of <strong>calling off</strong> the wedding, the couple decided to elope.</p>
</li>
<li>
<p><strong>to catch up (on)</strong></p>
<p><em>to meet with someone you haven't seen in a while</em></p>
<p>I met my friend for lunch to <strong>catch up</strong> because we haven't seen each other for a long time.</p>
<p>Wanna meet for coffee and <strong>catch up</strong>?</p>
<p>I want to <strong>catch up on</strong> what's happening, so tell me all about it.</p>
</li>
<li>
<p><strong>to check in</strong></p>
<p><em>a register a hotel stay</em></p>
<p>I went to the hotel to <strong>check in</strong> while my husband parked the car.</p>
</li>
<li>
<p><strong>to check out</strong></p>
<p><em>to try sth or to ask sb look at sth with surprise</em></p>
<p>I'm excited to <strong>check out</strong> the new park in the city.</p>
<p>Check it out! (Look at this!)</p>
</li>
<li>
<p><strong>to chip in</strong> </p>
<p><em>to help or contribute money or energy</em></p>
<p>I couldn't go to the party, but I still wanted to <strong>chip in</strong> for a gift.</p>
<p>My son likes to <strong>chip in</strong> and help me with the garden.</p>
</li>
<li>
<p><strong>to come down with</strong></p>
<p><em>to become sick but not serious</em></p>
<p>I'm not feeling so well. I think I'm coming down with something.</p>
<p>Sorry, I think I'm coming down with a cold.</p>
</li>
<li>
<p><strong>to come up with</strong></p>
<p><em>to find an idea for something</em></p>
<p>I need to come up with a great present for my mom's birthday.</p>
<p>I couldn't come up with anything special, so I just baked a cake.</p>
</li>
<li>
<p><strong>to cut back on</strong></p>
<p><em>to consume less of something</em></p>
<p>I'm trying to cut back on fried food, but it's so tasty.</p>
<p>I'm trying to cut back.</p>
</li>
<li>
<p><strong>to cut off</strong></p>
<p><em>to end abruptly, usually driving or speaking</em></p>
<p>The driver in the red car cut me off and almost caused a wreck.</p>
</li>
<li>
<p><strong>to drop by/in</strong></p>
<p><em>to stop by for a visit</em></p>
<p>Hey, are you home? I'm in the neighborhood and I wanted to drop by.</p>
<p>Drop by/in at any time.</p>
</li>
<li>
<p><strong>to end up</strong></p>
<p><em>to eventually decide or reach something</em></p>
<p>We ended up just ordering pizza and not going to the fancy restaurant.</p>
<p>He doesn't want to end up like his father.</p>
</li>
<li>
<p><strong>to fill in</strong></p>
<p><em>to provide information usually spoken</em></p>
<p>I missed the meeting. Can someone fill me in?</p>
</li>
<li>
<p><strong>to fill out</strong></p>
<p><em>to write information on a form</em></p>
<p>Could you please fill out these forms?</p>
</li>
<li>
<p><strong>to get around (to it)</strong></p>
<p><em>to delay doing something</em></p>
<p>I'll get around to that later.</p>
<p>I finally got around to cleaning the garage.</p>
</li>
<li>
<p><strong>to get back at</strong></p>
<p><em>get revenge on someone</em></p>
<p>My sister took my shoes to get back at me for taking her sweater.</p>
</li>
<li>
<p><strong>to keep on</strong></p>
<p><em>to continue doing something</em></p>
<p>If you keep on practicing, you will get better!</p>
<p>Keep on keeping on.</p>
</li>
<li>
<p><strong>to look out</strong></p>
<p><em>to watch out for something</em></p>
<p>When you hike, look out for snakes.</p>
<p>You need to keep an eye out for snakes,</p>
</li>
<li>
<p><strong>to look uo</strong></p>
<p><em>to research</em></p>
<p>I looked it up in the dictionary.</p>
</li>
<li>
<p><strong>to pull over</strong></p>
<p><em>to steer the vehicle to the side of the road and stop</em></p>
<p>It was raining so hard that I had to pull over and wait for the rain to stop.</p>
<p>The police officer pulled me over. </p>
<p>I got pulled over on my way to the party.</p>
</li>
<li>
<p><strong>to put off</strong></p>
<p><em>to postpone something</em></p>
<p>I can't put it off for any longer.</p>
</li>
<li>
<p><strong>to put up with</strong></p>
<p><em>to tolerate something or someone</em></p>
<p>My husband puts up with my terrible singing.</p>
<p>I don't know how he puts up with her! (a little bit harsh)</p>
</li>
<li>
<p><strong>to run away</strong></p>
<p><em>to leave or escape</em></p>
<p>My dog is too lazy to run away.</p>
<p>Did you ever try to run away from home as a kid?</p>
</li>
<li>
<p><strong>to run into</strong></p>
<p><em>to meet unexpectedly</em></p>
<p>I don't want to run into a bear while I'm hiking!</p>
<p>I ran into my high school teacher last week at the grocery store.</p>
</li>
<li>
<p><strong>to run out (of)</strong></p>
<p><em>to have nothing left</em></p>
<p>We ran out of milk yesterday.</p>
</li>
<li>
<p><strong>to stick with</strong></p>
<p><em>to continue doing something</em></p>
<p>I decided to stick with the drums.</p>
<p>Good friends stick with each other even through hard times.</p>
</li>
<li>
<p><strong>to think over</strong></p>
<p><em>to consider it seriously</em></p>
<p>Think it over before you buy a car.</p>
</li>
<li>
<p><strong>to turn down</strong></p>
<p><em>to refuse something or someone</em></p>
<p>I was so full from dinner that I had to turn down dessert.</p>
<p>She turned me down!</p>
</li>
<li>
<p><strong>to turn off</strong></p>
<p><em>to stop the flow of something</em></p>
<p>Some people have trouble turning off their brain before bed.</p>
</li>
<li>
<p><strong>to turn up</strong></p>
<p><em>to appear suddenly</em></p>
<p>My cat was gone for two days. Then he just turned up out of nowhere!</p>
</li>
<li>
<p><strong>to warm up to</strong></p>
<p><em>to start liking someone or something</em></p>
<p>It took my cat a while to warm up to me.</p>
<p>I didn't like the idea at the begining, but I started to warm up to it. </p>
</li>
<li>
<p><strong>to wear off</strong></p>
<p><em>to fade away</em></p>
<p>The energy from my morning tea starts to wear off around noon.</p>
<p>the marker is starting to wear off.</p>
</li>
<li>
<p><strong>to work (something) out</strong></p>
<p><em>to come up with a solution or compromise</em></p>
<p>You guys need to work something out yourself.</p>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
